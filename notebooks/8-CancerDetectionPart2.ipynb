{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We covered at lot this Term\n",
    "* How to write Deep Learning Models with Keras\n",
    "* How to used Neural Networks on Structued Data\n",
    "* How Image data is stored and manipulated\n",
    "* How to use CNNs\n",
    "\n",
    "## Today we will continue with our cancer screening example, using models written by other people\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Powerup - Keras Sequential\n",
    "\n",
    "So far we've been using Keras's Functional API\n",
    "* It is more flexible\n",
    "* It isn't as clean\n",
    "There is another frequently used method of writing models Sequential\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model in the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_input=tf.keras.layers.Input( shape=(96,96,3) ) # Shape here does not including the batch size \n",
    "cnn_layer1=tf.keras.layers.Convolution2D(64, (2,2),strides=2,padding='same')(cnn_input) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "cnn_layer2=tf.keras.layers.Convolution2D(126, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "cnn_layer3=tf.keras.layers.Convolution2D(256, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "\n",
    "flat=tf.keras.layers.Flatten()(cnn_activation) \n",
    "\n",
    "drop=tf.keras.layers.Dropout(0.3)(flat) \n",
    "\n",
    "dense_layer=tf.keras.layers.Dense(1)(drop) \n",
    "output=tf.keras.layers.Activation('sigmoid')(dense_layer)\n",
    "\n",
    "model=tf.keras.models.Model([cnn_input],[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()#add model layers\n",
    "model.add(tf.keras.layers.Convolution2D(64, (2,2),strides=2,padding='same',input_shape=(96,96,3)))\n",
    "model.add(tf.keras.layers.LeakyReLU()) \n",
    "model.add(tf.keras.layers.Dropout(0.3)) \n",
    "model.add(tf.keras.layers.Convolution2D(126, (2,2),strides=2,padding='same'))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Convolution2D(256, (2,2),strides=2,padding='same'))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both of the above create the same model and can use the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using other models for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example VGG16\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/0*xurYLT8UBpFKPNQA>\n",
    "\n",
    "This model was trained on ImageNet (http://www.image-net.org/)\n",
    "* Classification of a thousand of different objects, and trained with Millions of images\n",
    "* We want to take all those pre-trained weights and use them in our cancer example\n",
    "\n",
    "\n",
    "Remember from the models we wrote\n",
    "   * We have a stack of Convolutional Layers\n",
    "   * Flatten Layer\n",
    "   * Dense Layer(s) to make predictions\n",
    "\n",
    "Since the classification is only at the end, we can just remove it\n",
    "   * In the figure above these are all the layers in green\n",
    "   * We can replace it with new layers\n",
    "\n",
    "You'll hear this called **transfer learning**, because we're using the features learned from ImageNet to do classification on our data.\n",
    "\n",
    "**Important** we've been scaling our images from 0-1 by dividing by 255. This isn't the way it's always done, and if you don't use the right image pre-processing functions for the model you're using you might get meaningful looking results but ones that are very sub-optimal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_trained=tf.keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',input_tensor=None, input_shape=(96,96,3), pooling=None)\n",
    "\n",
    "#Load a pretrained model\n",
    "\n",
    "#set include_top to false this removes the dense layers at the end\n",
    "pre_trained=tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet',input_tensor=None, input_shape=(96,96,3), pooling=None)\n",
    "preprocessing=tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "\n",
    "# fix all these layers so we don't train them right away\n",
    "for l in pre_trained.layers:\n",
    "    l.trainable=False\n",
    "\n",
    "\n",
    "# Add our own new Dense Layers    \n",
    "flat=tf.keras.layers.Flatten()(pre_trained.output)\n",
    "top=tf.keras.layers.Dense(256)(flat)\n",
    "top=tf.keras.layers.LeakyReLU()(top)\n",
    "top=tf.keras.layers.Dropout(0.3)(top)\n",
    "\n",
    "classification=tf.keras.layers.Dense(1,activation='sigmoid')(top)\n",
    "\n",
    "# Model/compile like before\n",
    "\n",
    "\n",
    "model=tf.keras.models.Model([pre_trained.input],classification)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=1e-4),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make generators like before but now using the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True,\n",
    "            shear_range=1,\n",
    "            zoom_range=.05,\n",
    "            rotation_range=15                           \n",
    "            )  # randomly flip images\n",
    "\n",
    "train_generator=data_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/train',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=True)\n",
    "\n",
    "develop_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing,\n",
    "            width_shift_range=0,  # don't do anything but preprocess\n",
    "            height_shift_range=0,  \n",
    "            horizontal_flip=False,  \n",
    "            vertical_flip=False,\n",
    "            shear_range=0,\n",
    "            zoom_range=.00,\n",
    "            rotation_range=0                          \n",
    "            )  # randomly flip images\n",
    "\n",
    "develop_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/develop',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "\n",
    "test_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/test',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try_fit = False\n",
    "\n",
    "if try_fit:\n",
    "    history=model.fit_generator(train_generator,steps_per_epoch=1000,epochs=5,validation_data=develop_generator,validation_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if try_fit:\n",
    "    for l in pre_trained.layers:\n",
    "        l.trainable=True\n",
    "    \n",
    "    # You must! recompile after freezing layers\n",
    "    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=1e-4),metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(train_generator,steps_per_epoch=1000,epochs=10,validation_data=develop_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not try_fit:\n",
    "    model.load_weights('/projects/bgmp/shared/2019_ML_workshop/VGG16.png').expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.squeeze(develop_generator.classes)\n",
    "y_pred=np.squeeze(model.predict_generator(develop_generator))\n",
    "\n",
    "fine=[pred for pred,truth in zip(y_pred,y_true) if not truth  ]\n",
    "tumor=[pred for pred,truth in zip(y_pred,y_true) if  truth  ]\n",
    "\n",
    "plt.hist(tumor,range=(0,1),bins=200,density=True,histtype='step',label='tumor')\n",
    "plt.hist(fine,range=(0,1),bins=200,density=True,histtype='step',label='normal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_image(image_file,batch_size,x_range,y_range,preprocessing_function):\n",
    "    slide_image=openslide.OpenSlide(image_file)\n",
    "    res_x,res_y=slide_image.level_dimensions[2]\n",
    "    \n",
    "    coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "    #This is factor we need to scale the pixels at resolution 2 to the coordinates in resolution 0\n",
    "\n",
    "    sfactor_x=coord_x/res_x  \n",
    "    sfactor_y=coord_y/res_y \n",
    "    \n",
    "    batch=[]\n",
    "    index=0\n",
    "    for x in range(x_range[0]//4,x_range[1]//4,32):\n",
    "        for y in range(y_range[0]//4,y_range[1]//4,32):\n",
    "            image=np.asarray(slide_image.read_region( (int(x*sfactor_x),int(y*sfactor_y)),2,(96,96)  ))\n",
    "            batch.append( preprocessing_function(np.expand_dims(image[:,:,0:3],0))  )                                \n",
    "\n",
    "            if len(batch)==batch_size:\n",
    "                yield(np.concatenate(batch,0))\n",
    "                batch=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "\n",
    "width=6400\n",
    "height=6400\n",
    "\n",
    "x_start=60000\n",
    "y_start=120000\n",
    "\n",
    "x_stop=x_start+width*4\n",
    "y_stop=y_start+height*4\n",
    "\n",
    "\n",
    "\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "n_predictions=width//32*height//32\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "generator=scan_image('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif',batch_size,x_range,y_range,preprocessing)\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "output_scan=model.predict_generator(generator,steps)\n",
    "output_map=np.zeros((height//32,width//32))\n",
    "print(output_scan)\n",
    "for index,v in enumerate(output_scan):\n",
    "    \n",
    "    y=index%(width//32)\n",
    "    x=index//(width//32)\n",
    "    output_map[y,x]=v\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "plt.imshow(output_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Slides \n",
    "\n",
    "This slide with a tumor\n",
    "\n",
    "<img src=\"../assets/full_slide_scan_vgg16.png\">\n",
    "\n",
    "A slide with no tumor\n",
    "\n",
    "<img src=\"../assets/full_slide_scan_vgg16_normal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking a lot better\n",
    "\n",
    "Still some false positives along the slide/cell edges\n",
    "* We could try to harvest more of that data\n",
    "* Or we could institute a policy\n",
    "    * Don't use the model for cell on the edge of a slide\n",
    "* A human will make the diagnosis and the better the model the faster and more reliable it will be\n",
    "* How did our model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is the tumor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.xml')\n",
    "root = tree.getroot()\n",
    "tiff_file='/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif'\n",
    "slide_image=openslide.OpenSlide(tiff_file)\n",
    "tumor_coords=[(float(el.attrib['X']),float(el.attrib['Y'])) for el in root.findall('Annotations/Annotation/Coordinates/Coordinate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_x=[(x-x_start)/4./32.-2 for x,y in tumor_coords]\n",
    "tumor_y=[(y-y_start)/4./32.-2 for x,y in tumor_coords]\n",
    "f=plt.figure(figsize=(30,30))\n",
    "plt.imshow(output_map)\n",
    "plt.scatter(tumor_x,tumor_y)\n",
    "\n",
    "f=plt.figure(figsize=(30,30))\n",
    "plt.imshow(slide_image.read_region( (x_start,y_start),2,(height,width)))\n",
    "tumor_x=[(x-x_start)/4. for x,y in tumor_coords]\n",
    "tumor_y=[(y-y_start)/4. for x,y in tumor_coords]\n",
    "plt.scatter(tumor_x,tumor_y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We did highlight both tumors, but we also highlighed a different area of cells\n",
    "\n",
    "* Group Question, how would you try and fix this issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Can you do even better?\n",
    "\n",
    "Give it a try with a different pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for using existing models\n",
    "\n",
    "* Be careful not to 'blow up' existing well trained weights\n",
    "* Train the dense layers first classifier\n",
    "* Fine-Tune  other layers (use small learning rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go next - Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [U-Nets](https://arxiv.org/abs/1505.04597) - Pixel Labeling\n",
    "\n",
    "Example for  (CT) images - [Source](https://www.frontiersin.org/articles/10.3389/fgene.2019.01110/full)\n",
    "![image.png](https://www.frontiersin.org/files/Articles/492928/fgene-10-01110-HTML-r2/image_m/fgene-10-01110-g001.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection \n",
    "Malaria Detection - [Source](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04036-4)\n",
    "\n",
    "![image.png](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12859-021-04036-4/MediaObjects/12859_2021_4036_Fig3_HTML.png?as=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ilastick](https://www.ilastik.org/) -  the interactive learning and segmentation toolkit\n",
    "![image.jpg](https://www.ilastik.org/gallery/full/Figure-2-d.jpg)\n",
    "\n",
    "\n",
    "[QuPath](https://qupath.github.io/) - Quantitative Pathology & Bioimage Analysis\n",
    "\n",
    "![image.jpg](https://qupath.github.io/images/qupath_v0.2.0_larger.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
