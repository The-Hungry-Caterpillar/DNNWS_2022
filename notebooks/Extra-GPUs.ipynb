{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a GPU\n",
    "\n",
    "A quick reminder ! lets you call a terminal command in jupyter\n",
    "\n",
    "## Why GPUs\n",
    "\n",
    "* GPUs are fast! Esspecially when running code in parallel\n",
    "* But..\n",
    "    * Do have limitation, an important one is memory\n",
    "    \n",
    "# Some things to consider\n",
    "\n",
    "* Tensorflow as a cpu and a gpu version\n",
    "* This is also true of several other packages\n",
    "* We generally take care of that for you on Talapas\n",
    "    * You'll have to install the correct one when using your own hardware\n",
    "    \n",
    "\n",
    "A quick way to tell if you have the right GPU/Software installed is\n",
    "\n",
    "## nvida-smi\n",
    "This is a super useful command to see what and how many GPUs are on a system\n",
    "\n",
    "**note** This only goes for nvidia gpus, but most code you'll encounter is based on cuda (which requires a nvidia gpu)\n",
    "* There are some alternatives that we won't cover\n",
    "    * Open-cl\n",
    "    * ROCm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 16:30:41 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| N/A   48C    P0    82W / 149W |      0MiB / 12206MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "All GPUs have a fixed ammount of memory if your GPU runs out of memory it will crash\n",
    "* Larger Batch Sizes = More Memory\n",
    "\n",
    "On talapas you should see\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
    "| N/A   54C    P8    32W / 149W |      0MiB / 12206MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "```\n",
    "\n",
    "so far we've 0 of 12 GB of memory used on our k80 GPU, and it isn't processing anything (Volatile GPU-Util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000, 1000, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1000, 1000, 500)   2000      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 500)               0         \n",
      "=================================================================\n",
      "Total params: 2,000\n",
      "Trainable params: 2,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "be_nice=False\n",
    "\n",
    "if be_nice:\n",
    "    #Close the session we just made\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    tf.keras.backend.set_session(session)\n",
    "\n",
    "\n",
    "\n",
    "def build_silly_model():\n",
    "\n",
    "    big_input=tf.keras.layers.Input((1000,1000,3)) #~4Mb * 3 channel = 12mb image\n",
    "    big_cnn=tf.keras.layers.Conv2D(500,(1),padding='same')(big_input) #~500 images 4Mb an Image =2 GB\n",
    "    average=tf.keras.layers.GlobalAveragePooling2D()(big_cnn) # 500 Numbers ~ small in memory\n",
    "\n",
    "    silly_model=tf.keras.models.Model([big_input],[average])\n",
    "    silly_model.compile(loss='mse',optimizer='adam')\n",
    "    return silly_model\n",
    "\n",
    "silly_model=build_silly_model()\n",
    "print(silly_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 16:30:43 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| N/A   48C    P0    82W / 149W |      0MiB / 12206MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras hasn't put anything on the gpu yet\n",
    "\n",
    "# Make a prediction with our silly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size in fp64 24.0\n",
      "Image Size in fp32 12.0\n",
      "(1, 500)\n",
      "Output Size in fp32 0.002\n"
     ]
    }
   ],
   "source": [
    "input_image=np.ones((1,1000,1000,3))\n",
    "#1000*1000*8 bytes fp64\n",
    "print(\"Image Size in fp64\",input_image.size*input_image.itemsize/1e6)\n",
    "\n",
    "input_image=input_image.astype('float32')\n",
    "#1000*1000*4 bytes fp32\n",
    "print(\"Image Size in fp32\",input_image.size*input_image.itemsize/1e6)\n",
    "\n",
    "\n",
    "output_image=silly_model.predict(input_image,batch_size=1)\n",
    "print(output_image.shape)\n",
    "print(\"Output Size in fp32\",output_image.size*input_image.itemsize/1e6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 16:30:44 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| N/A   48C    P0   149W / 149W |  11664MiB / 12206MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    125605      C   ...envs/anaconda-tensorflow-gpu/bin/python 11651MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Usage \n",
    "\n",
    "One keras has to use the model, it creates a tensorflow session.\n",
    "\n",
    "1-process (this one) ends up using all! the avaliable GPU memory\n",
    "\n",
    "* This is a feature of tensorflow it will grab all the memory it can on all the gpus it can\n",
    "    * Even if your code dosen't use more than one gpu\n",
    "    \n",
    "\n",
    "    \n",
    "## Be a little bit nicer\n",
    "\n",
    "Restart the kernal and run again with `be_nice=True`\n",
    "You should see\n",
    "\n",
    "```                                                                        \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID   Type   Process name                             Usage      |\n",
    "|=============================================================================|\n",
    "|    0    113328      C   ...envs/anaconda-tensorflow-gpu/bin/python  8387MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "8 GBs? well a keras model is ready to fit your data, and creates activations that use more than just the layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1',\n",
       " 'conv2d/kernel/Initializer/random_uniform/shape',\n",
       " 'conv2d/kernel/Initializer/random_uniform/min',\n",
       " 'conv2d/kernel/Initializer/random_uniform/max',\n",
       " 'conv2d/kernel/Initializer/random_uniform/RandomUniform',\n",
       " 'conv2d/kernel/Initializer/random_uniform/sub',\n",
       " 'conv2d/kernel/Initializer/random_uniform/mul',\n",
       " 'conv2d/kernel/Initializer/random_uniform',\n",
       " 'conv2d/kernel',\n",
       " 'conv2d/kernel/IsInitialized/VarIsInitializedOp',\n",
       " 'conv2d/kernel/Assign',\n",
       " 'conv2d/kernel/Read/ReadVariableOp',\n",
       " 'conv2d/bias/Initializer/zeros',\n",
       " 'conv2d/bias',\n",
       " 'conv2d/bias/IsInitialized/VarIsInitializedOp',\n",
       " 'conv2d/bias/Assign',\n",
       " 'conv2d/bias/Read/ReadVariableOp',\n",
       " 'conv2d/dilation_rate',\n",
       " 'conv2d/Conv2D/ReadVariableOp',\n",
       " 'conv2d/Conv2D',\n",
       " 'conv2d/BiasAdd/ReadVariableOp',\n",
       " 'conv2d/BiasAdd',\n",
       " 'global_average_pooling2d/Mean/reduction_indices',\n",
       " 'global_average_pooling2d/Mean',\n",
       " 'Adam/iterations/Initializer/initial_value',\n",
       " 'Adam/iterations',\n",
       " 'Adam/iterations/IsInitialized/VarIsInitializedOp',\n",
       " 'Adam/iterations/Assign',\n",
       " 'Adam/iterations/Read/ReadVariableOp',\n",
       " 'Adam/lr/Initializer/initial_value',\n",
       " 'Adam/lr',\n",
       " 'Adam/lr/IsInitialized/VarIsInitializedOp',\n",
       " 'Adam/lr/Assign',\n",
       " 'Adam/lr/Read/ReadVariableOp',\n",
       " 'Adam/beta_1/Initializer/initial_value',\n",
       " 'Adam/beta_1',\n",
       " 'Adam/beta_1/IsInitialized/VarIsInitializedOp',\n",
       " 'Adam/beta_1/Assign',\n",
       " 'Adam/beta_1/Read/ReadVariableOp',\n",
       " 'Adam/beta_2/Initializer/initial_value',\n",
       " 'Adam/beta_2',\n",
       " 'Adam/beta_2/IsInitialized/VarIsInitializedOp',\n",
       " 'Adam/beta_2/Assign',\n",
       " 'Adam/beta_2/Read/ReadVariableOp',\n",
       " 'Adam/decay/Initializer/initial_value',\n",
       " 'Adam/decay',\n",
       " 'Adam/decay/IsInitialized/VarIsInitializedOp',\n",
       " 'Adam/decay/Assign',\n",
       " 'Adam/decay/Read/ReadVariableOp',\n",
       " 'global_average_pooling2d_target',\n",
       " 'Const',\n",
       " 'global_average_pooling2d_sample_weights',\n",
       " 'loss/global_average_pooling2d_loss/sub',\n",
       " 'loss/global_average_pooling2d_loss/Square',\n",
       " 'loss/global_average_pooling2d_loss/Mean/reduction_indices',\n",
       " 'loss/global_average_pooling2d_loss/Mean',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/weights/shape',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/weights/rank',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/values/shape',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/values/rank',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_scalar/x',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_scalar',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/switch_t',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/switch_f',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/pred_id',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/is_same_rank/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/switch_t',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/switch_f',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/pred_id',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims/dim',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ones_like/Shape',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ones_like/Const',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ones_like',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/axis',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1/dim',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/num_invalid_dims',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/x',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/Merge',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/is_valid_shape/Merge',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const_2',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const_3',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const_4',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/Const_5',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/switch_t',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/switch_f',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/pred_id',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/NoOp',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/control_dependency',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_0',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_2',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_4',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_5',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/data_7',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_3',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/control_dependency_1',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/assert_broadcastable/AssertGuard/Merge',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/ones_like/Shape',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/ones_like/Const',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights/ones_like',\n",
       " 'loss/global_average_pooling2d_loss/broadcast_weights',\n",
       " 'loss/global_average_pooling2d_loss/Mul',\n",
       " 'loss/global_average_pooling2d_loss/Const',\n",
       " 'loss/global_average_pooling2d_loss/Sum',\n",
       " 'loss/global_average_pooling2d_loss/Const_1',\n",
       " 'loss/global_average_pooling2d_loss/Sum_1',\n",
       " 'loss/global_average_pooling2d_loss/truediv',\n",
       " 'loss/global_average_pooling2d_loss/zeros_like',\n",
       " 'loss/global_average_pooling2d_loss/Greater',\n",
       " 'loss/global_average_pooling2d_loss/Select',\n",
       " 'loss/global_average_pooling2d_loss/Const_2',\n",
       " 'loss/global_average_pooling2d_loss/Mean_1',\n",
       " 'loss/mul/x',\n",
       " 'loss/mul',\n",
       " 'group_deps',\n",
       " 'VarIsInitializedOp',\n",
       " 'VarIsInitializedOp_1',\n",
       " 'VarIsInitializedOp_2',\n",
       " 'VarIsInitializedOp_3',\n",
       " 'VarIsInitializedOp_4',\n",
       " 'VarIsInitializedOp_5',\n",
       " 'VarIsInitializedOp_6',\n",
       " 'init']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out all the tensors the keras model defined\n",
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia-smi is still a great way to check total memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 16:30:45 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| N/A   48C    P0    93W / 149W |  11664MiB / 12206MiB |     19%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    125605      C   ...envs/anaconda-tensorflow-gpu/bin/python 11651MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#What Happens When You Run out of Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size in fp64 240.0\n",
      "Image Size in fp32 240.0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,500,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d/Conv2D/ReadVariableOp)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b1cd3dc2a1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilly_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output Size in fp32\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/packages/miniconda/20190102/envs/anaconda-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1878\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/packages/miniconda/20190102/envs/anaconda-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/packages/miniconda/20190102/envs/anaconda-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/packages/miniconda/20190102/envs/anaconda-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/packages/miniconda/20190102/envs/anaconda-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,500,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d/Conv2D/ReadVariableOp)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_image=np.ones((20,1000,1000,3)).astype('float32')\n",
    "#1000*1000*8 bytes fp64\n",
    "print(\"Image Size in fp64\",input_image.size*input_image.itemsize/1e6)\n",
    "\n",
    "input_image=input_image.astype('float32')\n",
    "#1000*1000*4 bytes fp32\n",
    "print(\"Image Size in fp32\",input_image.size*input_image.itemsize/1e6)\n",
    "\n",
    "\n",
    "output_image=silly_model.predict(input_image,batch_size=20)\n",
    "print(output_image.shape)\n",
    "print(\"Output Size in fp32\",output_image.size*input_image.itemsize/1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unless you're running on a giant future GPU you'll see\n",
    "```\n",
    "ResourceExhaustedError: OOM when allocating tensor with shape[20,500,1000,1000]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works fine since the batch size is small\n",
    "output_image=silly_model.predict(input_image,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of memory use a smaller batch size\n",
    "* Still out of memory? \n",
    "    * Try using fp16 (dosen't always work)\n",
    "    * Change your model     \n",
    "    * Try gradient checkpointing https://github.com/cybertronai/gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed\n",
    "Let's try a new smaller  model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_small_model():\n",
    "    big_input=tf.keras.layers.Input((100,100,3)) #~4Mb * 3 channel = 12mb image\n",
    "    big_cnn=tf.keras.layers.Conv2D(500,(2,2),padding='same')(big_input) #~500 images 4Mb an Image =2 GB\n",
    "    average=tf.keras.layers.GlobalAveragePooling2D()(big_cnn) # 500 Numbers ~ small in memory\n",
    "\n",
    "    silly_model=tf.keras.models.Model([big_input],[average])\n",
    "    silly_model.compile(loss='mse',optimizer='adam')\n",
    "    return silly_model\n",
    "\n",
    "\n",
    "gpu_model=build_small_model() #GPU is default\n",
    "\n",
    "with tf.device(\"cpu\"):\n",
    "    cpu_model=build_small_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 1 CPU  time 8.492326498031616 Seconds\n",
      "Batch Size 50 CPU  time 12.380130767822266 Seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "input_image=np.ones((500,100,100,3)).astype('float32')\n",
    "\n",
    "# With CPU\n",
    "\n",
    "itime=time()\n",
    "output_image=cpu_model.predict(input_image,batch_size=1)\n",
    "print(\"Batch Size 1 CPU  time\",time()-itime,\"Seconds\")\n",
    "\n",
    "itime=time()\n",
    "output_image=cpu_model.predict(input_image,batch_size=64)\n",
    "print(\"Batch Size 50 CPU  time\",time()-itime,\"Seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 1 GPU  time 1.003133773803711 Seconds\n",
      "Batch Size 50 GPU  time 1.099949598312378 Seconds\n"
     ]
    }
   ],
   "source": [
    "# With GPU\n",
    "itime=time()\n",
    "output_image=gpu_model.predict(input_image,batch_size=1)\n",
    "print(\"Batch Size 1 GPU  time\",time()-itime,\"Seconds\")\n",
    "\n",
    "itime=time()\n",
    "output_image=gpu_model.predict(input_image,batch_size=64)\n",
    "print(\"Batch Size 50 GPU  time\",time()-itime,\"Seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# In a seperate terimal try running\n",
    "\n",
    "```watch -n 0.5 nvidia-smi```\n",
    "\n",
    "run the code below (same gpu code again with more data) and see what the terminal output is. You should see something like\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                  Off |\n",
    "| N/A   54C    P8   148W / 149W |  11664MiB / 12206MiB |      98%     Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "```\n",
    "the Volatile GPU-Util shows 98% which tells you that the GPU is being (almost) fully utilized. \n",
    "\n",
    "You might notice with a batch size of 1 the Volatile GPU-Util is lower, which means it is not being fully utilized. \n",
    "In this case you'll get a bit better performance with a larger batch size (if it will fit in memory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 1 GPU  time 8.12438678741455 Seconds\n",
      "Batch Size 50 GPU  time 5.824122905731201 Seconds\n"
     ]
    }
   ],
   "source": [
    "# Again with more data so you can watch\n",
    "\n",
    "input_image=np.ones((5000,100,100,3)).astype('float32')\n",
    "\n",
    "itime=time()\n",
    "output_image=gpu_model.predict(input_image,batch_size=1)\n",
    "print(\"Batch Size 1 GPU  time\",time()-itime,\"Seconds\")\n",
    "\n",
    "itime=time()\n",
    "output_image=gpu_model.predict(input_image,batch_size=64)\n",
    "print(\"Batch Size 50 GPU  time\",time()-itime,\"Seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* GPUs are very powerful for deep learning, but come with memory constraints\n",
    "* nvidia-smi is you friend for understanding what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
