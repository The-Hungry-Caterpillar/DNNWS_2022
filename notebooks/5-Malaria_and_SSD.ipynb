{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Classification can tell you what object you are seeing but you may also need to answer:\n",
    "* Where is an object?\n",
    "* How many are there?\n",
    "\n",
    "We generally call this object detection\n",
    "\n",
    "# Outline\n",
    "\n",
    "\n",
    "\n",
    "* Our Goal for this lecture is to identify signs of Malaria in human blood smears\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Data Set\n",
    "\n",
    "*P. vivax* (malaria) infected human blood smears\n",
    "\n",
    "Accession number BBBC041 Â· Version 1\n",
    "\n",
    "\n",
    "\n",
    "From the Broad Institute https://data.broadinstitute.org/bbbc/BBBC041/\n",
    "\n",
    "\n",
    "<img src=https://data.broadinstitute.org/bbbc/BBBC041/BBBC041_example1.png>\n",
    "    \n",
    "Malaria is a disease caused by *Plasmodium* parasites that remains a major threat in global health, affecting 200 million people and causing 400,000 deaths a year. The main species of malaria that affect humans are *Plasmodium falciparum* and *Plasmodium vivax*.\n",
    "\n",
    "For malaria as well as other microbial infections, manual inspection of thick and thin blood smears by trained microscopists remains the gold standard for parasite detection and stage determination because of its low reagent and instrument cost and high flexibility. Despite manual inspection being extremely low throughput and susceptible to human bias, automatic counting software remains largely unused because of the wide range of variations in brightfield microscopy images. However, a robust automatic counting and cell classification solution would provide enormous benefits due to faster and more accurate quantitative results without human variability; researchers and medical professionals could better characterize stage-specific drug targets and better quantify patient reactions to drugs.\n",
    "\n",
    "Previous attempts to automate the process of identifying and quantifying malaria have not gained major traction partly due to difficulty of replication, comparison, and extension. Authors also rarely make their image sets available, which precludes replication of results and assessment of potential improvements. The lack of a standard set of images nor standard set of metrics used to report results has impeded the field. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinates \n",
    "\n",
    "We are going to use the index coordinate system\n",
    "\n",
    "<img src=../assets/index_coords.png>\n",
    "\n",
    "* **Packages often have different systems so be careful!**\n",
    "    \n",
    "\n",
    "## Bounding Boxes\n",
    "\n",
    "Data is often labeled by bounding boxes, which also can be describe in a number of different ways. We'll use \n",
    "Upper left corner (note this is the smallest y-coordinate), the length and the width. \n",
    "\n",
    "<img src=../assets/BBox.png>\n",
    "\n",
    "Each bounding box also has a category attached to it. For example moon or cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bbtoydata import BBToyData\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pixel=0\n",
    "y_pixel=1\n",
    "\n",
    "\n",
    "\n",
    "img=np.zeros((3,3))\n",
    "\n",
    "img[y_pixel,x_pixel]=1.0## Notice y,x here \n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(img,cmap='gray')\n",
    "\n",
    "plt.xlabel(r\" X -0.5 $\\rightarrow$ 3.5 \")\n",
    "plt.ylabel(r\" 2.5 $\\leftarrow$ -0.5  Y \")\n",
    "print(img)\n",
    "\n",
    "\n",
    "##Draw a bounding box\n",
    "rec_corner=((x_pixel-0.5,y_pixel-0.5)) #half a pixel Width\n",
    "width=1\n",
    "height=1\n",
    "# We'll uses pat\n",
    "rec=patches.Rectangle(rec_corner, width,height,edgecolor='r',facecolor='none')\n",
    "ax.add_patch(rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add a lot of bounding boxes so let's write a function to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test\n",
    "Draw a 2x2 box in the lower left hand corner of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be drawing a lot of boxes so lets  make it a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bbox(ax,bbox,color='r'):\n",
    "    corner_x,corner_y,width,height=bbox\n",
    "    rec=patches.Rectangle((corner_x-0.5,corner_y-0.5), width,height,edgecolor=color,facecolor='none')\n",
    "    ax.add_patch(rec)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying where something is with a neural network\n",
    "\n",
    "Again, we'll use a toy example that's easy to under and visualize.\n",
    "\n",
    "Start by finding just one object.\n",
    "\n",
    "* 3-Object Classes \n",
    "    * Square\n",
    "    * Circle\n",
    "    * Triangle\n",
    "* 1 - Background Class\n",
    "\n",
    "* 4 - bbox x, y, width, height\n",
    "\n",
    "We need a classification network to identify the shape, and a new network go predict the bounding box\n",
    "\n",
    "* We'll use two prediction networks that share features!\n",
    "\n",
    "<img src=../assets/network_diagrams/1_object_identifier.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data\n",
    "\n",
    "We'll use a set of images that contain either a square, circle, triangle, or nothing.\n",
    "\n",
    "* Goal will be to identify the object, and draw a bounding box around it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=BBToyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_example(image,labels):\n",
    "    colors=['r','g','b','y','#000000']\n",
    "    fig,ax = plt.subplots(1,figsize=(10,10))\n",
    "    ax.imshow(image,cmap='gray')\n",
    "    for cat,bbox in labels:\n",
    "        add_bbox(ax,bbox,color=colors[cat])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    index=np.random.randint(bb.X_train.shape[0])\n",
    "    print(bb.Y_train[index])\n",
    "\n",
    "    plot_example(np.squeeze(bb.X_train[index]),(bb.Y_train[index]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our Convolutional Feature Finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_input=tf.keras.layers.Input( shape=bb.X_train.shape[1:] ) # Shape here does not including the batch size \n",
    "cnn_layer1=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_input) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "\n",
    "cnn_layer2=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_activation) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "\n",
    "#cnn_layer3=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_activation) \n",
    "#cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "\n",
    "flat=tf.keras.layers.Flatten()(cnn_activation) \n",
    "flat=tf.keras.layers.Dropout(0.5)(flat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Our Prediction layer\n",
    "cat_output_layer=tf.keras.layers.Dense(4)(flat) \n",
    "cat_output=tf.keras.layers.Activation('softmax')(cat_output_layer)\n",
    "\n",
    "\n",
    "#Make our bounding box regressor\n",
    "bbox_output=tf.keras.layers.Dense(50)(flat) \n",
    "bbox_output=tf.keras.layers.LeakyReLU()(bbox_output)\n",
    "bbox_output=tf.keras.layers.Dense(50)(bbox_output) \n",
    "bbox_output=tf.keras.layers.LeakyReLU()(bbox_output)\n",
    "bbox_output=tf.keras.layers.Dense(4)(bbox_output) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Model(cnn_input,[cat_output,bbox_output])\n",
    "\n",
    "\n",
    "##Two!! loss fuctions one for each output, the total loss is the sum of each of these\n",
    "model.compile(loss=['categorical_crossentropy','mae'],\n",
    "              optimizer='adam',\n",
    "              metrics=[])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make our Data\n",
    "We need to transform labels from a list to a one hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_single_object_labels(labels):\n",
    "    Y_bbox=[]\n",
    "    Y_cat=[]\n",
    "    for l in labels:\n",
    "        if len(l)==0:\n",
    "            Y_cat.append(3) #Background\n",
    "            Y_bbox.append(np.array([[0,0,1,1]])) #Divide by our image size so everything is between 0-1\n",
    "        else:\n",
    "            cat,bbox=l[0]\n",
    "            Y_cat.append(cat)\n",
    "            Y_bbox.append(np.expand_dims(bbox,0)/50,) #Divide by our image size so everything is between 0-1\n",
    "    Y_cat=tf.keras.utils.to_categorical(Y_cat, num_classes=4)\n",
    "    Y_bbox=np.concatenate(Y_bbox)\n",
    "    return [Y_cat,Y_bbox]\n",
    "\n",
    "Y_train=get_single_object_labels(bb.Y_train)\n",
    "Y_develop=get_single_object_labels(bb.Y_develop)\n",
    "Y_test=get_single_object_labels(bb.Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history=model.fit(bb.X_train, Y_train, \n",
    "          batch_size=32, epochs=25, verbose=1,\n",
    "         validation_data=(bb.X_develop,Y_develop)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_pred,bbox_pred=model.predict(bb.X_develop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):    \n",
    "    cat=np.argmax(cat_pred[i])\n",
    "    label=[ [cat,bbox_pred[i]*50]]\n",
    "    plot_example(np.squeeze(bb.X_develop[i]),label)\n",
    "    print(cat,bbox_pred[i]*100)\n",
    "    print(bb.Y_develop[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying more than one thing at a time\n",
    "\n",
    "**Run the next several cells to get your model training before we get started**\n",
    "\n",
    "\n",
    "That worked pretty well, but we have another problem - what if there is more than one object in the image? \n",
    "* There are several ways to handle this:\n",
    "    * Scan the image (like we did with the cancer example)\n",
    "        * This works but can be expensive\n",
    "    * Use multiple detectors\n",
    "        * Strategry used by algorithms like SSD (single shot detector) or YOLO (you only look once)\n",
    "        * Faster and better suited for devices\n",
    "* Will discuss using multiple detectors\n",
    "    * The algorithm above had 1 detector, and predicted one bounding box and one class label\n",
    "    * We can add more of them to detect several objects\n",
    "* Challenges\n",
    "    * How do we efficiently add detectors?\n",
    "    * How do we assign objects to detectors?\n",
    "\n",
    "* Convolutional Detectors\n",
    "    * One way to add a lot of detectors quickly is to not use a dense layer at all\n",
    "        * We replace the dense network above with two convolutional layers\n",
    "        * Each layer will have 4 filters (so the output will have 4 channels)\n",
    "            * We can apply a softmax to each pixel in one layer to get a category prediction\n",
    "            * We can use the second layer for bounding box predictions\n",
    "        \n",
    "    \n",
    "\n",
    "<img src=../assets/network_diagrams/multi_object_identifier.png>\n",
    "\n",
    "In the case below our output CNN layers will have a shape of 5 pixels by 5 pixels,\n",
    "so we we'll assign each of these pixels to watch a box in the original input image.\n",
    "The area each pixel is watching for is called a prior box or an anchor. \n",
    "\n",
    "If the center of an object in in the box we assign it to that detector, otherwise a detector is assigned a background class.\n",
    "\n",
    "<img src=../assets/network_diagrams/Detectors.png>\n",
    "\n",
    "Another detail is instead of directly predicting the bounding box with make a prediction with respect to the prior box, we predict the x,y offsets to the new bounding box and the length and width scales.  \n",
    "\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build It\n",
    "Same problem now with the possiblity of multiple objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_multi=BBToyData(multi_object= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(np.squeeze(bb_multi.X_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_input=tf.keras.layers.Input( shape=bb_multi.X_train.shape[1:] ) # Shape here does not including the batch size \n",
    "cnn_layer1=tf.keras.layers.Convolution2D(16, (4,4),strides=2,padding='same')(cnn_input) #50x50\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "#cnn_activation=tf.keras.layers.BatchNormalization()(cnn_activation) \n",
    "#cnn_activation=tf.keras.layers.Dropout(0.5)(cnn_activation) \n",
    "\n",
    "cnn_layer2=tf.keras.layers.Convolution2D(32, (4,4),strides=2,padding='same')(cnn_activation) #25x25\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "#cnn_activation=tf.keras.layers.Dropout(0.5)(cnn_activation) \n",
    "\n",
    "#cnn_activation=tf.keras.layers.BatchNormalization()(cnn_activation) \n",
    "\n",
    "cnn_layer3=tf.keras.layers.Convolution2D(64, (10,10),strides=5,padding='same')(cnn_activation) #5x5\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "#cnn_activation=tf.keras.layers.BatchNormlization()(cnn_activation) \n",
    "#cnn_activation=tf.keras.layers.Dropout(0.5)(cnn_activation) \n",
    "\n",
    "\n",
    "\n",
    "###Prediction Layers\n",
    "prediction=tf.keras.layers.Convolution2D(4,(5,5),padding='same',activation='softmax')(cnn_activation)\n",
    "#Softmax is applied to the last \n",
    "bbox_offset=tf.keras.layers.Convolution2D(4,(5,5),padding='same')(cnn_activation)\n",
    "\n",
    "\n",
    "model=tf.keras.models.Model(cnn_input,[prediction,bbox_offset])\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','mse'],\n",
    "                  optimizer='adam',\n",
    "                  metrics=[])\n",
    "model.summary()\n",
    "\n",
    "test,test1=model.predict(bb_multi.X_train[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_labels(input_labels):\n",
    "    Y_train=[[],[]]\n",
    "    \n",
    "    for image_labels in input_labels:\n",
    "        class_features=np.zeros((5,5,4))\n",
    "        class_features[:,:,3]=1. #Defaults to no class\n",
    "\n",
    "        bbox_features=np.zeros((5,5,4))\n",
    "        bbox_features[:,:,2:]=1. #Scale =1\n",
    "\n",
    "        detected=[]\n",
    "        \n",
    "        \n",
    "        for i,(cat,bbox) in enumerate(image_labels):\n",
    "            bbox=[i/100. for i in bbox] # Normalize\n",
    "            bbox_center_x=int((bbox[0]+bbox[2]/2.)//0.2) #priorbox bin\n",
    "            bbox_center_y=int((bbox[1]+bbox[3]/2.)//0.2) #priorbox bin\n",
    "            \n",
    "            bbox_offset_x=(bbox[0]+bbox[2]/2.)/0.2-(bbox_center_x+0.5)#priorbox bin\n",
    "            bbox_offset_y=(bbox[1]+bbox[3]/2.)/0.2-(bbox_center_y+0.5) #priorbox bin\n",
    "            \n",
    "            bbox_scale_x=bbox[2]/0.2\n",
    "            bbox_scale_y=bbox[3]/0.2\n",
    "            # Y comes first here\n",
    "            class_features[bbox_center_y,bbox_center_x,cat]=1.\n",
    "            class_features[bbox_center_y,bbox_center_x,3]=0\n",
    "\n",
    "            bbox_features[bbox_center_y,bbox_center_x,:]=[bbox_offset_x,bbox_offset_y,bbox_scale_x,bbox_scale_y]\n",
    "        Y_train[0].append(np.expand_dims(class_features,0))\n",
    "        Y_train[1].append(np.expand_dims(bbox_features,0))\n",
    "    Y_train[0]=np.concatenate(Y_train[0])\n",
    "    Y_train[1]=np.concatenate(Y_train[1])\n",
    "    \n",
    "    return Y_train\n",
    "Y_train=get_labels(bb_multi.Y_train)\n",
    "Y_develop=get_labels(bb_multi.Y_develop)\n",
    "Y_test=get_labels(bb_multi.Y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plt.imshow(np.argmax(Y_train[0][0],axis=-1))\n",
    "bbox=Y_train[1][0]\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    index=np.random.randint(len(bb_multi.X_train))\n",
    "    plot_example(np.squeeze(bb_multi.X_train[index]),bb_multi.Y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bb_multi.X_train))\n",
    "history=model.fit(bb_multi.X_train, Y_train, \n",
    "        batch_size=32, epochs=15, verbose=1,\n",
    "        validation_data=(bb_multi.X_develop,Y_develop)\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']),plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_labels(image,cat_map,bbox_map,show_background=False):\n",
    "    # Each map is a 5x5 image \n",
    "    labels=[]\n",
    "    for y,r in enumerate(bbox_map):\n",
    "        for x,v in enumerate(r):\n",
    "            cat=np.argmax(cat_map[y,x,:])\n",
    "#            if cat_map[y,x,cat] < 0.9:continue\n",
    "            if not show_background and cat==3:continue\n",
    "            #Center - l/2\n",
    "            width=.2*bbox_map[y,x,2]\n",
    "            height=.2*bbox_map[y,x,3]\n",
    "            if cat !=3:\n",
    "                print(bbox_map[y,x,:])\n",
    "                print(cat_map[y,x,:])\n",
    "\n",
    "            x_start=(x+bbox_map[y,x,0]+.5)*.2 -width/2.\n",
    "            y_start=(y+bbox_map[y,x,1]+.5)*.2 -height/2.\n",
    "            labels.append([cat,[(x_start)*100,y_start*100,width*100,height*100]])\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "cat_map,bbox_map=model.predict(bb_multi.X_develop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    labels=predictions_to_labels(np.squeeze(bb_multi.X_develop[i]),cat_map[i],bbox_map[i],show_background=True)\n",
    "\n",
    "#    labels=predictions_to_labels(np.squeeze(bb_multi.X_develop[i]),Y_develop[0][i],Y_develop[1][i],show_background=True)\n",
    "\n",
    "    plot_example(np.squeeze(bb_multi.X_develop[i]),labels)\n",
    "\n",
    "    print('Predicted Labels')\n",
    "    plt.imshow(np.hstack([cat_map[i,:,:,n] for n in range(4)]))\n",
    "    plt.show()\n",
    "\n",
    "    print('True Labels')\n",
    "    plt.imshow(np.hstack([Y_develop[0][i,:,:,n] for n in range(4)]))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fullly Functional Algorigthm\n",
    "\n",
    "There are still problems with the simple model above\n",
    "* What happens if more than one object matches a box?\n",
    "* You might see problems if the objects are really big\n",
    "* You might also see problems if the objects are really smalle\n",
    "\n",
    "The best object detection algorithm use several prior boxes per location: i.e SSD\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/974/1*51joMGlhxvftTxGtA4lA7Q.png\">\n",
    "\n",
    "* We used one convolutional map that was 5x5\n",
    "* These algorithms use several maps with different sizes (for smaller and larger objects)\n",
    "* We only used square prior boxes\n",
    "     * These algorithms use several aspect ratio bounding boxes per point\n",
    "     \n",
    "**These are well engineered, and can take some time to reproduce, so we'll use an existing implementation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model is a good example of how everything we've talked about can be rolled to a one analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't be a hero part2\n",
    "\n",
    "Use an existing package from github\n",
    "https://github.com/pierluigiferrari/ssd_keras\n",
    "\n",
    "* Fairly Normal Open Source Package\n",
    "* Limited Docs\n",
    "* Some Examples\n",
    "* Not Super Easy to use\n",
    "    * I'll try to point out where what you learned above will help you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('./ssd_keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to design a system to help with malaria diagnosis\n",
    "   * We have blood smear slides labeled with bounding boxes\n",
    "   * Each bounding box identifies an infected cell and its stage of development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict={}\n",
    "cat_dict[\"background\"]=0\n",
    "cat_dict[\"ring\"]=1\n",
    "cat_dict['trophozoite']=2\n",
    "cat_dict['schizont']=3\n",
    "cat_dict['gametocyte']=4\n",
    "\n",
    "int_2_cat={}\n",
    "for i,v in cat_dict.items():\n",
    "    int_2_cat[v]=i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ring stage example\n",
    "<img src=\"https://www.mcdinternational.org/trainings/malaria/english/DPDx5/images/ParasiteImages/M-R/Malaria/falciparum/Pfal-rings-atlasdx.JPG\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Annotations come in a json format\n",
    "* These generally look like dictionaries to python users\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is stored on Talapas\n",
    "images_dir=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/\"\n",
    "annotations_train=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/train.json\"\n",
    "annotations_develop=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/develop.json\"\n",
    "annotations_test=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_label=json.load(open(annotations_train))\n",
    "\n",
    "\n",
    "print(train_label.keys())\n",
    "print(\"Image Data\",train_label['images'][0])\n",
    "\n",
    "print(\"Annotations 0\",train_label['annotations'][0])\n",
    "print(\"Annotations 1\",train_label['annotations'][1])\n",
    "print(\"Annotations 2\",train_label['annotations'][2])\n",
    "print(\"Annotations 3\",train_label['annotations'][3])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the list of annotations is matched to images using the tag image_id\n",
    "* This is informational, but our package knows how to read these files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Working With Bounding Boxes\n",
    "Use the lists above to answer the following questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question How Many Annotations Are in the Train Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Box Has The Largest Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important data checks\n",
    "\n",
    "all_bboxs=[ anno['bbox'] for anno in train_label['annotations']]\n",
    "all_classes=[ anno['category_id'] for anno in train_label['annotations']]\n",
    "\n",
    "for select in range(1,5):\n",
    "    widths=[bb[2] for bb,cat in zip(all_bboxs,all_classes) if cat == select  ] \n",
    "    heights=[bb[3] for bb,cat in zip(all_bboxs,all_classes) if cat == select ] \n",
    "    areas=[bb[3]*bb[2] for bb,cat in zip(all_bboxs,all_classes) if cat == select  ] \n",
    "\n",
    "    print( len(widths), int_2_cat[select], \" in training set\" )\n",
    "    print(\"Average Width\", np.mean(widths),'pixels')\n",
    "    print(\"Average Height\", np.mean(heights),'pixels')\n",
    "\n",
    "    print(\"Max Width\", np.max(widths),'pixels')\n",
    "    print(\"Max Height\", np.max(heights),'pixels')\n",
    "\n",
    "    print(\"Min Width\", np.min(widths),'pixels')\n",
    "    print(\"Min Height\", np.min(heights),'pixels')\n",
    "\n",
    "    print(\"__________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our targets are mostly trophozites \n",
    "* Our targets are fairly square \n",
    "* Our targets are ~70-255 pixels in size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Our Model - Part 1 Data\n",
    "Lots of imports for our package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "\n",
    "\n",
    "#from models.keras_ssd7 import build_model\n",
    "from models.keras_ssd300 import ssd_300 as build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember how we needed to properly preprocess the data for existing packages?\n",
    "This one has custom data generators for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 1200 # Height of the input images\n",
    "img_width = 1600 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "image_augmentation=SSDDataAugmentation(img_height,img_width)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.parse_json(        images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_train],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "develop_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "\n",
    "\n",
    "develop_dataset.parse_json(       images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_develop],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n",
    "\n",
    "test_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "test_dataset.parse_json(         images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_test],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_generator =    train_dataset.generate(batch_size=10,\n",
    "                                         shuffle=False,\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and draw example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Grab a batch of images\n",
    "\n",
    "batch_images, batch_labels, batch_filenames = next(plot_generator)\n",
    "print(len(batch_images))\n",
    "#These labels are in corner representation (xmin,ymin) (xmax,ymax)\n",
    "# Need to convert to (xmin,ymin),(lenth,width)\n",
    "\n",
    "def convert_ssd_labels(label):\n",
    "    if len(label)==5:\n",
    "        return [int(label[0]),[label[1],label[2],label[3]-label[1],label[4]-label[2]]] \n",
    "    if len(label)==6: #This are labels that also include a prediction\n",
    "        return [int(label[0]),[label[2],label[3],label[4]-label[2],label[5]-label[3]]] \n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    labels=[convert_ssd_labels(l) for l in batch_labels[i]]\n",
    "    print(labels)\n",
    "    plot_example(batch_images[i],labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model - Part 2 The Model\n",
    "* Step one Panic!\n",
    "* Step two stay calm and follow the default configurations (I pulled these from a notebook example)\n",
    "\n",
    "* Important things you need to know (from above)\n",
    "    * Your CNN layers make as many predictions as there output maps\n",
    "    * Your training annotations needed to be properly encoded\n",
    "    * You eventually need to decode the training labels\n",
    "* This is the same thing you did before, but now with more layers\n",
    "** The Really Important Part** All these settings, scales, aspect, ratios, etc., Must match. The setting for the Model must be the same as the encoder and the decoder\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "n_classes=4\n",
    "\n",
    "scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color)\n",
    "\n",
    "\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=5\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[image_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "develop_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "_1,_2=next(train_generator)\n",
    "\n",
    "print(_2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conv4_3_norm_mbox_conf (Conv2D) (None, 150, 200, 20) 92180       conv4_3_norm[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "fc7_mbox_conf (Conv2D)          (None, 75, 100, 30)  276510      fc7[0][0]                        \n",
    "__________________________________________________________________________________________________\n",
    "conv6_2_mbox_conf (Conv2D)      (None, 38, 50, 30)   138270      conv6_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "conv7_2_mbox_conf (Conv2D)      (None, 19, 25, 30)   69150       conv7_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "conv8_2_mbox_conf (Conv2D)      (None, 17, 23, 20)   46100       conv8_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "conv9_2_mbox_conf (Conv2D)      (None, 15, 21, 20)   46100       conv9_2[0][0]                    \n",
    "_________________________________________________________________________________________________\n",
    "input_1 (InputLayer)            (None, 1200, 1600, 3) 0                            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors at different resolution\n",
    "### Boxes layer size\n",
    "\n",
    "4 boxes per pixel * 4 predictions per box = 16 measurements\n",
    "### Boxes layer size\n",
    "\n",
    "5 (4+1 background) classes per box = 4*5=20\n",
    "\n",
    "| Map Size      |Pixels in original image per prediction feature pixel|\n",
    "| ------------- |-------------|\n",
    "|150x200 |  8x8   |\n",
    "|75x100  |  16x16 |\n",
    "|38x50   |  ~32x32 |\n",
    "|19x25   |  ~64x64 |\n",
    "|17x23   |  ~64x64 |\n",
    "|15x21   |  ~64x64 |\n",
    "\n",
    "\n",
    "**One thing to notice immedialte is that our 'biggest' detector starts out at 64x64 pixels**\n",
    "* What was the average size of our objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd300.h5\")\n",
    "\n",
    "if False:\n",
    "\n",
    "    final_epoch     = 500\n",
    "    \n",
    "    history = model.fit_generator(generator=train_generator,\n",
    "                              epochs=final_epoch,\n",
    "                              steps_per_epoch=len(train_dataset.images)//batch_size,\n",
    "                              validation_data=develop_generator,\n",
    "                              validation_steps=len(develop_dataset.images)//batch_size,\n",
    "                              )\n",
    "\n",
    "    #model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd300.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'processed_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_images,batch_labels=next(example_generator)\n",
    "y_pred = model.predict(batch_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                       confidence_thresh=0.3,\n",
    "                                       iou_threshold=0.1,\n",
    "                                       top_k=200,\n",
    "                                       normalize_coords=True,#normalize_coords,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    labels=[convert_ssd_labels(l) for l in y_pred_decoded[i]]\n",
    "    plot_example(batch_images[i],labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets evaluate the model and do some experiments\n",
    "\n",
    "We are far from perfect, but lets evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Model\n",
    "\n",
    "How good is it? An important part of these algorithms is evaluating the quality of matches. \n",
    "\n",
    "## Intersect Over Union\n",
    "Intersect over union\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png>\n",
    "\n",
    "This is a very useful quanity\n",
    "\n",
    "* 1 if the agreement is perfect, 0 if the boxes don't overlap\n",
    "* Some entries may have several overlapping boxes\n",
    "    * IOU can be a measure of how similar they are\n",
    "        * If IOU > 0.1 only use the highest confidence bounding box\n",
    "        (iou_threshold)\n",
    "* Does a Truth Box have a match\n",
    "    * Need to define how good a match counts as a match\n",
    "        * If IOU > .5 a box matches the truth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "develop_for_pred_gen = develop_dataset.generate(batch_size=1,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              },\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "true_labels=develop_dataset.labels\n",
    "y_pred=model.predict_generator(develop_for_pred_gen,len(develop_dataset.images))\n",
    "\n",
    "\n",
    "print(\"Done with Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decode First Five Images\")\n",
    "y_pred_decoded = decode_detections(y_pred[0:5],\n",
    "                                   confidence_thresh=0.0, ##Not cofidence cut\n",
    "                                   iou_threshold=0.1, # Overlap removal\n",
    "                                   top_k=200, #Only the best 200 boxes\n",
    "                                   normalize_coords=True,#normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Look at One Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index=2\n",
    "\n",
    "plt.imshow(develop_images[image_index])\n",
    "plt.show()\n",
    "\n",
    "print(\"labels\",y_pred_decoded[image_index])\n",
    "print(\"Truth\", develop_dataset.labels[image_index])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoded Labels for prediction are a list \n",
    "each element is in the form [best category, confidence, x,y,length,width]\n",
    "sorted with best result last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_categories=[l[0] for l in y_pred_decoded[image_index] ]\n",
    "all_confidences=[l[1] for l in y_pred_decoded[image_index]]\n",
    "\n",
    "plt.hist(all_categories)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(all_confidences)\n",
    "plt.show(\"All Confidences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not very confident, put lets look at the best two boxes\n",
    "plot_labels=[convert_ssd_labels(l) for l in y_pred_decoded[image_index][-10:]]\n",
    "\n",
    "print(\"Best 10 Boxes\")\n",
    "plot_example(develop_images[image_index],plot_labels)\n",
    "\n",
    "\n",
    "print(\"Truth Boxes\")\n",
    "plot_labels=[convert_ssd_labels(l) for l in true_labels[image_index]]\n",
    "\n",
    "plot_example(develop_images[image_index],plot_labels)\n",
    "             \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Bad, Lets make a more Statistical Statment\n",
    "\n",
    "Lets start by deciding anything with a confidence greater than 0.4 is a match (I encourage you to play with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.4, ##Add a Confidence Cut\n",
    "                                   iou_threshold=0.1, \n",
    "                                   top_k=200, \n",
    "                                   normalize_coords=True,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Look at a Bunch of Distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "def area_minmax(bbox):\n",
    "    xmin,ymin,xmax,ymax=bbox\n",
    "    return(xmax-xmin)*(ymax-ymin)\n",
    "\n",
    "def intersect_minmax(bbox1,bbox2):\n",
    "    xmin1,ymin1,xmax1,ymax1=bbox1\n",
    "    xmin2,ymin2,xmax2,ymax2=bbox2\n",
    "\n",
    "    xintmin=max(xmin1,xmin2)\n",
    "    yintmin=max(ymin1,ymin2)\n",
    "\n",
    "    xintmax=min(xmax1,xmax2)\n",
    "    yintmax=min(ymax1,ymax2)\n",
    "   \n",
    "    int_width=(xintmax-xintmin)    \n",
    "    int_height=(yintmax-yintmin)\n",
    "    if int_width <0 or int_height <0 :return 0\n",
    "    return int_width*int_height\n",
    "            \n",
    "def iou(bbox1,bbox2):\n",
    "    intarea=intersect_minmax(bbox1,bbox2)\n",
    "    u=area_minmax(bbox1)+area_minmax(bbox2)-intarea\n",
    "    return intarea/u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten_labels(labels):\n",
    "    values=[]\n",
    "    for l in labels:\n",
    "        for n in l:\n",
    "            values.append(n)\n",
    "    return values\n",
    "\n",
    "\n",
    "t_all=flatten_labels(true_labels)\n",
    "p_all=flatten_labels(y_pred_decoded)\n",
    "\n",
    "true_area=[area_minmax(l[1:5]) for l in t_all]\n",
    "pred_area=[area_minmax(l[2:6]) for l in p_all]\n",
    "\n",
    "true_width=[l[3]-l[1] for l in t_all]\n",
    "pred_width=[l[4]-l[2] for l in p_all]\n",
    "\n",
    "true_x=[l[1] for l in t_all]\n",
    "pred_x=[l[2] for l in p_all]\n",
    "\n",
    "true_y=[l[2] for l in t_all]\n",
    "pred_y=[l[3] for l in p_all]\n",
    "\n",
    "\n",
    "true_cat=[l[0] for l in t_all]\n",
    "pred_cat=[l[0] for l in p_all]\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(true_area,range=(50*50,200*200),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_area,range=(50*50,200*200),bins=10,label='pred',histtype='step')\n",
    "plt.xlabel(\"Area\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_width,range=(50,200),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_width,range=(50,200),bins=10,label='pred',histtype='step')\n",
    "plt.xlabel(\"Width\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_cat,range=(0,10),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_cat,range=(0,10),bins=10,label='pred',histtype='step')\n",
    "plt.xlabel(\"Category\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(true_x,range=(0,1600),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_x,range=(0,1600),bins=10,label='pred',histtype='step')\n",
    "plt.xlabel(\"X\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(true_x,range=(0,1200),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_x,range=(0,1200),bins=10,label='pred',histtype='step')\n",
    "plt.xlabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well do the results match the truth\n",
    "\n",
    "Make another choice if a has an IOU > 0.1 it has a match (I also encourage you to play with this, remember we're creating algorithms and the often means making choices to be more or less conservative )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#match best box\n",
    "def match(truth,pred):\n",
    "    best=[]\n",
    "    for t in truth:\n",
    "        matches=[ [iou(t[1:],p[2:]),i] for i,p in enumerate(pred)]\n",
    "        matches.sort()\n",
    "        if matches!=[]:            \n",
    "            best.append(matches[-1]) # largest iou\n",
    "        else:\n",
    "            best.append([0,-1])\n",
    "    return best\n",
    "\n",
    "matches=[match(t,p) for t,p in zip(true_labels,y_pred_decoded)]\n",
    "\n",
    "n_no_matches=0\n",
    "n_matches=0\n",
    "correct=0\n",
    "for index,image in enumerate(matches):\n",
    "    for li,m in enumerate(image):\n",
    "        if m[0] < .1 or m[1]==-1:n_no_matches+=1 # no match\n",
    "        else:\n",
    "            n_matches+=1 # no match\n",
    "            y_pred_decoded[index][m[1]][0]==true_labels[index][li][0]\n",
    "            correct+=1\n",
    "\n",
    "print(n_matches/(n_no_matches+n_matches)*100,\"% True Boxes Matched\")\n",
    "print(correct/(n_no_matches+n_matches)*100,\"% True Boxes Matched and identified\")\n",
    "print(len(p_all)-n_matches,\" wrong boxes\", correct,\"correct boxes\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was a Lot of Code, Some Take Aways\n",
    "\n",
    "## Multi-Box Detectors can be used to find objects in images\n",
    "* You can write them yourself, but a lot of prexisting packages are around\n",
    "* The magic that makes this work is using fully convolutional predictors (no dense layers at the end)\n",
    "   * This also means you can use any size image\n",
    "* You have to encode annotations, and decode them\n",
    "    * How exactly this is done will depend on the exact algorithm\n",
    "    * There are a lot of ways to mess this up, so don't get discouraged if it dosen't work at first\n",
    "* One of the first things you can tune with these models is to pick better anchor box sizes and aspect ratios!\n",
    "    \n",
    "# Other ways to do this\n",
    "   * Yolo (you only look once)\n",
    "   * Recursive CNNS (similar to scaning an image like we did last lecture)\n",
    "    \n",
    "# If you finish early I encourage you to play around with this or the previous datasets for practice. we will wrap up at the end\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
