{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camelyonpatch import CamelyOnPatch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Powerup\n",
    "\n",
    "Checking wether something an element in a list can be slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "dataset=np.random.uniform(0,1,size=(200000,2))\n",
    "train=np.random.choice(range(len(dataset)),int(len(dataset)*.8),replace=False)\n",
    "\n",
    "# This loop is slow\n",
    "itime=time()\n",
    "develop=[]\n",
    "for i in train:\n",
    "    if i not in train:\n",
    "        develop.append(i)\n",
    "print(\"Takes \",time()-itime,\" Seconds\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do something like the above it is much faster if you use a dictionary to keep track of what elements exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict={}\n",
    "for i in train:\n",
    "    train_dict[i]=0 #The value here dosen't matter \n",
    "    \n",
    "# This loop is fast!\n",
    "itime=time()\n",
    "develop=[]\n",
    "for i in train:\n",
    "    if i not in train_dict:\n",
    "        develop.append(i)\n",
    "print(\"Takes \",time()-itime,\" Seconds\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? When using a list every element is checked, however, a dictionary uses a trick called a hash function to convert the item into an location, and it makes only one checks to see if there is data in that location.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world Bio-Medical Datasets\n",
    "\n",
    "This notebook is a start on how you can use machine learning on bio-medical datasets.\n",
    "\n",
    "## Tumor Identification Segmentation\n",
    "\n",
    "We're going to start with the PatchCamelyon (PCam)\n",
    "https://github.com/basveeling/pcam\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/basveeling/pcam/master/pcam.jpg>\n",
    "\n",
    "\n",
    "## The RAW Data\n",
    "\n",
    "This dataset was created from whole slide images for example this one:\n",
    "* Whole slide image scans are gigantic!\n",
    "    * Really Big     (i.e. in this example 97,792 pixels by 221,184 pixels\n",
    "        * ~21,000 MegaPixels\n",
    "* Below is a low resolution view\n",
    "* These image don't generally fit into memory, but python packages exist to manipulate them\n",
    "\n",
    "For this file we're relying on a package called openslide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../assets/full_slide.png>\n",
    "Labels in this dataset are the boundaries of each tumor.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Slide Data\n",
    "\n",
    "* The data we're using is stored in .tif files that are very large\n",
    "    * Too large to fit into memory\n",
    "    * Too large to easily visualize \n",
    "* We'll use https://openslide.org/ to read these files\n",
    "\n",
    "* Files have levels of magnification from 0 (full-res) to 9 (low res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a full slide image\n",
    "tiff_file='/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif'\n",
    "slide_image=openslide.OpenSlide(tiff_file)\n",
    "\n",
    "# Slide images are divided into levels with level 0 being the highest resolution\n",
    "for i,res in enumerate(slide_image.level_dimensions):\n",
    "    print('The Resolution at level',i, 'is', res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the index coordinate system\n",
    "\n",
    "<img src=../assets/index_coords.png>\n",
    "\n",
    "\n",
    "Note that for open slide we need to use the index coordinates for **level 0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can read each region of the image at one time at different levels\n",
    "\n",
    "#Top left_pixel (x,y) in level 0, level,width,height\n",
    "print(\"Full Image at level 8\")\n",
    "image=np.asarray(slide_image.read_region( (0,0),8,(382, 864)  ))\n",
    "f=plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i,v in enumerate(slide_image.level_dimensions):\n",
    "    print(\"Level\",i,\"Resolution\",v)\n",
    "    f=plt.figure(figsize=(10,10))\n",
    "    image=np.asarray(slide_image.read_region( (50000,120000),i,(500,500)  ))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "You'll often get image labels in many different forms. The labeled information in this dataset comes from expert annotations delivered as an XML file.\n",
    "* Note: When working on real models these data steps often take up much of your time and are essential to get correct\n",
    "* How you use your data determines what you ML model learns\n",
    "\n",
    "\n",
    "# Raw Annotations (XML)\n",
    "\n",
    "Annotations for this dataset come in an XML format. This format is self-describing text. Let's look at an example.\n",
    "\n",
    "As a reminder we can use \"!\" to run shell commands, and a simple shell command to print out a file is **cat** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading XML by Eye\n",
    "XML has data described by elements. An element looks like this\n",
    "\n",
    "`<ASAP_Annotations>...</ASAP_Annotations>`\n",
    "\n",
    "Everything in between belongs to this element. \n",
    "\n",
    "In this example, we have a hierarchy that looks like\n",
    " * ASAP_annotations\n",
    "      * Annotations\n",
    "           * Annotation\n",
    "             * Coordinates\n",
    "             * Coordinates\n",
    "             * ...\n",
    "           * Annotation\n",
    "             * Coordinates\n",
    "             * Coordinates\n",
    "             * ...\n",
    "In other words, each image has annotations that are made up of a bunch of coordinates. For this data, each Annotation is the outline of a tumor, and this image has two tumors.\n",
    "\n",
    "## Attributes\n",
    "Each Element can also have **Attributes** in the file above they show up in the element definition\n",
    "\n",
    "This entry\n",
    "\n",
    "`<Coordinate Order=\"1641\" X=\"71123.3\" Y=\"127809\" />`\n",
    "\n",
    "has attributes Order,X,Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading XML with code\n",
    "\n",
    "XML is a standard format so there are many tools around to read it. We'll use python's xml \n",
    "library.\n",
    "\n",
    "**Goal:** Get a numpy array with the list of coordinates for each tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "#This import is different from normal but matplotlib has a useful function we'll use \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open XML File\n",
    "tree = ET.parse('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Select All Annotations\n",
    "tumor_annotation=[el for el in root.findall('Annotations/Annotation')]\n",
    "\n",
    "# Make A List of Tumors\n",
    "tumors=[]\n",
    "for annotation in tumor_annotation:\n",
    "    tumors.append(np.array([ [float(el.attrib['X']),float(el.attrib['Y'])] for el in annotation.findall('Coordinates/Coordinate')]))\n",
    "\n",
    "#convert to np array\n",
    "tumors=[np.array(t) for t in tumors]\n",
    "    \n",
    "#Plot the outlines of each tumor\n",
    "for t in tumors:\n",
    "    plt.scatter(t[:,0],t[:,1])\n",
    "plt.show()\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "We've read in our two tumors, try to plot them on top of the of the image data at Level 5 Resolution\n",
    "\n",
    "* You'll need to read the image in\n",
    "* You'll neet to make sure you're tumor coordinates are in the correct place for level 5 resolution\n",
    "\n",
    "**Give it a Try**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your Code Here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Dataset for Classification\n",
    "\n",
    "* Where going to turn our images and annotation into a **classification** problem\n",
    "\n",
    "Steps:\n",
    "  1. Ask a question we want an answer too:\n",
    "    * Q. Is a these cells part of a tumor?\n",
    "  2. Create our Xs\n",
    "   * Divide our big picture into lots of small pictures\n",
    "  3. Create or Ys\n",
    "   * Is any pixel within our region of interst part of a tumor (T/F)? \n",
    "   * Our region of interest could be all of our small picture, but how would are algorithm be able to succesfully guess if only the corner pixel is part of a tumor. Instead it's better to take a box in the middle of the \n",
    "<img src=../assets/Prediction_fig.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. How Big Do Our Small Pictures need to be\n",
    "    A. It Depends on the question you're asking\n",
    "\n",
    "If you questions are how many dounuts are in this picture, you'll need the whole image\n",
    "<img src=../assets/dn.jpg height=100>\n",
    "\n",
    "If your question is where are the yellow sprinkles the smaller images will do fine\n",
    "\n",
    "<img src=../assets/dn1.png height=50 width=100>\n",
    "<img src=../assets/dn2.png height=50 width=100>\n",
    "<img src=../assets/dn3.png height=50 width=100>\n",
    "<img src=../assets/dn4.png height=50 width=100>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Split of the Data\n",
    "\n",
    "Lets start with the an interesting region and cut it up into 96x96x3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These coordinates are at level 0\n",
    "width=6400\n",
    "height=6400\n",
    "\n",
    "x_start=60000\n",
    "y_start=120000\n",
    "\n",
    "print(\"Small Images Produced Required\",width//32*height//32)\n",
    "\n",
    "#Covert these for later use to level2\n",
    "level0_dimension=slide_image.level_dimensions[0]\n",
    "level2_dimension=slide_image.level_dimensions[2]\n",
    "\n",
    "sfactor_x=level0_dimension[0]/level2_dimension[0]\n",
    "sfactor_y=level0_dimension[1]/level2_dimension[1]\n",
    "\n",
    "x_stop=x_start+width*4\n",
    "y_stop=y_start+height*4\n",
    "\n",
    "\n",
    "print('Scaling factor to level 2',sfactor_x,sfactor_y)\n",
    "\n",
    "# It's just a factor of 4\n",
    "f=plt.figure(figsize=(50,50))\n",
    "plt.imshow(slide_image.read_region( (x_start,y_start),2,(height,width)))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is a point inside a Polygon?\n",
    "\n",
    "to get our labels we'll ask if a point is within a tumor we'll use a helpful function from matplotlib \"contains points\", we'll turn our tumor coords in to a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is Example using our first turmor\n",
    "\n",
    "\n",
    "    \n",
    "     \n",
    "print(tumors[0].shape)\n",
    "tumor_paths=[matplotlib.path.Path(t) for t in tumors]\n",
    "path = tumor_paths[0]\n",
    "\n",
    "points=np.zeros(shape=(1000,2))\n",
    "points[:,0]=np.random.uniform(69000,73000,size=1000)\n",
    "points[:,1]=np.random.uniform(131000,136000,size=1000)\n",
    "inside = path.contains_points(points)\n",
    "not_inside=np.logical_not(inside)\n",
    "\n",
    "\n",
    "plt.scatter(tumors[0][:,0],tumors[0][:,1])\n",
    "plt.scatter(points[inside,0],points[inside,1])\n",
    "plt.scatter(points[not_inside,0],points[not_inside,1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise \n",
    "\n",
    "Make a good looking plot with points inside and outside of the second tumor in this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Your code here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing out an Image File\n",
    "\n",
    "A lot of the code we've seen uses pillow (PIL) in the background will use it now to turn each of our numpy arrays into jpgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is simple in concept, move a 96x96 box in steps of 32 pixles accross and image and save each path. However it takes a fair bit of code, and the book keeping can get complicated.\n",
    "\n",
    "* We'll Create a directory structure that can be used directly by keras\n",
    "    * /Folder\n",
    "        * /tumor\n",
    "            * Patches with a tumor\n",
    "        * /normal\n",
    "            * Patches that do not have a tumor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir='wslide_test_data/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    os.mkdir(out_dir+'tumor')\n",
    "    os.mkdir(out_dir+'normal')\n",
    "\n",
    "def scan_image(image_file,batch_size,x_range,y_range,tumors):\n",
    "    labels=[]\n",
    "    slide_image=openslide.OpenSlide(image_file)\n",
    "    res_x,res_y=slide_image.level_dimensions[2]    \n",
    "    coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "    #This is factor we need to scale the pixels at resolution 2 to the coordinates in resolution 0\n",
    "\n",
    "    sfactor_x=coord_x/res_x  \n",
    "    sfactor_y=coord_y/res_y \n",
    "    \n",
    "    index=0\n",
    "    itr=0\n",
    "    \n",
    "    \n",
    "    for x in range(x_range[0]//4,x_range[1]//4,32):\n",
    "        for y in range(y_range[0]//4,y_range[1]//4,32):\n",
    "            image=slide_image.read_region( (int(x*sfactor_x),int(y*sfactor_y)),2,(96,96)  )\n",
    "            if itr%100==0:\n",
    "                print(image)\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            path=matplotlib.path.Path([((x+32)*sfactor_x,(y+32)*sfactor_y),\n",
    "                                       ((x+64)*sfactor_x,(y+32)*sfactor_y),\n",
    "                                       ((x+64)*sfactor_x,(y+64)*sfactor_y),\n",
    "                                       ((x+32)*sfactor_x,(y+64)*sfactor_y) ])\n",
    "            label=any([ path.contains_points(t).any() for t in tumors])\n",
    "            \n",
    "            if label:\n",
    "                image.save(out_dir+'normal/'+str(itr)+'.png')\n",
    "            else:\n",
    "                image.save(out_dir+'tumor/'+str(itr)+'.png')\n",
    "\n",
    "                \n",
    "            itr+=1\n",
    "                \n",
    "coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "              \n",
    "x_range=[0,coord_x]\n",
    "y_range=[0,coord_y]\n",
    "\n",
    "\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "\n",
    "generator=scan_image(tiff_file,10,x_range,y_range,tumors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls wslide_test_data/neg | wc\n",
    "!ls wslide_test_data/pos | wc\n",
    "! du -sm wslide_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've made you first dataset \n",
    "\n",
    "This was just one image and you would have to run a similar function of all the data in the \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/*\n",
    "\n",
    "!ls /projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/test/normal/*|wc\n",
    "!ls /projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/test/tumor/*|wc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
