{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning in Python\n",
    "\n",
    "Learn how to get started training Neural Networks with keras, so you can explore more on your own. \n",
    "* I'm Not going to go into theory, but feel free to ask about it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Mauritius_Road_Signs_-_Warning_Sign_-_Other_dangers.svg/556px-Mauritius_Road_Signs_-_Warning_Sign_-_Other_dangers.svg.png\" style=\"width:50px\">\n",
    "\n",
    "It's actually pretty easy to get started training Machine learning algorithms, but be aware there are plenty of examples of well trained, well coded, and well intentioned ML algorithms that do harmful things.\n",
    "\n",
    "\n",
    "<a href=\"https://www.technologyreview.com/s/613274/facebook-algorithm-discriminates-ai-bias\"> Facebook’s ad-serving algorithm discriminates by gender and race\n",
    " </a>\n",
    "    \n",
    "<a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\"> Amazon scraps secret AI recruiting tool that showed bias against women\n",
    " </a>\n",
    "\n",
    "<a href=\"https://www.thedailybeast.com/why-doctors-arent-afraid-of-better-more-efficient-ai-diagnosing-cancer\"> Ruler in picture an indicator for Cancer </a>\n",
    "\n",
    "<a href=\"https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\"> Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than a day </a>\n",
    "\n",
    "Be aware and careful before you deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.ap-south-1.amazonaws.com/techleer/207.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab\n",
    "## Artificial Intelligence\n",
    "    An all encompassing term for a broad field the most promising of which is currently machine learning\n",
    "## Machine Learning\n",
    "* Deep Learning - Deep Neural Networks of all forms\n",
    "* ‘Traditional’ Machine Learning  - Pretty much everything else\n",
    "    Trees, SVMs, Linear Regression, Naive Bayes...\n",
    "* **X’s = Input variables**\n",
    "* **Y’s = Target Variables**\n",
    "* Loss function - Numerical Goal of the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "* Find f(x) such that f(x) best approximates y\n",
    "* Examples:\n",
    "    * Given some pixels (x) tell me the probability it’s a cat (y)\n",
    "    * Given news articles (x) tell me a stocks value (y)\n",
    "    * Given some sequences x find some low dimensional space (z) that represent my data \n",
    "      * f1(x)=z f2(z)=x  \n",
    "* **Important Note: No prediction of causality** \n",
    "* Function outputs and targets can be stochastic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms all start form a series data examples\n",
    "\n",
    "\n",
    "# Input Data\n",
    "* **numpy arrays** \n",
    "* pandas dataframes\n",
    "* hdf5, etc\n",
    "* **shape = (examples x data dimentions)** \n",
    "    * RGB Image Dataset (Number of images x Height x Width x 3) *(3=RGB)\n",
    "    * Text (Text blocks x ? ) examples with varying length can have an unspecified dimension size\n",
    "        * When training each batch needs to be the same length\n",
    "* Divide into 2 or 3 splits\n",
    "  * 2 Training/Testing (one for training and one for checking for over-fitting)\n",
    "  * 3 Training/Development/Testing \n",
    "      * One for training, one for checking for over-fitting (Development) )\n",
    "      * One for testing performance, but not for making any modeling decisions\n",
    "          * i.e. in this case testing is the data you want the model to actually work with  \n",
    "          \n",
    "# A short Menu of ML layers\n",
    "* Convolutional Layers (Conv1D, Conv2D, Conv3D)\n",
    "    * Input sequences of fixed or varying length best when array values that are close together are correlate i.e pictures\n",
    "    * Output a new sequence normally lower dimension, but with more channels    \n",
    "* Recurrent Neural Networks (RNN, LSTM, GRUS)\n",
    "    * Input sequence\n",
    "    * Output sequence or a fixed dimensional output    \n",
    "\n",
    "* Embedding Network\n",
    "    * A learnable mapping from a large set of integers, to a fixed output\n",
    "    * Input integer\n",
    "    * Ouput vector\n",
    "\n",
    "* Dense Network\n",
    "    * Fixed Input\n",
    "    * Fixed Output\n",
    "\n",
    "* Dropout\n",
    "    * Good at preventing overfitting\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with a simple prediction a straight Line\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,50))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1 #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 101.7156 - val_loss: 57.9757\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 54.8536 - val_loss: 53.8127\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 50.4118 - val_loss: 49.5869\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 45.7172 - val_loss: 44.2380\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 40.8688 - val_loss: 39.3770\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 36.0995 - val_loss: 34.6685\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 31.6166 - val_loss: 30.1286\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 27.4622 - val_loss: 26.2155\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 23.6870 - val_loss: 22.3887\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 20.1923 - val_loss: 19.3311\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 17.1805 - val_loss: 16.1697\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 14.4729 - val_loss: 13.5846\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 12.0987 - val_loss: 11.3332\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 10.0356 - val_loss: 9.4362\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 8.2775 - val_loss: 7.6909\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 1s 143us/step - loss: 6.7583 - val_loss: 6.2795\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 5.4506 - val_loss: 5.0377\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 4.3690 - val_loss: 4.0074\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 3.4587 - val_loss: 3.1712\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 2.7145 - val_loss: 2.4450\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 2.0899 - val_loss: 1.8859\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 1s 132us/step - loss: 1.5837 - val_loss: 1.4104\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 1s 125us/step - loss: 1.1780 - val_loss: 1.0491\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 1s 123us/step - loss: 0.8687 - val_loss: 0.7564\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 0.6191 - val_loss: 0.5330\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 1s 131us/step - loss: 0.4325 - val_loss: 0.3696\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.2959 - val_loss: 0.2473\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 0.1967 - val_loss: 0.1612\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 1s 127us/step - loss: 0.1262 - val_loss: 0.1031\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0796 - val_loss: 0.0638\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0493 - val_loss: 0.0394\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.0303 - val_loss: 0.0237\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0184 - val_loss: 0.0148\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.0118 - val_loss: 0.0096\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.0081 - val_loss: 0.0069\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0022 - val_loss: 0.0036\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 1s 128us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.0011 - val_loss: 9.7496e-04\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 138us/step - loss: 9.3233e-04 - val_loss: 8.0545e-04\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 9.3388e-04 - val_loss: 7.3226e-04\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 9.3510e-04 - val_loss: 8.5472e-04\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 7.0260e-04 - val_loss: 0.0014\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 8.0795e-04 - val_loss: 0.0011\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 5.9986e-04 - val_loss: 4.3496e-04\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 5.9897e-04 - val_loss: 4.1155e-04\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 4.3384e-04 - val_loss: 6.5849e-04\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 4.9711e-04 - val_loss: 3.3667e-04\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 4.1299e-04 - val_loss: 3.6525e-04\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 3.3116e-04 - val_loss: 2.5414e-04\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 3.0829e-04 - val_loss: 2.6936e-04\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 2.5239e-04 - val_loss: 4.4583e-04\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 3.6554e-04 - val_loss: 2.1546e-04\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 2.2099e-04 - val_loss: 3.7558e-04\n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 1.7649e-04 - val_loss: 1.0949e-04\n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 3.0472e-04 - val_loss: 2.1012e-04\n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 2.0603e-04 - val_loss: 1.5141e-04\n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 1.3824e-04 - val_loss: 7.3516e-05\n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 1.3721e-04 - val_loss: 2.8991e-04\n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 8.1818e-05 - val_loss: 5.5850e-05\n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 1.4623e-04 - val_loss: 4.4427e-05\n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 5.7044e-05 - val_loss: 3.9681e-05\n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 4.7703e-05 - val_loss: 6.7511e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc635df0e48>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(50,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X,Y,epochs=100,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc637d07940>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,50))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 24.9101 - val_loss: 13.0101\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 12.0101 - val_loss: 10.8033\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 9.6939 - val_loss: 8.4772\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 7.4437 - val_loss: 6.3687\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 5.4814 - val_loss: 4.6385\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 3.9187 - val_loss: 3.2423\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 2.7123 - val_loss: 2.2420\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 1.8664 - val_loss: 1.5452\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 1.2895 - val_loss: 1.0890\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.9225 - val_loss: 0.8062\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: 0.6921 - val_loss: 0.6287\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.5669 - val_loss: 0.5366\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4974 - val_loss: 0.4904\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.4610 - val_loss: 0.4663\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 0.4455 - val_loss: 0.4607\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4393 - val_loss: 0.4631\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4353 - val_loss: 0.4513\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.4372 - val_loss: 0.4535\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.4333 - val_loss: 0.4607\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4347 - val_loss: 0.4747\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4385 - val_loss: 0.4526\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 0.4365 - val_loss: 0.4572\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4371 - val_loss: 0.4559\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4357 - val_loss: 0.4509\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4379 - val_loss: 0.4575\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4380 - val_loss: 0.4560\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4366 - val_loss: 0.4625\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4362 - val_loss: 0.4511\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4400 - val_loss: 0.4632\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4446 - val_loss: 0.4513\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4412 - val_loss: 0.4512\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4394 - val_loss: 0.4501\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 0.4417 - val_loss: 0.4510\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 0.4474 - val_loss: 0.4594\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4414 - val_loss: 0.4640\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4441 - val_loss: 0.4696\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 0.4425 - val_loss: 0.4828\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4450 - val_loss: 0.4544\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.4470 - val_loss: 0.4675\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4424 - val_loss: 0.4724\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4442 - val_loss: 0.4615\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4422 - val_loss: 0.4571\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4468 - val_loss: 0.4646\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4450 - val_loss: 0.4511\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4413 - val_loss: 0.4500\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4440 - val_loss: 0.4615\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 0.4446 - val_loss: 0.4514\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4505 - val_loss: 0.4562\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4461 - val_loss: 0.4626\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4498 - val_loss: 0.4515\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4436 - val_loss: 0.4681\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4482 - val_loss: 0.4910\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4504 - val_loss: 0.4552\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4428 - val_loss: 0.4632\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 1s 142us/step - loss: 0.4561 - val_loss: 0.4706\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4407 - val_loss: 0.4549\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4426 - val_loss: 0.4721\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4423 - val_loss: 0.4557\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4426 - val_loss: 0.4638\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4476 - val_loss: 0.4534\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4418 - val_loss: 0.4529\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 1s 144us/step - loss: 0.4404 - val_loss: 0.4507\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4431 - val_loss: 0.4504\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4433 - val_loss: 0.4646\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4394 - val_loss: 0.4518\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 1s 132us/step - loss: 0.4416 - val_loss: 0.4510\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4417 - val_loss: 0.4513\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4483 - val_loss: 0.4556\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4507 - val_loss: 0.4599\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 0.4404 - val_loss: 0.4513\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4395 - val_loss: 0.4523\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4507 - val_loss: 0.4574\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 1s 132us/step - loss: 0.4399 - val_loss: 0.4513\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 1s 128us/step - loss: 0.4503 - val_loss: 0.4518\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 0.4466 - val_loss: 0.4933\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 1s 131us/step - loss: 0.4528 - val_loss: 0.4501\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4386 - val_loss: 0.4929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4519 - val_loss: 0.4617\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4435 - val_loss: 0.4848\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4475 - val_loss: 0.4510\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: 0.4431 - val_loss: 0.4534\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4445 - val_loss: 0.4503\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4393 - val_loss: 0.4517\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4394 - val_loss: 0.4553\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 0.4476 - val_loss: 0.4791\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 1s 134us/step - loss: 0.4439 - val_loss: 0.4596\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.4487 - val_loss: 0.4929\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4481 - val_loss: 0.4639\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4481 - val_loss: 0.4572\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.4438 - val_loss: 0.4556\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4424 - val_loss: 0.4627\n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 1s 137us/step - loss: 0.4396 - val_loss: 0.4532\n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4436 - val_loss: 0.4514\n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4400 - val_loss: 0.4545\n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 1s 136us/step - loss: 0.4528 - val_loss: 0.4546\n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 1s 138us/step - loss: 0.4425 - val_loss: 0.4558\n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 1s 141us/step - loss: 0.4386 - val_loss: 0.4505\n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 1s 135us/step - loss: 0.4466 - val_loss: 0.4965\n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 1s 139us/step - loss: 0.4521 - val_loss: 0.4577\n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 1s 140us/step - loss: 0.4406 - val_loss: 0.4558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc638766940>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,50))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(50,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X,Y,epochs=100,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "X_test=np.random.uniform(0,10,size=(100,50))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,881\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 2s 451us/step - loss: 0.8004 - val_loss: 0.5009\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.4834 - val_loss: 0.4910\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.4662 - val_loss: 0.4690\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.4618 - val_loss: 0.4719\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.4508 - val_loss: 0.4569\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 0.4463 - val_loss: 0.4539\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.4451 - val_loss: 0.4776\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.4359 - val_loss: 0.4462\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.4264 - val_loss: 0.4438\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.4121 - val_loss: 0.4244\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.3965 - val_loss: 0.4204\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.3589 - val_loss: 0.3670\n",
      "Epoch 13/50\n",
      "5000/5000 [==============================] - 1s 251us/step - loss: 0.3159 - val_loss: 0.3304\n",
      "Epoch 14/50\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2906 - val_loss: 0.2848\n",
      "Epoch 15/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.2586 - val_loss: 0.2927\n",
      "Epoch 16/50\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.2308 - val_loss: 0.2431\n",
      "Epoch 17/50\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 0.2039 - val_loss: 0.2032\n",
      "Epoch 18/50\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 0.1690 - val_loss: 0.1627\n",
      "Epoch 19/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.1563 - val_loss: 0.1496\n",
      "Epoch 20/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.1400 - val_loss: 0.1439\n",
      "Epoch 21/50\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.1380 - val_loss: 0.1389\n",
      "Epoch 22/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.1280 - val_loss: 0.1440\n",
      "Epoch 23/50\n",
      "5000/5000 [==============================] - 1s 251us/step - loss: 0.1189 - val_loss: 0.1552\n",
      "Epoch 24/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.1151 - val_loss: 0.1225\n",
      "Epoch 25/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.1119 - val_loss: 0.1083\n",
      "Epoch 26/50\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.1091 - val_loss: 0.1098\n",
      "Epoch 27/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.1032 - val_loss: 0.1001\n",
      "Epoch 28/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.0971 - val_loss: 0.0951\n",
      "Epoch 29/50\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.0933 - val_loss: 0.1093\n",
      "Epoch 30/50\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.0917 - val_loss: 0.0878\n",
      "Epoch 31/50\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.0866 - val_loss: 0.0887\n",
      "Epoch 32/50\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 0.0872 - val_loss: 0.0969\n",
      "Epoch 33/50\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 0.0957 - val_loss: 0.0888\n",
      "Epoch 34/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.0844 - val_loss: 0.0836\n",
      "Epoch 35/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.0791 - val_loss: 0.0852\n",
      "Epoch 36/50\n",
      "5000/5000 [==============================] - 1s 250us/step - loss: 0.0768 - val_loss: 0.1009\n",
      "Epoch 37/50\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 0.0804 - val_loss: 0.0730\n",
      "Epoch 38/50\n",
      "5000/5000 [==============================] - 1s 258us/step - loss: 0.0780 - val_loss: 0.0792\n",
      "Epoch 39/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.0707 - val_loss: 0.1039\n",
      "Epoch 40/50\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.0692 - val_loss: 0.0731\n",
      "Epoch 41/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.0695 - val_loss: 0.0780\n",
      "Epoch 42/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.0669 - val_loss: 0.0660\n",
      "Epoch 43/50\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 0.0627 - val_loss: 0.0620\n",
      "Epoch 44/50\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 0.0666 - val_loss: 0.0634\n",
      "Epoch 45/50\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.0659 - val_loss: 0.0596\n",
      "Epoch 46/50\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 0.0616 - val_loss: 0.0649\n",
      "Epoch 47/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.0579 - val_loss: 0.0578\n",
      "Epoch 48/50\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.0582 - val_loss: 0.0586\n",
      "Epoch 49/50\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.0558 - val_loss: 0.0621\n",
      "Epoch 50/50\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 0.0541 - val_loss: 0.0733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc6443a75f8>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,50))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(50,)) \n",
    "###Lets Add another layer and an Activation###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.summary()\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X,Y,epochs=50,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "X_test=np.random.uniform(0,10,size=(100,50))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,881\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/500\n",
      "5000/5000 [==============================] - 2s 498us/step - loss: 6.6943 - val_loss: 0.6839\n",
      "Epoch 2/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 1.5230 - val_loss: 0.5725\n",
      "Epoch 3/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 1.0768 - val_loss: 0.5432\n",
      "Epoch 4/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.8861 - val_loss: 0.5202\n",
      "Epoch 5/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.7760 - val_loss: 0.5034\n",
      "Epoch 6/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.6690 - val_loss: 0.4934\n",
      "Epoch 7/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.6407 - val_loss: 0.4799\n",
      "Epoch 8/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.6009 - val_loss: 0.4752\n",
      "Epoch 9/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.5677 - val_loss: 0.4700\n",
      "Epoch 10/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.5381 - val_loss: 0.4637\n",
      "Epoch 11/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.5274 - val_loss: 0.4628\n",
      "Epoch 12/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.5216 - val_loss: 0.4642\n",
      "Epoch 13/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.5066 - val_loss: 0.4607\n",
      "Epoch 14/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.4958 - val_loss: 0.4602\n",
      "Epoch 15/500\n",
      "5000/5000 [==============================] - 1s 276us/step - loss: 0.4857 - val_loss: 0.4558\n",
      "Epoch 16/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.4750 - val_loss: 0.4546\n",
      "Epoch 17/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.4768 - val_loss: 0.4532\n",
      "Epoch 18/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4696 - val_loss: 0.4538\n",
      "Epoch 19/500\n",
      "5000/5000 [==============================] - 1s 276us/step - loss: 0.4700 - val_loss: 0.4528\n",
      "Epoch 20/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.4624 - val_loss: 0.4540\n",
      "Epoch 21/500\n",
      "5000/5000 [==============================] - 1s 276us/step - loss: 0.4655 - val_loss: 0.4521\n",
      "Epoch 22/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.4614 - val_loss: 0.4516\n",
      "Epoch 23/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4582 - val_loss: 0.4513\n",
      "Epoch 24/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4567 - val_loss: 0.4525\n",
      "Epoch 25/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.4574 - val_loss: 0.4513\n",
      "Epoch 26/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.4532 - val_loss: 0.4520\n",
      "Epoch 27/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.4555 - val_loss: 0.4507\n",
      "Epoch 28/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4550 - val_loss: 0.4510\n",
      "Epoch 29/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.4511 - val_loss: 0.4507\n",
      "Epoch 30/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.4520 - val_loss: 0.4502\n",
      "Epoch 31/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.4543 - val_loss: 0.4508\n",
      "Epoch 32/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.4529 - val_loss: 0.4512\n",
      "Epoch 33/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.4484 - val_loss: 0.4495\n",
      "Epoch 34/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4490 - val_loss: 0.4500\n",
      "Epoch 35/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.4474 - val_loss: 0.4498\n",
      "Epoch 36/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.4488 - val_loss: 0.4501\n",
      "Epoch 37/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.4456 - val_loss: 0.4492\n",
      "Epoch 38/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.4452 - val_loss: 0.4480\n",
      "Epoch 39/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.4445 - val_loss: 0.4485\n",
      "Epoch 40/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.4447 - val_loss: 0.4477\n",
      "Epoch 41/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.4447 - val_loss: 0.4462\n",
      "Epoch 42/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.4417 - val_loss: 0.4454\n",
      "Epoch 43/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4398 - val_loss: 0.4426\n",
      "Epoch 44/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.4387 - val_loss: 0.4406\n",
      "Epoch 45/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.4384 - val_loss: 0.4389\n",
      "Epoch 46/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.4352 - val_loss: 0.4353\n",
      "Epoch 47/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.4300 - val_loss: 0.4294\n",
      "Epoch 48/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.4206 - val_loss: 0.4064\n",
      "Epoch 49/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.4138 - val_loss: 0.4035\n",
      "Epoch 50/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.4108 - val_loss: 0.3904\n",
      "Epoch 51/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.4018 - val_loss: 0.3882\n",
      "Epoch 52/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.3889 - val_loss: 0.3915\n",
      "Epoch 53/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.3940 - val_loss: 0.3730\n",
      "Epoch 54/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.3890 - val_loss: 0.3696\n",
      "Epoch 55/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3921 - val_loss: 0.3778\n",
      "Epoch 56/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.3839 - val_loss: 0.3662\n",
      "Epoch 57/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.3772 - val_loss: 0.3676\n",
      "Epoch 58/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3830 - val_loss: 0.3560\n",
      "Epoch 59/500\n",
      "5000/5000 [==============================] - 1s 277us/step - loss: 0.3711 - val_loss: 0.3598\n",
      "Epoch 60/500\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.3723 - val_loss: 0.3536\n",
      "Epoch 61/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.3741 - val_loss: 0.3612\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3643 - val_loss: 0.3548\n",
      "Epoch 63/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.3696 - val_loss: 0.3753\n",
      "Epoch 64/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3682 - val_loss: 0.3620\n",
      "Epoch 65/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3670 - val_loss: 0.3462\n",
      "Epoch 66/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3606 - val_loss: 0.3604\n",
      "Epoch 67/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.3665 - val_loss: 0.3631\n",
      "Epoch 68/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3645 - val_loss: 0.3449\n",
      "Epoch 69/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.3515 - val_loss: 0.3339\n",
      "Epoch 70/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.3544 - val_loss: 0.3409\n",
      "Epoch 71/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3541 - val_loss: 0.3354\n",
      "Epoch 72/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3528 - val_loss: 0.3316\n",
      "Epoch 73/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.3531 - val_loss: 0.3293\n",
      "Epoch 74/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.3488 - val_loss: 0.3367\n",
      "Epoch 75/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.3526 - val_loss: 0.3257\n",
      "Epoch 76/500\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.3477 - val_loss: 0.3219\n",
      "Epoch 77/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.3425 - val_loss: 0.3322\n",
      "Epoch 78/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.3457 - val_loss: 0.3128\n",
      "Epoch 79/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3459 - val_loss: 0.3229\n",
      "Epoch 80/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.3476 - val_loss: 0.3282\n",
      "Epoch 81/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3397 - val_loss: 0.3144\n",
      "Epoch 82/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3481 - val_loss: 0.3294\n",
      "Epoch 83/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3514 - val_loss: 0.3241\n",
      "Epoch 84/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.3432 - val_loss: 0.3289\n",
      "Epoch 85/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3347 - val_loss: 0.3191\n",
      "Epoch 86/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3438 - val_loss: 0.3212\n",
      "Epoch 87/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.3361 - val_loss: 0.3020\n",
      "Epoch 88/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.3359 - val_loss: 0.3061\n",
      "Epoch 89/500\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 0.3364 - val_loss: 0.3228\n",
      "Epoch 90/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3386 - val_loss: 0.3121\n",
      "Epoch 91/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3312 - val_loss: 0.3296\n",
      "Epoch 92/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3275 - val_loss: 0.3041\n",
      "Epoch 93/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3329 - val_loss: 0.3436\n",
      "Epoch 94/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3237 - val_loss: 0.2881\n",
      "Epoch 95/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3220 - val_loss: 0.2939\n",
      "Epoch 96/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3239 - val_loss: 0.2877\n",
      "Epoch 97/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.3258 - val_loss: 0.3635\n",
      "Epoch 98/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3211 - val_loss: 0.2794\n",
      "Epoch 99/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3285 - val_loss: 0.2974\n",
      "Epoch 100/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3189 - val_loss: 0.2880\n",
      "Epoch 101/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.3255 - val_loss: 0.2867\n",
      "Epoch 102/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.3202 - val_loss: 0.2845\n",
      "Epoch 103/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.3193 - val_loss: 0.2885\n",
      "Epoch 104/500\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 0.3124 - val_loss: 0.2785\n",
      "Epoch 105/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.3105 - val_loss: 0.3070\n",
      "Epoch 106/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.3076 - val_loss: 0.2735\n",
      "Epoch 107/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3114 - val_loss: 0.2788\n",
      "Epoch 108/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.3083 - val_loss: 0.2876\n",
      "Epoch 109/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.3168 - val_loss: 0.2692\n",
      "Epoch 110/500\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.3039 - val_loss: 0.2770\n",
      "Epoch 111/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3018 - val_loss: 0.3258\n",
      "Epoch 112/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3098 - val_loss: 0.2529\n",
      "Epoch 113/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.3033 - val_loss: 0.2760\n",
      "Epoch 114/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.3065 - val_loss: 0.2491\n",
      "Epoch 115/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.3045 - val_loss: 0.2813\n",
      "Epoch 116/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.3020 - val_loss: 0.3081\n",
      "Epoch 117/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2985 - val_loss: 0.2824\n",
      "Epoch 118/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2994 - val_loss: 0.2609\n",
      "Epoch 119/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2967 - val_loss: 0.2592\n",
      "Epoch 120/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2951 - val_loss: 0.2651\n",
      "Epoch 121/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2997 - val_loss: 0.2589\n",
      "Epoch 122/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2943 - val_loss: 0.2426\n",
      "Epoch 123/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2921 - val_loss: 0.2907\n",
      "Epoch 124/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.3006 - val_loss: 0.2763\n",
      "Epoch 125/500\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.2940 - val_loss: 0.2610\n",
      "Epoch 126/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2930 - val_loss: 0.2598\n",
      "Epoch 127/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2964 - val_loss: 0.2744\n",
      "Epoch 128/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2948 - val_loss: 0.2691\n",
      "Epoch 129/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2949 - val_loss: 0.2692\n",
      "Epoch 130/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.2916 - val_loss: 0.2533\n",
      "Epoch 131/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2883 - val_loss: 0.2565\n",
      "Epoch 132/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.2913 - val_loss: 0.2622\n",
      "Epoch 133/500\n",
      "5000/5000 [==============================] - 1s 278us/step - loss: 0.2890 - val_loss: 0.2622\n",
      "Epoch 134/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2886 - val_loss: 0.2526\n",
      "Epoch 135/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.2849 - val_loss: 0.2551\n",
      "Epoch 136/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2953 - val_loss: 0.2923\n",
      "Epoch 137/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2890 - val_loss: 0.2466\n",
      "Epoch 138/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2840 - val_loss: 0.2544\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2931 - val_loss: 0.2677\n",
      "Epoch 140/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2817 - val_loss: 0.2677\n",
      "Epoch 141/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2841 - val_loss: 0.2803\n",
      "Epoch 142/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2850 - val_loss: 0.2771\n",
      "Epoch 143/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2879 - val_loss: 0.2659\n",
      "Epoch 144/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2895 - val_loss: 0.2754\n",
      "Epoch 145/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2847 - val_loss: 0.2793\n",
      "Epoch 146/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2915 - val_loss: 0.2689\n",
      "Epoch 147/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2916 - val_loss: 0.2872\n",
      "Epoch 148/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2834 - val_loss: 0.2400\n",
      "Epoch 149/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2845 - val_loss: 0.2691\n",
      "Epoch 150/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2849 - val_loss: 0.2870\n",
      "Epoch 151/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2886 - val_loss: 0.2471\n",
      "Epoch 152/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2924 - val_loss: 0.2944\n",
      "Epoch 153/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2792 - val_loss: 0.2370\n",
      "Epoch 154/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.2827 - val_loss: 0.2595\n",
      "Epoch 155/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.2804 - val_loss: 0.2665\n",
      "Epoch 156/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2849 - val_loss: 0.2823\n",
      "Epoch 157/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2891 - val_loss: 0.2480\n",
      "Epoch 158/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2874 - val_loss: 0.2774\n",
      "Epoch 159/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2861 - val_loss: 0.2675\n",
      "Epoch 160/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2840 - val_loss: 0.2523\n",
      "Epoch 161/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2802 - val_loss: 0.2443\n",
      "Epoch 162/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2782 - val_loss: 0.2314\n",
      "Epoch 163/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2791 - val_loss: 0.2599\n",
      "Epoch 164/500\n",
      "5000/5000 [==============================] - 1s 258us/step - loss: 0.2779 - val_loss: 0.2501\n",
      "Epoch 165/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2754 - val_loss: 0.2588\n",
      "Epoch 166/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2796 - val_loss: 0.2395\n",
      "Epoch 167/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2730 - val_loss: 0.2511\n",
      "Epoch 168/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2746 - val_loss: 0.2255\n",
      "Epoch 169/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2757 - val_loss: 0.2459\n",
      "Epoch 170/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2750 - val_loss: 0.2838\n",
      "Epoch 171/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2768 - val_loss: 0.2382\n",
      "Epoch 172/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2753 - val_loss: 0.2693\n",
      "Epoch 173/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2711 - val_loss: 0.2423\n",
      "Epoch 174/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2761 - val_loss: 0.2513\n",
      "Epoch 175/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2835 - val_loss: 0.2529\n",
      "Epoch 176/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2799 - val_loss: 0.2396\n",
      "Epoch 177/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2756 - val_loss: 0.2649\n",
      "Epoch 178/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2773 - val_loss: 0.2547\n",
      "Epoch 179/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2730 - val_loss: 0.2511\n",
      "Epoch 180/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2766 - val_loss: 0.2583\n",
      "Epoch 181/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2699 - val_loss: 0.2400\n",
      "Epoch 182/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2718 - val_loss: 0.2267\n",
      "Epoch 183/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2703 - val_loss: 0.2551\n",
      "Epoch 184/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2805 - val_loss: 0.2355\n",
      "Epoch 185/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2666 - val_loss: 0.2624\n",
      "Epoch 186/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2690 - val_loss: 0.2248\n",
      "Epoch 187/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2677 - val_loss: 0.2352\n",
      "Epoch 188/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2754 - val_loss: 0.2489\n",
      "Epoch 189/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2774 - val_loss: 0.2700\n",
      "Epoch 190/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2728 - val_loss: 0.2402\n",
      "Epoch 191/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2779 - val_loss: 0.2721\n",
      "Epoch 192/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2736 - val_loss: 0.2528\n",
      "Epoch 193/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.2684 - val_loss: 0.2314\n",
      "Epoch 194/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2740 - val_loss: 0.2476\n",
      "Epoch 195/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2688 - val_loss: 0.2487\n",
      "Epoch 196/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.2739 - val_loss: 0.2345\n",
      "Epoch 197/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2687 - val_loss: 0.2474\n",
      "Epoch 198/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2688 - val_loss: 0.2524\n",
      "Epoch 199/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2662 - val_loss: 0.2272\n",
      "Epoch 200/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.2665 - val_loss: 0.3094\n",
      "Epoch 201/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2668 - val_loss: 0.2520\n",
      "Epoch 202/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2687 - val_loss: 0.2486\n",
      "Epoch 203/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2575 - val_loss: 0.2829\n",
      "Epoch 204/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.2666 - val_loss: 0.2247\n",
      "Epoch 205/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2634 - val_loss: 0.2695\n",
      "Epoch 206/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2602 - val_loss: 0.2414\n",
      "Epoch 207/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.2695 - val_loss: 0.2161\n",
      "Epoch 208/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2641 - val_loss: 0.2197\n",
      "Epoch 209/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.2528 - val_loss: 0.2792\n",
      "Epoch 210/500\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.2484 - val_loss: 0.2001\n",
      "Epoch 211/500\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 0.2664 - val_loss: 0.2186\n",
      "Epoch 212/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2573 - val_loss: 0.2052\n",
      "Epoch 213/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2567 - val_loss: 0.2329\n",
      "Epoch 214/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2563 - val_loss: 0.1893\n",
      "Epoch 215/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2583 - val_loss: 0.2306\n",
      "Epoch 216/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2565 - val_loss: 0.2355\n",
      "Epoch 217/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2476 - val_loss: 0.2431\n",
      "Epoch 218/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2552 - val_loss: 0.2423\n",
      "Epoch 219/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2480 - val_loss: 0.2171\n",
      "Epoch 220/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2533 - val_loss: 0.2363\n",
      "Epoch 221/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2594 - val_loss: 0.2330\n",
      "Epoch 222/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2640 - val_loss: 0.2317\n",
      "Epoch 223/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2499 - val_loss: 0.2648\n",
      "Epoch 224/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2562 - val_loss: 0.2260\n",
      "Epoch 225/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2498 - val_loss: 0.2419\n",
      "Epoch 226/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2500 - val_loss: 0.2375\n",
      "Epoch 227/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2573 - val_loss: 0.2438\n",
      "Epoch 228/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2607 - val_loss: 0.2250\n",
      "Epoch 229/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2554 - val_loss: 0.1955\n",
      "Epoch 230/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2553 - val_loss: 0.2287\n",
      "Epoch 231/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2513 - val_loss: 0.2181\n",
      "Epoch 232/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2473 - val_loss: 0.2432\n",
      "Epoch 233/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2533 - val_loss: 0.2566\n",
      "Epoch 234/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.2445 - val_loss: 0.1942\n",
      "Epoch 235/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2505 - val_loss: 0.2190\n",
      "Epoch 236/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2505 - val_loss: 0.1943\n",
      "Epoch 237/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2488 - val_loss: 0.2279\n",
      "Epoch 238/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2506 - val_loss: 0.2882\n",
      "Epoch 239/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.2446 - val_loss: 0.2079\n",
      "Epoch 240/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2516 - val_loss: 0.2386\n",
      "Epoch 241/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2512 - val_loss: 0.2018\n",
      "Epoch 242/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2537 - val_loss: 0.2022\n",
      "Epoch 243/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2474 - val_loss: 0.2247\n",
      "Epoch 244/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2545 - val_loss: 0.2354\n",
      "Epoch 245/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2490 - val_loss: 0.2209\n",
      "Epoch 246/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2450 - val_loss: 0.2308\n",
      "Epoch 247/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2406 - val_loss: 0.2773\n",
      "Epoch 248/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2461 - val_loss: 0.2575\n",
      "Epoch 249/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2465 - val_loss: 0.2088\n",
      "Epoch 250/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2446 - val_loss: 0.2481\n",
      "Epoch 251/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2407 - val_loss: 0.2280\n",
      "Epoch 252/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2434 - val_loss: 0.2352\n",
      "Epoch 253/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2530 - val_loss: 0.2017\n",
      "Epoch 254/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2484 - val_loss: 0.2383\n",
      "Epoch 255/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.2483 - val_loss: 0.2438\n",
      "Epoch 256/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.2445 - val_loss: 0.2238\n",
      "Epoch 257/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2504 - val_loss: 0.2385\n",
      "Epoch 258/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2455 - val_loss: 0.2342\n",
      "Epoch 259/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.2463 - val_loss: 0.2200\n",
      "Epoch 260/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2456 - val_loss: 0.2207\n",
      "Epoch 261/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2444 - val_loss: 0.2813\n",
      "Epoch 262/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2495 - val_loss: 0.2767\n",
      "Epoch 263/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2488 - val_loss: 0.2203\n",
      "Epoch 264/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2493 - val_loss: 0.2362\n",
      "Epoch 265/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2483 - val_loss: 0.2581\n",
      "Epoch 266/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2470 - val_loss: 0.2216\n",
      "Epoch 267/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.2418 - val_loss: 0.2312\n",
      "Epoch 268/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.2410 - val_loss: 0.2592\n",
      "Epoch 269/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2417 - val_loss: 0.2552\n",
      "Epoch 270/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.2414 - val_loss: 0.2651\n",
      "Epoch 271/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2423 - val_loss: 0.2472\n",
      "Epoch 272/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2390 - val_loss: 0.2149\n",
      "Epoch 273/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2494 - val_loss: 0.2372\n",
      "Epoch 274/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2437 - val_loss: 0.2337\n",
      "Epoch 275/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2483 - val_loss: 0.2346\n",
      "Epoch 276/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2417 - val_loss: 0.2661\n",
      "Epoch 277/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2435 - val_loss: 0.2104\n",
      "Epoch 278/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2509 - val_loss: 0.2550\n",
      "Epoch 279/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2432 - val_loss: 0.2286\n",
      "Epoch 280/500\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 0.2484 - val_loss: 0.2530\n",
      "Epoch 281/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.2513 - val_loss: 0.2034\n",
      "Epoch 282/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2460 - val_loss: 0.2529\n",
      "Epoch 283/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2359 - val_loss: 0.2511\n",
      "Epoch 284/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2424 - val_loss: 0.2765\n",
      "Epoch 285/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.2474 - val_loss: 0.2206\n",
      "Epoch 286/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2402 - val_loss: 0.2696\n",
      "Epoch 287/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2455 - val_loss: 0.2381\n",
      "Epoch 288/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2428 - val_loss: 0.2537\n",
      "Epoch 289/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2562 - val_loss: 0.2653\n",
      "Epoch 290/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2440 - val_loss: 0.2501\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2409 - val_loss: 0.2284\n",
      "Epoch 292/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2397 - val_loss: 0.2543\n",
      "Epoch 293/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2387 - val_loss: 0.2302\n",
      "Epoch 294/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2383 - val_loss: 0.2177\n",
      "Epoch 295/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2417 - val_loss: 0.2303\n",
      "Epoch 296/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2397 - val_loss: 0.2748\n",
      "Epoch 297/500\n",
      "5000/5000 [==============================] - 1s 276us/step - loss: 0.2356 - val_loss: 0.2018\n",
      "Epoch 298/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2484 - val_loss: 0.2142\n",
      "Epoch 299/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2436 - val_loss: 0.2241\n",
      "Epoch 300/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2428 - val_loss: 0.2381\n",
      "Epoch 301/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.2422 - val_loss: 0.2168\n",
      "Epoch 302/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2423 - val_loss: 0.2187\n",
      "Epoch 303/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2419 - val_loss: 0.2287\n",
      "Epoch 304/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.2356 - val_loss: 0.2010\n",
      "Epoch 305/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2421 - val_loss: 0.2077\n",
      "Epoch 306/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2378 - val_loss: 0.2088\n",
      "Epoch 307/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2313 - val_loss: 0.2929\n",
      "Epoch 308/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.2440 - val_loss: 0.2293\n",
      "Epoch 309/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2426 - val_loss: 0.2323\n",
      "Epoch 310/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2388 - val_loss: 0.2698\n",
      "Epoch 311/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2369 - val_loss: 0.2218\n",
      "Epoch 312/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2302 - val_loss: 0.2099\n",
      "Epoch 313/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2361 - val_loss: 0.2676\n",
      "Epoch 314/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2318 - val_loss: 0.2263\n",
      "Epoch 315/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.2255 - val_loss: 0.2764\n",
      "Epoch 316/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2271 - val_loss: 0.2052\n",
      "Epoch 317/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2305 - val_loss: 0.2469\n",
      "Epoch 318/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2276 - val_loss: 0.2568\n",
      "Epoch 319/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.2268 - val_loss: 0.1886\n",
      "Epoch 320/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.2225 - val_loss: 0.2054\n",
      "Epoch 321/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2284 - val_loss: 0.2099\n",
      "Epoch 322/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2295 - val_loss: 0.1945\n",
      "Epoch 323/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2226 - val_loss: 0.2309\n",
      "Epoch 324/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.2268 - val_loss: 0.1791\n",
      "Epoch 325/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2213 - val_loss: 0.2197\n",
      "Epoch 326/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2147 - val_loss: 0.2060\n",
      "Epoch 327/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2130 - val_loss: 0.2197\n",
      "Epoch 328/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2120 - val_loss: 0.2181\n",
      "Epoch 329/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2185 - val_loss: 0.1942\n",
      "Epoch 330/500\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.2098 - val_loss: 0.1859\n",
      "Epoch 331/500\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 0.2161 - val_loss: 0.1793\n",
      "Epoch 332/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2124 - val_loss: 0.2409\n",
      "Epoch 333/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2118 - val_loss: 0.1645\n",
      "Epoch 334/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2123 - val_loss: 0.2118\n",
      "Epoch 335/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.2071 - val_loss: 0.2038\n",
      "Epoch 336/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2078 - val_loss: 0.1853\n",
      "Epoch 337/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.1945 - val_loss: 0.2214\n",
      "Epoch 338/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.2151 - val_loss: 0.2320\n",
      "Epoch 339/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1979 - val_loss: 0.2770\n",
      "Epoch 340/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1971 - val_loss: 0.1620\n",
      "Epoch 341/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2022 - val_loss: 0.1747\n",
      "Epoch 342/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.2109 - val_loss: 0.2184\n",
      "Epoch 343/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1992 - val_loss: 0.1712\n",
      "Epoch 344/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2012 - val_loss: 0.2559\n",
      "Epoch 345/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1968 - val_loss: 0.1566\n",
      "Epoch 346/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1997 - val_loss: 0.2415\n",
      "Epoch 347/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1976 - val_loss: 0.2530\n",
      "Epoch 348/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2037 - val_loss: 0.2256\n",
      "Epoch 349/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1953 - val_loss: 0.2302\n",
      "Epoch 350/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1948 - val_loss: 0.1536\n",
      "Epoch 351/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.2004 - val_loss: 0.1548\n",
      "Epoch 352/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1963 - val_loss: 0.1846\n",
      "Epoch 353/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.1964 - val_loss: 0.1926\n",
      "Epoch 354/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1906 - val_loss: 0.1869\n",
      "Epoch 355/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1905 - val_loss: 0.2234\n",
      "Epoch 356/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.1924 - val_loss: 0.2043\n",
      "Epoch 357/500\n",
      "5000/5000 [==============================] - 1s 260us/step - loss: 0.1861 - val_loss: 0.1735\n",
      "Epoch 358/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1925 - val_loss: 0.2323\n",
      "Epoch 359/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1889 - val_loss: 0.2211\n",
      "Epoch 360/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2040 - val_loss: 0.1378\n",
      "Epoch 361/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1870 - val_loss: 0.1450\n",
      "Epoch 362/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1926 - val_loss: 0.1394\n",
      "Epoch 363/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.2068 - val_loss: 0.2457\n",
      "Epoch 364/500\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 0.1873 - val_loss: 0.2356\n",
      "Epoch 365/500\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 0.1921 - val_loss: 0.1683\n",
      "Epoch 366/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.1926 - val_loss: 0.1565\n",
      "Epoch 367/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1925 - val_loss: 0.1479\n",
      "Epoch 368/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1832 - val_loss: 0.1571\n",
      "Epoch 369/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1847 - val_loss: 0.1609\n",
      "Epoch 370/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1916 - val_loss: 0.2033\n",
      "Epoch 371/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1812 - val_loss: 0.1565\n",
      "Epoch 372/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1867 - val_loss: 0.1331\n",
      "Epoch 373/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1809 - val_loss: 0.1939\n",
      "Epoch 374/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1823 - val_loss: 0.1430\n",
      "Epoch 375/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1890 - val_loss: 0.1884\n",
      "Epoch 376/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1787 - val_loss: 0.1311\n",
      "Epoch 377/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1747 - val_loss: 0.1401\n",
      "Epoch 378/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1774 - val_loss: 0.1847\n",
      "Epoch 379/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1702 - val_loss: 0.1740\n",
      "Epoch 380/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1842 - val_loss: 0.2443\n",
      "Epoch 381/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1814 - val_loss: 0.1358\n",
      "Epoch 382/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1753 - val_loss: 0.2082\n",
      "Epoch 383/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1821 - val_loss: 0.2308\n",
      "Epoch 384/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1722 - val_loss: 0.1274\n",
      "Epoch 385/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1810 - val_loss: 0.1431\n",
      "Epoch 386/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1757 - val_loss: 0.1416\n",
      "Epoch 387/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1781 - val_loss: 0.1357\n",
      "Epoch 388/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1806 - val_loss: 0.1315\n",
      "Epoch 389/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.1737 - val_loss: 0.1258\n",
      "Epoch 390/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1772 - val_loss: 0.1412\n",
      "Epoch 391/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1837 - val_loss: 0.1307\n",
      "Epoch 392/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1759 - val_loss: 0.1241\n",
      "Epoch 393/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.1752 - val_loss: 0.1186\n",
      "Epoch 394/500\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 0.1735 - val_loss: 0.1418\n",
      "Epoch 395/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.1716 - val_loss: 0.1268\n",
      "Epoch 396/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1805 - val_loss: 0.1254\n",
      "Epoch 397/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1706 - val_loss: 0.2038\n",
      "Epoch 398/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1804 - val_loss: 0.1330\n",
      "Epoch 399/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.1752 - val_loss: 0.1293\n",
      "Epoch 400/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1749 - val_loss: 0.1435\n",
      "Epoch 401/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1725 - val_loss: 0.1308\n",
      "Epoch 402/500\n",
      "5000/5000 [==============================] - 1s 264us/step - loss: 0.1731 - val_loss: 0.1419\n",
      "Epoch 403/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.1784 - val_loss: 0.1282\n",
      "Epoch 404/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1774 - val_loss: 0.1424\n",
      "Epoch 405/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1781 - val_loss: 0.2027\n",
      "Epoch 406/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1651 - val_loss: 0.1619\n",
      "Epoch 407/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1683 - val_loss: 0.1400\n",
      "Epoch 408/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1736 - val_loss: 0.1345\n",
      "Epoch 409/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1664 - val_loss: 0.1115\n",
      "Epoch 410/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.1697 - val_loss: 0.1496\n",
      "Epoch 411/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1722 - val_loss: 0.1213\n",
      "Epoch 412/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.1740 - val_loss: 0.1289\n",
      "Epoch 413/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1674 - val_loss: 0.2022\n",
      "Epoch 414/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1739 - val_loss: 0.1311\n",
      "Epoch 415/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1713 - val_loss: 0.1608\n",
      "Epoch 416/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1745 - val_loss: 0.1346\n",
      "Epoch 417/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1676 - val_loss: 0.1335\n",
      "Epoch 418/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1710 - val_loss: 0.2180\n",
      "Epoch 419/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1702 - val_loss: 0.1099\n",
      "Epoch 420/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.1788 - val_loss: 0.1358\n",
      "Epoch 421/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1713 - val_loss: 0.1293\n",
      "Epoch 422/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1679 - val_loss: 0.1181\n",
      "Epoch 423/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.1854 - val_loss: 0.1120\n",
      "Epoch 424/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1693 - val_loss: 0.1387\n",
      "Epoch 425/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1598 - val_loss: 0.1183\n",
      "Epoch 426/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1701 - val_loss: 0.1237\n",
      "Epoch 427/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1696 - val_loss: 0.1269\n",
      "Epoch 428/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1763 - val_loss: 0.1227\n",
      "Epoch 429/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1667 - val_loss: 0.1373\n",
      "Epoch 430/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1774 - val_loss: 0.1191\n",
      "Epoch 431/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1691 - val_loss: 0.1856\n",
      "Epoch 432/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1669 - val_loss: 0.1403\n",
      "Epoch 433/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1668 - val_loss: 0.1110\n",
      "Epoch 434/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1805 - val_loss: 0.1661\n",
      "Epoch 435/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1697 - val_loss: 0.1251\n",
      "Epoch 436/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1637 - val_loss: 0.2044\n",
      "Epoch 437/500\n",
      "5000/5000 [==============================] - 1s 275us/step - loss: 0.1597 - val_loss: 0.1695\n",
      "Epoch 438/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1668 - val_loss: 0.1193\n",
      "Epoch 439/500\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 0.1699 - val_loss: 0.1329\n",
      "Epoch 440/500\n",
      "5000/5000 [==============================] - 1s 250us/step - loss: 0.1675 - val_loss: 0.1893\n",
      "Epoch 441/500\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 0.1638 - val_loss: 0.1226\n",
      "Epoch 442/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1667 - val_loss: 0.1155\n",
      "Epoch 443/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1717 - val_loss: 0.1660\n",
      "Epoch 444/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1627 - val_loss: 0.1312\n",
      "Epoch 445/500\n",
      "5000/5000 [==============================] - 1s 274us/step - loss: 0.1641 - val_loss: 0.1286\n",
      "Epoch 446/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1642 - val_loss: 0.1518\n",
      "Epoch 447/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1683 - val_loss: 0.1083\n",
      "Epoch 448/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1593 - val_loss: 0.1614\n",
      "Epoch 449/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1696 - val_loss: 0.2230\n",
      "Epoch 450/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1603 - val_loss: 0.1125\n",
      "Epoch 451/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1631 - val_loss: 0.1201\n",
      "Epoch 452/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1748 - val_loss: 0.1601\n",
      "Epoch 453/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1581 - val_loss: 0.1077\n",
      "Epoch 454/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.1642 - val_loss: 0.1976\n",
      "Epoch 455/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.1523 - val_loss: 0.1171\n",
      "Epoch 456/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1609 - val_loss: 0.1807\n",
      "Epoch 457/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1643 - val_loss: 0.1612\n",
      "Epoch 458/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.1547 - val_loss: 0.1171\n",
      "Epoch 459/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.1636 - val_loss: 0.1217\n",
      "Epoch 460/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1599 - val_loss: 0.1167\n",
      "Epoch 461/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1694 - val_loss: 0.1805\n",
      "Epoch 462/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1665 - val_loss: 0.1142\n",
      "Epoch 463/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1717 - val_loss: 0.1187\n",
      "Epoch 464/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1622 - val_loss: 0.1381\n",
      "Epoch 465/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1624 - val_loss: 0.1472\n",
      "Epoch 466/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1773 - val_loss: 0.1665\n",
      "Epoch 467/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1620 - val_loss: 0.1358\n",
      "Epoch 468/500\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 0.1628 - val_loss: 0.1426\n",
      "Epoch 469/500\n",
      "5000/5000 [==============================] - 1s 266us/step - loss: 0.1537 - val_loss: 0.1256\n",
      "Epoch 470/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.1611 - val_loss: 0.1196\n",
      "Epoch 471/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1542 - val_loss: 0.1077\n",
      "Epoch 472/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1537 - val_loss: 0.1573\n",
      "Epoch 473/500\n",
      "5000/5000 [==============================] - 1s 267us/step - loss: 0.1579 - val_loss: 0.1262\n",
      "Epoch 474/500\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 0.1529 - val_loss: 0.1126\n",
      "Epoch 475/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1622 - val_loss: 0.1130\n",
      "Epoch 476/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1676 - val_loss: 0.1179\n",
      "Epoch 477/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1740 - val_loss: 0.1212\n",
      "Epoch 478/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1540 - val_loss: 0.1046\n",
      "Epoch 479/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1535 - val_loss: 0.1024\n",
      "Epoch 480/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1584 - val_loss: 0.1707\n",
      "Epoch 481/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1653 - val_loss: 0.1241\n",
      "Epoch 482/500\n",
      "5000/5000 [==============================] - 1s 277us/step - loss: 0.1606 - val_loss: 0.1791\n",
      "Epoch 483/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1647 - val_loss: 0.1282\n",
      "Epoch 484/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1566 - val_loss: 0.1690\n",
      "Epoch 485/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1771 - val_loss: 0.1612\n",
      "Epoch 486/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1671 - val_loss: 0.1317\n",
      "Epoch 487/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1666 - val_loss: 0.1108\n",
      "Epoch 488/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1521 - val_loss: 0.1120\n",
      "Epoch 489/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1583 - val_loss: 0.1135\n",
      "Epoch 490/500\n",
      "5000/5000 [==============================] - 1s 269us/step - loss: 0.1612 - val_loss: 0.1252\n",
      "Epoch 491/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1570 - val_loss: 0.1533\n",
      "Epoch 492/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1524 - val_loss: 0.1316\n",
      "Epoch 493/500\n",
      "5000/5000 [==============================] - 1s 273us/step - loss: 0.1587 - val_loss: 0.1272\n",
      "Epoch 494/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1653 - val_loss: 0.1458\n",
      "Epoch 495/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1522 - val_loss: 0.1134\n",
      "Epoch 496/500\n",
      "5000/5000 [==============================] - 1s 272us/step - loss: 0.1554 - val_loss: 0.1063\n",
      "Epoch 497/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1682 - val_loss: 0.1067\n",
      "Epoch 498/500\n",
      "5000/5000 [==============================] - 1s 270us/step - loss: 0.1517 - val_loss: 0.1302\n",
      "Epoch 499/500\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 0.1472 - val_loss: 0.1366\n",
      "Epoch 500/500\n",
      "5000/5000 [==============================] - 1s 268us/step - loss: 0.1498 - val_loss: 0.0998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc64684aa90>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,50))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(50,)) \n",
    "###Lets Add another layer and an Activation###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dropout(0.5)(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dropout(0.5)(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.summary()\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X,Y,epochs=500,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "X_test=np.random.uniform(0,10,size=(100,50))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load some input data in the 2-split format\n",
    "index_from=3\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=index_from)\n",
    "\n",
    "\n",
    "word_2_index={k:(v+index_from) for k,v in tf.keras.datasets.imdb.get_word_index().items()}\n",
    "word_2_index['<PAD>']=0\n",
    "word_2_index['<START>']=1\n",
    "word_2_index['<UNK>']=2\n",
    "\n",
    "index_2_word={}\n",
    "\n",
    "for word in word_2_index:\n",
    "    index_2_word[ word_2_index[word]]=word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data\n",
    "\n",
    "The coding/research part of most machine learning algorithms is how to utilize data in a way an algorthim understands\n",
    "\n",
    "For this data we assign each word in a sentence a unique integer\n",
    "include a token (integer for unknown, pad, and start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence'] [14, 9, 6, 4130]\n"
     ]
    }
   ],
   "source": [
    "check=['this','is','a','sentence']\n",
    "print(check, [word_2_index[i] for i in check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word=np.max(list(word_2_index.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "1 = Positive Review 0 = Negative Review \n",
      "label 1\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "print(\" \".join([index_2_word[i] for i in x_train[0]]))\n",
    "\n",
    "print('1 = Positive Review','0 = Negative Review ')\n",
    "print('label',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Read the raw the raw text from these movie reviews, and predict wether the review is positive or not\n",
    "* Need to go from an array (1-D unknown length) to a probability (1 number)\n",
    "* Need to build a series of layers to make that possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Array of Ints -> **Embedding** -> Array of Vectors -> **RNN** -> fixed output -> **Dense** -> Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_layer=tf.keras.layers.Input( (None,))\n",
    "print(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=tf.keras.layers.Embedding(last_word,100)(input_layer)\n",
    "nn=tf.keras.layers.LSTM(10)(nn)\n",
    "nn=tf.keras.layers.Dense(10)(nn)\n",
    "nn=tf.keras.layers.LeakyReLU()(nn)\n",
    "nn=tf.keras.layers.Dropout()(nn)\n",
    "output=tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "\n",
    "model=tf.keras.models.Model(input_layer,output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200, dtype='int32',value=0.0)\n",
    "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200, dtype='int32',value=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_25 to have shape (50,) but got array with shape (200,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-1aafbc44a878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 993\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    326\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_25 to have shape (50,) but got array with shape (200,)"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_2_ints(sentence):\n",
    "    return np.array([[word_2_index[s] for s in sentence.split()]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8690597]]\n",
      "[[0.7399085]]\n",
      "[[0.83663946]]\n",
      "[[0.2783985]]\n",
      "[[0.8146159]]\n",
      "[[0.5867508]]\n",
      "[[0.5854596]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(sentence_2_ints('<START> this movie is the very best i have ever seen') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> i have mixed feelings about this movie') ))\n",
    "print(model.predict(sentence_2_ints('<START> i have mixed feelings about this movie it may like it in the end') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> i have never seen a worse film') ))\n",
    "print(model.predict(sentence_2_ints('<START> hi is this where i google the information') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> star trek') ))\n",
    "print(model.predict(sentence_2_ints('<START> star wars') ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
