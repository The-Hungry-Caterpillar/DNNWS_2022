{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "Dense Neural Networks are an essential building block used in Deep Learning for image analysis. Our goals here are:\n",
    "* Learn how to use a Neural Network in Classification\n",
    "* How do I use a trained neural network\n",
    "    * Using a Neural Network to predict good wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification is a problem where each of our examples (x) belongs to a class (y). Since Neural networks are universal function approximators, we can use\n",
    "\n",
    "$P(y|x)$\n",
    "\n",
    "\n",
    "\n",
    "## Binary Classification\n",
    "When we just have 2 classes (A or B/True or False) then we only need to predict the probability of one since\n",
    "\n",
    "$P(y=b|x)$=1-$P(y=a|x)$\n",
    "\n",
    "Our network inputs are our features, and our network output is $P(y=a|x)$.\n",
    "\n",
    "**Important:** We need to use the correct activation function and the correct loss function. We know $y_{pred}$\n",
    "is between zero and one, so we can add a 'sigmoid' activation\n",
    "\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "<img src='../assets/sigmoid.png'>\n",
    "\n",
    "For the loss function we use something called binary cross-entropy, which uses our labels (0 or 1) to train our network.\n",
    "\n",
    "$L=\\frac{1}{N}*\\sum_i(-y_{true,i}*ln(y_{pred,i})-(1-y_{true,i})*ln(1-y_{pred,i}))$\n",
    "\n",
    "Since our labels are only 0 or 1 this loss will \n",
    "* Make $-1*ln(y_{pred})$ as small as possible ($y_{pred}=1$) when $y_{true}=1$\n",
    "* Make $-1*ln(1-y_{pred})$ as small as possible ($y_{pred}=0$) when $y_{true}=0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## If you're curious\n",
    "\n",
    "This is essentially the negative log likelihood of a Bernoulli distribution\n",
    "\n",
    "$P(y,p)=p^{y}(1-p)^{1-y}$\n",
    "\n",
    "$-ln(P(y,p))=-ln(p^{y}(1-p)^{1-y})= -y*ln(p)-(1-y)*ln(1-p)$\n",
    "\n",
    "# What it Means for our Neural Network\n",
    "\n",
    "* When we build our model we need to use a sigmoid activation on the last layer\n",
    "\n",
    "    output=tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    \n",
    "    **or**\n",
    "    \n",
    "    dense=tf.keras.layers.Dense(1)\n",
    "    \n",
    "    output=tf.keras.layers.Activation('sigmoid')(dense)\n",
    "    \n",
    "    \n",
    "* When we compile a model we need to use \"binary_crossentropy\" as a loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class A\n",
    "n_data=10000\n",
    "\n",
    "y_train=np.ones(n_data) #Class A =1\n",
    "y_develop=np.ones(n_data)\n",
    "y_test=np.ones(n_data)\n",
    "\n",
    "x_train=np.random.normal(1,1,size=n_data)\n",
    "x_develop=np.random.normal(1,1,size=n_data)\n",
    "x_test=np.random.normal(1,1,size=n_data)\n",
    "\n",
    "\n",
    "#Class B\n",
    "y_train=np.append(y_train,np.zeros(n_data)) #Class B=0\n",
    "y_develop=np.append(y_develop,np.zeros(n_data)) \n",
    "y_test=np.append(y_test,np.zeros(n_data)) \n",
    "\n",
    "x_train=np.append(x_train,np.random.normal(-1,1,size=n_data))\n",
    "x_develop=np.append(x_develop,np.random.normal(-1,1,size=n_data))\n",
    "x_test=np.append(x_test,np.random.normal(-1,1,size=n_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple way to look at a feature is a histogram\n",
    "Plot how many times does class A have a feature value within a bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for class A\n",
    "a_mask=y_train==1\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-4,4),histtype='step',label='Class A')\n",
    "\n",
    "#Histogram for Class B\n",
    "b_mask=y_train==0\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-4,4),histtype='step',label='Class B')\n",
    "\n",
    "plt.hist(x_train,bins=16,range=(-4,4),histtype='step',label='Input Data')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('N Examples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(1,)) \n",
    "###Same as Before###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "#Sigmoid!!!!\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "#Binary Cross Entropy!!!\n",
    "nn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(x_train,y_train,validation_data=(x_develop,y_develop),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test=np.linspace(-4,4,100)\n",
    "Y_true=1/(1+np.exp(-2*X_test))#Proof Left to Reader\n",
    "\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "plt.scatter(X_test,Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test,Y_true,label='Truth',marker='+')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('P(y=a|x)')\n",
    "\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-4,4),histtype='step',label='Class B',density=True)\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-4,4),histtype='step',label='Class A',density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Multi-Class\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class A\n",
    "n_data=10000\n",
    "\n",
    "y_train=np.ones(n_data) #Class A =1\n",
    "y_develop=np.ones(n_data)\n",
    "y_test=np.ones(n_data)\n",
    "\n",
    "x_train=np.random.normal(1,1,size=n_data)\n",
    "x_develop=np.random.normal(1,1,size=n_data)\n",
    "x_test=np.random.normal(1,1,size=n_data)\n",
    "\n",
    "\n",
    "#Class B\n",
    "y_train=np.append(y_train,np.zeros(n_data)) #Class B=0\n",
    "y_develop=np.append(y_develop,np.zeros(n_data)) \n",
    "y_test=np.append(y_test,np.zeros(n_data)) \n",
    "\n",
    "x_train=np.append(x_train,np.random.normal(-1,1,size=n_data))\n",
    "x_develop=np.append(x_develop,np.random.normal(-1,1,size=n_data))\n",
    "x_test=np.append(x_test,np.random.normal(-1,1,size=n_data))\n",
    "\n",
    "#Class B\n",
    "y_train=np.append(y_train,np.ones(n_data)*2) #Class B=0\n",
    "y_develop=np.append(y_develop,np.zeros(n_data)) \n",
    "y_test=np.append(y_test,np.zeros(n_data)) \n",
    "\n",
    "x_train=np.append(x_train,np.random.normal(-3,1,size=n_data))\n",
    "x_develop=np.append(x_develop,np.random.normal(-1,1,size=n_data))\n",
    "x_test=np.append(x_test,np.random.normal(-3,1,size=n_data))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for class A\n",
    "a_mask=y_train==1\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-7,5),histtype='step',label='Class A')\n",
    "\n",
    "#Histogram for Class B\n",
    "b_mask=y_train==0\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-7,5),histtype='step',label='Class B')\n",
    "\n",
    "c_mask=y_train==2\n",
    "plt.hist(x_train[c_mask],bins=16,range=(-7,5),histtype='step',label='Class C')\n",
    "\n",
    "plt.hist(x_train,bins=16,range=(-7,5),histtype='step',label='Input Data')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('N Examples')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "\n",
    "**Reminder**\n",
    "   * Classification is problem where each of our examples (x) belongs to a class (y). Since Neural networks are universal function approximators, we can use $P(y|x)$\n",
    "\n",
    "**Like before to change our problem we need**\n",
    "* The correct activation on our last layer - **softmax**\n",
    "* The correct loss function - **categorical_crossentropy**\n",
    "\n",
    "We have more than two classes (0,1,2...) and we need to predict the probability of all of them. However, we have a constraint that all the probabilities must sum to one.\n",
    "\n",
    "**Our network**\n",
    " * Inputs are our images\n",
    " * Output is a Dense layer with dimension equal to the number of classes\n",
    "     * Each output represents $\\{P(y=0|x),(y=1|x),(y=2|x)\\ ...\\}$.\n",
    " * We require $\\sum_i P(y=i|x) = 1$.\n",
    "\n",
    "* To enforce this we use a different activation function: a **softmax**\n",
    "\n",
    "    * $\\sigma(x)_i= \\frac{e^{x_i}}{\\sum_i e^{x_i}}$\n",
    "    \n",
    "* Our loss function becomes\n",
    "\n",
    " $L=-\\frac{1}{N}\\sum_i \\sum_n y_{true,i,n}*ln(y_{pred,i,n})$\n",
    "\n",
    "* What this means\n",
    "    * $y_{true,i,n}$ is a vector with a 1 in the dimention that example belongs to and a zero everywhere else\n",
    "        *  i.e. Ankle boot = class 9 = (0,0,0,0,0,0,0,0,0,1)\n",
    "    * The sum in this loss term  $\\sum_n y_{true,i,n}*ln(y_{pred,i,n})$\n",
    "        * is zero except for the one value when n=class of $y_{true}$\n",
    "        * Then it's just $ln(y_{pred,i,n})$\n",
    "        * This is same as binary classfication: make -1*$ln(y_{pred,i,n})$ as small as possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data set has labels stored as integers, but the labels we need for our loss function need to be  **one-hot** encoded\n",
    "\n",
    "**one-hot** - A vector of zeros except for one entry with a 1 that represents the class of an object\n",
    "   * i.e. Ankle boot = class 9 = (0,0,0,0,0,0,0,0,0,1)\n",
    "\n",
    "keras has a utility to convert integers like this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
    "y_develop_one_hot =  tf.keras.utils.to_categorical(y_develop, 3)\n",
    "y_test_one_hot =  tf.keras.utils.to_categorical(y_test, 3)\n",
    "\n",
    "print('Example:',y_train[0],'=',y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(1,)) \n",
    "###Same as Before###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "#Sigmoid!!!!\n",
    "output_layer = tf.keras.layers.Dense(3,activation='softmax')(nn)\n",
    "\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "#Binary Cross Entropy!!!\n",
    "nn_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(x_train,y_train_one_hot,validation_data=(x_develop,y_develop_one_hot),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.linspace(-7,4,100)\n",
    "#Y_true=1/(1+np.exp(-2*X_test))#Proof Left to Reader\n",
    "\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "\n",
    "\n",
    "plt.scatter(X_test,Y_pred[:,0],label='Prediction A',marker='x')\n",
    "plt.scatter(X_test,Y_pred[:,1],label='Prediction B',marker='x')\n",
    "plt.scatter(X_test,Y_pred[:,2],label='Prediction C',marker='x')\n",
    "\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('P(y=a|x)')\n",
    "\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-7,4),histtype='step',label='Class B',density=True)\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-7,4),histtype='step',label='Class A',density=True)\n",
    "plt.hist(x_train[c_mask],bins=16,range=(-7,4),histtype='step',label='Class C',density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense network summary\n",
    "\n",
    "* Dense networks take fixed length input and have a fixed length output\n",
    "* Like All Neural Network layers they require an activation function\n",
    "    * The activation of the last layer is usally determined by the goal\n",
    "        * Regression - Linear\n",
    "        * Classification - Sigmoid\n",
    "        * Multi-Class Classification - Softmax (more later)\n",
    "* They can be stacked to represent more complicated functions\n",
    "* You're taking your chances when predicting data that's very different from you're training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real World Example\n",
    "We're going to use a number of measured values to try and predict good wine.\n",
    "Source:https://archive.ics.uci.edu/ml/datasets/Wine+Quality (Cortez et al., 2009)\n",
    "\n",
    "* Goal: Predict whether a wine is excellent based score given to the wine by an expert panel (1-10)\n",
    "    * Score > 5 Excellent\n",
    "    * Score <=5 Average\n",
    "\n",
    "* Features:\n",
    "  * 1 - fixed acidity\n",
    "  * 2 - volatile acidity\n",
    "  * 3 - citric acid\n",
    "  * 4 - residual sugar\n",
    "  * 5 - chlorides\n",
    "  * 6 - free sulfur dioxide\n",
    "  * 7 - total sulfur dioxide\n",
    "  * 8 - density\n",
    "  * 9 - pH\n",
    "  * 10 - sulphates\n",
    "  * 11 - alcohol\n",
    "  * 12 - Red/White\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winedata\n",
    "import importlib\n",
    "importlib.reload(winedata)\n",
    "\n",
    "\n",
    "wd=winedata.WineData()\n",
    "print(wd.x_train.shape,wd.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.y_develop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make histograms for each of our 12 input features, and check to see how many wines are labeled Excellent vs. Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(12):\n",
    "    good=[]\n",
    "    bad=[]\n",
    "    for vals,label in zip(wd.x_train,wd.y_train):\n",
    "        if label:\n",
    "            good.append(vals[n])\n",
    "        else:\n",
    "            bad.append(vals[n])\n",
    "\n",
    "    label=wd.header[n]\n",
    "    \n",
    "    \n",
    "    max_range=max([max(good),max(bad)])\n",
    "    min_range=min([min(good),min(bad)])\n",
    "    \n",
    "    plt.hist(good,density=True,histtype='step',label='Excellent',range=(min_range,max_range),bins=20)\n",
    "    plt.hist(bad,density=True,histtype='step',label='Average',range=(min_range,max_range),bins=20)\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print(\"Excellent Data Points\",len(good))\n",
    "print(\"Average Data Points\",len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(12,)) \n",
    "###Lets Add another layer and an Activation\n",
    "nn = tf.keras.layers.Dense(10)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "wine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "wine_model.summary()\n",
    "wine_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_model.fit(wd.x_train,wd.y_train,epochs=50,validation_data=(wd.x_develop,wd.y_develop)) #Have Keras make a test/validation split for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "pred=wine_model.predict(wd.x_develop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape,wd.y_develop.shape) \n",
    "\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if i],bins=50,range=(0,1),density=True,histtype='step',label=\"Excellent\")\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if not i],bins=50,range=(0,1),density=True,histtype='step',label=\"Average\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('NN Output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "for n in range(12):\n",
    "    good=[]\n",
    "    bad=[]\n",
    "    for i,v in enumerate(pred[:,0]):\n",
    "        if v >.8:\n",
    "            good.append(wd.x_develop[i,n])\n",
    "        else:\n",
    "            bad.append(wd.x_develop[i,n])\n",
    "    print(len(good),len(bad))\n",
    "    label=wd.header[n]\n",
    "    max_range=max([max(good),max(bad)])\n",
    "    min_range=min([min(good),min(bad)])\n",
    "    \n",
    "    plt.hist(good,density=True,histtype='step',label='Excellent',range=(min_range,max_range),bins=20)\n",
    "    plt.hist(bad,density=True,histtype='step',label='Average',range=(min_range,max_range),bins=20)\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we happy with the model's preformance on the develop set? \n",
    "* Yes let's confirm with the test set\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the evaluate function to caculate the loss and accuracy of different datasets\n",
    "develop_loss,develop_accuracy=wine_model.evaluate(wd.x_develop,wd.y_develop)\n",
    "\n",
    "test_loss,test_accuracy=wine_model.evaluate(wd.x_test,wd.y_test)\n",
    "\n",
    "print(\"Develop loss\",develop_loss, \" Develop Accuracy\", develop_accuracy)\n",
    "print(\"Test loss\",test_loss, \" Test Accuracy\", test_accuracy)\n",
    "print(\"Test Loss Differs from develop loss by \",round((test_loss-develop_loss)/(test_loss)*100,2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test loss should only slightly differ from the develop loss, if that's true you have a well trained network ready to evaluate new wine.\n",
    "\n",
    "\n",
    "# Try it yourself can you train a better neural network?\n",
    "\n",
    "\n",
    "* Start with the network above\n",
    "* Try adding layers, or hidden units\n",
    "* Watch the val_loss when training make sure it doesn't start going up \n",
    "* if it does stop the training \n",
    "* **Don't use test data yet!** we'll see who has the best accuracy on the test data at the end of the class \n",
    "\n",
    "## A few extra tools to help you\n",
    "* Early Stopping can be used to automatically stop training if the val_loss stops going down\n",
    "\n",
    "```python\n",
    "stop_early=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\n",
    "```\n",
    "* patience = how many epochs to wait without improvement before stopping\n",
    "* restore_best_weights = rewind to weights with the best validation loss!\n",
    "* monitor = what early stopping is looking at, val_loss is normal, but you can also use val_accuracy or other metrics\n",
    "\n",
    "You can use normalizations or not in your model, it is a layer that can be added like any other\n",
    "\n",
    "```python\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)(input_layer)\n",
    "```\n",
    "\n",
    "\n",
    "## Callbacks and Mertrics\n",
    "When fitting a function you can use callbacks and metrics to control the process\n",
    " \n",
    "**EarlyStopping is a callback** - callbacks run at the end of an epoch and are functions that cant stop training\n",
    " \n",
    " \n",
    "* Callbacks are added when you fit\n",
    "  \n",
    "```python\n",
    "wine_model.fit(wd.x_train,wd.y_train,validation_data=(wd.x_develop,wd.y_develop),callbacks=[stop_early])\n",
    "```\n",
    "    \n",
    "    \n",
    "**Metrics** are extra information that is calculated alongside the loss when fitting like 'accuracy'\n",
    "  * metrics are added to the model when you compile\n",
    "  \n",
    "  \n",
    " ```python\n",
    "    wine_model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=lr),metrics=['accuracy'])\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "# Remember\n",
    "* More layers and neurons give you better flexibility, but take longer to train and you need to worry more about overfitting\n",
    "* Your input layer needs to be the same size as the number of features you'll use\n",
    "* Your out layer needs to have the same size and activation function to match the problem\n",
    "* Learning rates, batch sizes, training, and early stopping can all be used! As well as any other tricks you might know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Answer? Check the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss,test_accuracy=my_model.evaluate(wd.x_test,wd.y_test)\n",
    "print(\"Test loss\",test_loss, \" Test Accuracy\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
