{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check for Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "print(\"If this cell fails please check your Open OnDemand Setup\", \n",
    "      \"Make sure you requested enough resources and the correct enviroment\")\n",
    "\n",
    "assert(tf.__version__==\"2.6.0\")\n",
    "assert(int(os.environ[\"SLURM_MEM_PER_NODE\"]) > 8000 )\n",
    "assert(int(os.environ[\"SLURM_CPUS_ON_NODE\"]) >= 2 )\n",
    "\n",
    "print('Your setup looks good!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Programming Powerup\n",
    "\n",
    "Numpy arrays have a large number of useful ways to access data, we'll talk about two in this powerup!\n",
    "\n",
    "## Masking\n",
    "A handy way to select elements in NumPy is masking:\n",
    "\n",
    "* This lets you easily do things like Select X's for example where Y=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Here is some fake data\n",
    "Xs=np.random.uniform(size=(20))\n",
    "Ys=np.random.uniform(size=(20)) > .8 #80% of our Ys should be true\n",
    "\n",
    "print(Xs)\n",
    "print(Ys)\n",
    "\n",
    "# Here we'll use as mask to grab all True Xs\n",
    "mask=(Ys==1)\n",
    "\n",
    "print(mask)\n",
    "\n",
    "print(Xs[mask])\n",
    "#Or since Ys are true or false \n",
    "print(Xs[Ys])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lists as Indices\n",
    "A handy way to select elements in NumPy is using lists:\n",
    "\n",
    "* This lets you easily do things like Select The 3 Biggest Elements in X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some other Handy Tricks\n",
    "\n",
    "Xs=np.random.uniform(size=(5)) #Input Data\n",
    "Ys=np.random.uniform(size=(5)) >.5 #Target Data\n",
    "\n",
    "print(\"Xs\",Xs)\n",
    "print(\"Ys\",Xs)\n",
    "\n",
    "#Lists for index's work too\n",
    "\n",
    "print(\"Selected Xs\",Xs[[1,2,4]])\n",
    "\n",
    "#This can be useful if you want to grab the labels for smallest values of x\n",
    "\n",
    "#This gives you index's of an array in a list\n",
    "sort_i=np.argsort(Xs)\n",
    "print('Sorted Xs',Xs[sort_i])\n",
    "print('Largest 3 Xs', Xs[sort_i[-3:]])\n",
    "\n",
    "biggest_index=sort_i[-1]\n",
    "smallest_index=sort_i[0]\n",
    "\n",
    "print('Label for Largest X ',Ys[biggest_index])\n",
    "print('Label for Smallest X ',Ys[smallest_index])\n",
    "\n",
    "print('The Same X 4 Times ',Xs[[1,1,1,1]])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs=np.random.uniform(size=(100)) #Input Data\n",
    "\"try to print the 5th,6th, and 8th, largest element in the Xs above in two lines of code\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes extensive use of examples and figures from [here](http://cs231n.github.io/convolutional-networks/), which is a great reference for further details.\n",
    "\n",
    "\n",
    "# GOALS\n",
    "\n",
    "* Understand how Image data is stored and used\n",
    "* Write a Multi-Class classification model\n",
    "* Be able to use convolutional layers\n",
    "* Build a network for Image Classification\n",
    "* Understand Over-fitting and some ways to deal with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: MNIST - Fashion\n",
    "\n",
    "For this example we'll use MNIST- Fashion, a collection of small 28x28 pixel images of various pieces of clothing. It is a common benchmark along with with the original MNIST which is a collection of hand written digits. We will load the data directly from keras.\n",
    "\n",
    "\n",
    "\n",
    "## The Task\n",
    "This is a multi-class classification problem, identify the type of object in the image\n",
    "\n",
    "|Label| Class  |\n",
    "|------ | ------|\n",
    "|    0|T-shirt/top|\n",
    "|    1|Trouser|\n",
    "|    2| Pullover|\n",
    "|    3| Dress|\n",
    "|    4| Coat|\n",
    "|    5| Sandal|\n",
    "|    6| Shirt|\n",
    "|    7| Sneaker|\n",
    "|    8| Bag|\n",
    "|    9| Ankle boot|\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll rely on tensorflow and the handy package Keras that comes with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from random import random\n",
    "from sys import version\n",
    "print(\"Import complete\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(_xtrain, _ytrain), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "#We want to include a develop set so let's split the training set\n",
    "train_index=[]\n",
    "develop_index=[]\n",
    "for i in range(len(_xtrain)):\n",
    "    if random() <0.8:\n",
    "        train_index.append(i)\n",
    "    else:\n",
    "        develop_index.append(i)\n",
    "X_train=_xtrain[train_index]\n",
    "Y_train=_ytrain[train_index]\n",
    "\n",
    "X_develop=_xtrain[develop_index]\n",
    "Y_develop=_ytrain[develop_index]\n",
    "\n",
    "\n",
    "np.set_printoptions(linewidth=115)\n",
    "n_targets=np.max(Y_test)+1\n",
    "print('A Single Image:\\n',X_train[0])\n",
    "plt.imshow(X_train[0],cmap='gray')\n",
    "plt.show()\n",
    "print('Example Label:', Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note above that the labels are integers from 0-9\n",
    "* Also note the images are integers from 0-255 (uint8)\n",
    "\n",
    "We will deal with the labels first. Lets make some useful arrays and dictionaries to keep track of what each integer means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is useful for making plots it takes an integer\n",
    "lookup_dict={\n",
    "    0 :'T-shirt/top',\n",
    "    1 :'Trouser',\n",
    "    2 :'Pullover',\n",
    "    3 :'Dress',\n",
    "    4 :'Coat',\n",
    "    5 :'Sandal',\n",
    "    6 :'Shirt',\n",
    "    7 :'Sneaker',\n",
    "    8 :'Bag',\n",
    "    9 :'Ankle boot' \n",
    "}\n",
    "\n",
    "\n",
    "#Lets make a list in the order of the labels above so [T-Shirt,Trouser,...]\n",
    "labels=list(lookup_dict.values())\n",
    "\n",
    "#Check to make sure labels list is in the right order (not guaranteed in python < 3.6)\n",
    "if not all([v==lookup_dict[i] for i,v in enumerate(labels) ]):\n",
    "    print('This looks like an old version of python making labels the long way, you are using python version', version)\n",
    "    labels=['' for i in range(n_targets) ] #make a list with the right size\n",
    "    for key in lookup_dict:\n",
    "        labels[key]=lookup_dict[key] #Assign list to the vaules\n",
    "        \n",
    "#Always good to make simple checks that what you think is going to work actually is working\n",
    "#Here we check that our array of labels is in the same order as the dictionary we wrote above\n",
    "assert(all([v==lookup_dict[i] for i,v in enumerate(labels) ]))\n",
    "print(\"Array and dictionary are in same order\")    \n",
    "\n",
    "#Another Simple Check (Keras is well tested this will work, but it's good to get in the habit when using your own data)\n",
    "assert(len(X_train)==len(Y_train))\n",
    "print(\"X_train and Y_train are the same length\") \n",
    "assert(len(X_develop)==len(Y_develop))\n",
    "print(\"X_develop and Y_develop are the same length\")   \n",
    "assert(len(X_test)==len(Y_test))\n",
    "print(\"X_test and Y_test are the same length\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "\n",
    "**Reminder**\n",
    "   * Classification is problem where each of our examples (x) belongs to a class (y). Since Neural networks are universal function approximators, we can use $P(y|x)$\n",
    "\n",
    "**Like before to change our problem we need**\n",
    "* The correct activation on our last layer - **softmax**\n",
    "* The correct loss function - **categorical_crossentropy**\n",
    "\n",
    "We have more than two classes (0,1,2...) and we need to predict the probability of all of them. However, we have a constraint that all the probabilities must sum to one.\n",
    "\n",
    "**Our network**\n",
    " * Inputs are our images\n",
    " * Output is a Dense layer with dimension equal to the number of classes\n",
    "     * Each output represents $\\{P(y=0|x),(y=1|x),(y=2|x)\\ ...\\}$.\n",
    " * We require $\\sum_i P(y=i|x) = 1$.\n",
    "\n",
    "* To enforce this we use a different activation function: a **softmax**\n",
    "\n",
    "    * $\\sigma(x)_i= \\frac{e^{x_i}}{\\sum_i e^{x_i}}$\n",
    "    \n",
    "* Our loss function becomes\n",
    "\n",
    " $L=-\\frac{1}{N}\\sum_i \\sum_n y_{true,i,n}*ln(y_{pred,i,n})$\n",
    "\n",
    "* What this means\n",
    "    * $y_{true,i,n}$ is a vector with a 1 in the dimention that example belongs to and a zero everywhere else\n",
    "        *  i.e. Ankle boot = class 9 = (0,0,0,0,0,0,0,0,0,1)\n",
    "    * The sum in this loss term  $\\sum_n y_{true,i,n}*ln(y_{pred,i,n})$\n",
    "        * is zero except for the one value when n=class of $y_{true}$\n",
    "        * Then it's just $ln(y_{pred,i,n})$\n",
    "        * This is same as binary classfication: make -1*$ln(y_{pred,i,n})$ as small as possible\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data set has labels stored as integers, but the labels we need for our loss function need to be  **one-hot** encoded\n",
    "\n",
    "**one-hot** - A vector of zeros except for one entry with a 1 that represents the class of an object\n",
    "   * i.e. Ankle boot = class 9 = (0,0,0,0,0,0,0,0,0,1)\n",
    "\n",
    "keras has a utility to convert integers like this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = tf.keras.utils.to_categorical(Y_train, 10)\n",
    "Y_develop_one_hot =  tf.keras.utils.to_categorical(Y_develop, 10)\n",
    "Y_test_one_hot =  tf.keras.utils.to_categorical(Y_test, 10)\n",
    "\n",
    "print('Example:',Y_train[0],'=',Y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets handle the image data\n",
    "* Our Convolutional Neural Networks need a shape of Batch x Height x Width x Channels, for us (batch_size x 28 x 28 x 1)\n",
    "* In this case channels=1, but for a color image you'll have 3 RGB and sometimes 4 with a transparency channel RGBA \n",
    "* It's much easier for a neural network to handle data with range from 0-1, rather than 0-255, so we will scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f=plt.figure(figsize=(15,3))\n",
    "plt.imshow(np.squeeze(np.hstack(X_train[0:7])),cmap='gray') #hstack aranges the first 7 images into one long image\n",
    "\n",
    "#Reshape\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_develop = X_develop.reshape(X_develop.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "print(\"Datatype:\",X_train.dtype, \"\\nMax value:\", X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pixel values imported as an integer array that saturates at `255`.  Let's turn the data into floats $\\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "if X_train.max()>1:  \n",
    "    X_train = X_train/255\n",
    "    X_test = X_test/255\n",
    "    X_develop = X_develop/255\n",
    "\n",
    "assert(np.max(X_train) <=1)\n",
    "assert(np.max(X_test) <=1)\n",
    "assert(np.max(X_develop) <=1)\n",
    "print(\"all sets scaled to float values between\", X_train.min(), \"and\", X_train.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Take Away\n",
    "\n",
    "* Image data is 3 dimensional (width,height,channel (i.e color) )\n",
    "    * It is often stored from 0-255 and should be normalized between 0-1\n",
    "* Class labels are given as integers and need to be converted to **one hot** vectors\n",
    "    \n",
    "* Multi-classification problems \n",
    "    * Use **softmax** as an output\n",
    "    * Use **Categorical Cross Entropy** as a loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Network for Image Classification\n",
    "\n",
    "* We can use everything we learned in Lesson 2 for Image classification\n",
    "* But we need one extra layer\n",
    "    * Dense Layers take 1-D data not 3-D data\n",
    "    * Convert the two by Flattening\n",
    "    * tf.keras.layers.Flatten()\n",
    "    \n",
    "All this does is reshape the input data\n",
    "\n",
    "$\\begin{pmatrix}a & b \\\\c & d\\end{pmatrix} \\rightarrow (a,b,c,d)$\n",
    "\n",
    "Let's try the network below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input( shape=X_train.shape[1:] ) # Shape here does not include the batch size \n",
    "\n",
    "## Here is our magic layer to turn image data into something a dense layer can use\n",
    "flat_input=tf.keras.layers.Flatten()(input_layer )#Dense layers take a shape of ( batch x features)\n",
    "##\n",
    "hidden_layer1=tf.keras.layers.Dense(100)(flat_input)    \n",
    "hidden_layer_activation=tf.keras.layers.LeakyReLU()(hidden_layer1)\n",
    "hidden_layer2=tf.keras.layers.Dense(100)(hidden_layer_activation)\n",
    "hidden_layer_activation=tf.keras.layers.LeakyReLU()(hidden_layer2)\n",
    "output_layer=tf.keras.layers.Dense(n_targets,activation='softmax')(hidden_layer_activation)\n",
    "dense_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "dense_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dense_model.summary()\n",
    "\n",
    "history=dense_model.fit(X_train, Y_train_one_hot, \n",
    "          batch_size=32, epochs=10, verbose=1,\n",
    "         validation_data=(X_develop,Y_develop_one_hot)\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curves\n",
    "\n",
    "The keras fit function returns a history object, that we've ignored until now, but it's a very important tool.\n",
    "It records the loss of the training and development datasets at each epoch, as well as metrics like accuracy.\n",
    "Let's plot the loss.\n",
    "\n",
    "**Most imporantly**\n",
    "* Is the development loss greater than the train loss?\n",
    "    * If so your model is overfit and will give worse performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll do this a lot so let's put it in a function\n",
    "def plot_history(history):     \n",
    "    plt.plot(history.history['loss'],label='Train')\n",
    "    plt.plot(history.history['val_loss'],label='Develop')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim((0,1.5*np.max(history.history['val_loss'])))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques to deal with over-fitting and we'll talk more about them latter, but the easiest way is to just stop the training earlier. You can do this with\n",
    "\n",
    "\n",
    "```keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)```\n",
    "\n",
    "This is a callback, or a function that can be used to control the fitting process. It's called at the end of every epoch, or even the end of every batch. We can use these functions by adding them to the fit functions with\n",
    "\n",
    "\n",
    "```\n",
    "model.fit(...,\n",
    "  callbacks=[func1,func2])\n",
    "  ```\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "history=dense_model.fit(X_train, Y_train_one_hot, \n",
    "          batch_size=32, epochs=10, verbose=1,\n",
    "         validation_data=(X_develop,Y_develop_one_hot),\n",
    "          callbacks=[es] \n",
    "                       )\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we picked up training where we left off the early stopping function quits training as soon as the develop loss stops going down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excerise\n",
    "\n",
    "With that let's practice writing our own Dense network image classifier\n",
    "We will a new dataset as an example cifar10\n",
    "\n",
    "\n",
    "labels=https://www.cs.toronto.edu/~kriz/cifar.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR data into train and test sets\n",
    "(_cfxtrain, _cfytrain), (cfX_test, cfY_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#Split into Train and Develop\n",
    "\n",
    "train_index=[]\n",
    "develop_index=[]\n",
    "for i in range(len(_cfxtrain)):\n",
    "    if random() <0.8:\n",
    "        train_index.append(i)\n",
    "    else:\n",
    "        develop_index.append(i)\n",
    "cfX_train=_cfxtrain[train_index]\n",
    "cfY_train=_cfytrain[train_index]\n",
    "\n",
    "cfX_develop=_cfxtrain[develop_index]\n",
    "cfY_develop=_cfytrain[develop_index]\n",
    "\n",
    "f=plt.figure(figsize=(15,3))\n",
    "plt.imshow(np.hstack(cfX_train[0:7])) #hstack aranges the first 7 images into one long image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Scale your data to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your code here normalize cfX_train/test/develop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_set in [cfX_train,cfX_develop,cfX_test]:\n",
    "    assert np.max(data_set)==1., 'Max of your data set is '+str(np.max(data_set))+' not 1'\n",
    "    assert np.min(data_set)==0., 'Max of your data set is '+str(np.min(data_set))+' not 0'\n",
    "\n",
    "print('Great job! Your dataset is normalized correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Create One-Hot encoded labels\n",
    "Name them:\n",
    "* cfY_train_one_hot\n",
    "* cfY_develop_one_hot\n",
    "* cfY_test_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'cfY_train_one_hot' in locals(),  'cfY_train_one_hot not found' \n",
    "assert 'cfY_develop_one_hot' in locals(),  'cfY_develop_one_hot not found' \n",
    "assert 'cfY_test_one_hot' in locals(),  'cfY_test_one_hot not found' \n",
    "\n",
    "assert (cfY_train_one_hot).shape[1]==10,  'cfY_train_one_hot not the correct size' \n",
    "assert (cfY_develop_one_hot).shape[1]==10,  'cfY_develop_one_hot not the correct size' \n",
    "assert (cfY_test_one_hot).shape[1]==10,  'cfY_test_one_hot not the correct size'\n",
    "print(\"One-Hot encoded labels created, correct size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Create a Dense Neural Network\n",
    "Write your own dense image classifier.\n",
    "\n",
    "Remeber you'll need: \n",
    "* an input layer\n",
    "* a flatten layer\n",
    "* some dense layers with activations\n",
    "* an output layer with a softmax activation\n",
    "\n",
    "Create and compile a model named **cifar_model**\n",
    "* Make sure the loss is catagorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'cifar_model' in locals(), \"Could not find cifar_model\"\n",
    "assert cifar_model.input_shape ==(None,32,32,3), \"Check your input shape is correct\"\n",
    "assert cifar_model.output_shape[1] ==10, \"Check your output shape is correct\"\n",
    "assert cifar_model._is_compiled, \"Make sure to compile your model\"\n",
    "assert cifar_model.loss=='categorical_crossentropy', \"Check your loss to make sure it's correct\"\n",
    "assert (np.abs(np.sum(cifar_model.predict(cfX_train[0:10]),axis=1)-1) < 1e-5).all(), \"Outputs don't sum to 1 make sure you have the correct activation\"\n",
    "\n",
    "print('Fantastic Job! It looks like your model is ready to fit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot your loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model.save(\"my_cifar_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model=tf.keras.models.load_model(\"my_cifar_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use your Model\n",
    "\n",
    "Will try a quick example with a photo. You can use mine or upload your own to Talapas\n",
    "\n",
    "* We need to open our photos and resize/reshape them for our model\n",
    "* We need rescale them to match training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_image=PIL.Image.open('/gpfs/projects/bgmp/shared/2019_ML_workshop/datasets/Test_Photos/dog.jpg')\n",
    "dog_array=np.asarray(dog_image)\n",
    "print(dog_array.shape)\n",
    "plt.imshow(dog_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length,width=dog_image.size\n",
    "min_length=min([length,width])\n",
    "\n",
    "#Crop a box with coordinates (left,top,right,bottom)  0,0 is the upper left corner\n",
    "new_image=dog_image.crop((0,0,min_length,min_length))\n",
    "print(\"Cropped Size\",new_image.size)\n",
    "new_image=new_image.resize((32,32))\n",
    "print(\"Resized Image\",new_image)\n",
    "\n",
    "new_array=np.asarray(new_image)\n",
    "plt.imshow(new_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it together into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(input_file):\n",
    "    image=PIL.Image.open(input_file)\n",
    "    length,width=dog_image.size\n",
    "    min_length=min([length,width])\n",
    "\n",
    "    #Crop a box with coordinates (left,top,right,bottom)  0,0 is the upper left corner\n",
    "    new_image=image.crop((0,0,min_length,min_length))\n",
    "    new_image=new_image.resize((32,32))\n",
    "    new_array=np.asarray(new_image)\n",
    "    new_array=new_array/255\n",
    "    new_array=np.expand_dims(new_array,axis=0)\n",
    "    return new_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_images=[\n",
    "'/gpfs/projects/bgmp/shared/2019_ML_workshop/datasets/Test_Photos/dog.jpg',\n",
    "    '/gpfs/projects/bgmp/shared/2019_ML_workshop/datasets/Test_Photos/bird.jpg',\n",
    "    '/gpfs/projects/bgmp/shared/2019_ML_workshop/datasets/Test_Photos/unknown.jpg'\n",
    "]\n",
    "\n",
    "data=[process_image(f) for f in my_images]\n",
    "print(\"First Image Shape\",data[0].shape)\n",
    "data=np.concatenate(data,axis=0)\n",
    "print(\"Data Set Shape\",data.shape)\n",
    "\n",
    "plt.imshow(np.hstack(data))\n",
    "print(\"Dataset max,min:\",np.max(data),np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_pred=cifar_model.predict(data)\n",
    "\n",
    "#\n",
    "\n",
    "photo_last_space=cifar_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_category=['airplane',\n",
    "                'automobile', \n",
    "                'bird',\n",
    "                'cat', \n",
    "                'deer', \n",
    "                'dog',\n",
    "                'frog', \n",
    "                'horse', \n",
    "                'ship', \n",
    "                'truck']\n",
    "\n",
    "for i,pred in enumerate(photo_pred):\n",
    "    sort_list=np.argsort(pred)\n",
    "    first=sort_list[-1]\n",
    "    second=sort_list[-2]\n",
    "    print(\"Image\",i,\" predicted as:\",cifar_category[first],round(pred[first]*100),\"%\",\" Second place:\",cifar_category[second],round(pred[second]*100),\"%\" )\n",
    "    _train_data=cfX_train[cfY_train[:,0]==first][0:5]\n",
    "    plt.imshow(np.hstack(_train_data))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How were your results?\n",
    "\n",
    "Mine were somewhat disappointing; this is one reason among many why convolutional neural networks, which we will see next time are almost always used in image analysis tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
