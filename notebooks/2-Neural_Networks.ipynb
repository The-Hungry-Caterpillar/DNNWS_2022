{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Reminder\n",
    "* Find f(x) such that f(x) best approximates y\n",
    "* Examples:\n",
    "    * Given some pixels (x) tell me the probability itâ€™s a cat (y)\n",
    "    * Given news articles (x) tell me a stocks value (y)\n",
    "    * Given some sequences x find some low dimensional space (z) that represent my data \n",
    "      * f1(x)=z f2(z)=x  \n",
    "\n",
    "# Outline\n",
    "* Dense (Fully Connected Neural Networks)\n",
    "  * Example Regression Fits\n",
    " \n",
    "# Goals\n",
    "\n",
    "Dense Neural Netrworks are an essential building block used in Deep Learning image analysis our goals are:\n",
    "* Know what a Dense Neural Network is\n",
    "    * Know what an activation function is and what it does\n",
    "* Know how to write a Dense Neural Network\n",
    "* How do I train a Dense Neural Network\n",
    "    * Experiment with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Packages\n",
    "\n",
    "We're going to be working primarily with Keras and Tensorflow. They're some alternatives like PyTorch, but they all allow you to build ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first Layer\n",
    "A Dense or fully connected layer\n",
    "\n",
    "<img src=\"../assets/network_diagrams/dense.png\">\n",
    "\n",
    "A dense layer has a connection between every input variable and every output node. Each connection is represented by a weight $W_{i,n}$ from and input $X_n$ to an output $O_i$. The output is a sum over all the input variables times there weights plus a bias $B_i$\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "We will need to fit this to data, which means finding the best values for $W_{i,n}$ and $B_i$ to approximate our data.\n",
    "\n",
    "We will often also stack layers $l$, so the output of one layer feeds into the next\n",
    "\n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$   \n",
    "\n",
    "\n",
    "* We'll discuss this more in later in the lecture, but it's important when stacking layers we use an non-linear activation function $\\sigma$\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use a Dense Network\n",
    "\n",
    "* When you have fixed input size and a fixed output size \n",
    "* When your input size isn't too big\n",
    "    * We'll have to add something extra for image data\n",
    "    \n",
    "# Practice Building Networks\n",
    "\n",
    "## Keras organizes a network by layers\n",
    "\n",
    "Look at the code below, It has \n",
    "* One input layer with an input size = 3 \n",
    "* One output layer and an output size = 1\n",
    "* A layer is connect to a previous layer by passing the previous layer as an argument\n",
    "  * i.e   \n",
    "  output_layer=**tf.keras.layers.Dense**(*these arguments initialize the layer* )(**input_layer** *this argument connects the layers* ) #this call is to connect to input layer\n",
    "  \n",
    "## Networks are wrapped up into a Model\n",
    "\n",
    "A model tells Keras which inputs/outputs you want to use for example\n",
    "\n",
    "**linear_model=tf.keras.models.Model(input_layer,output_layer)**\n",
    "\n",
    "you'll need this model to fit to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()\n",
    "model_type=type(linear_model)\n",
    "print(model_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above layer has 4 parameters (a weight for each connection, and one bias term)\n",
    "\n",
    "We can represent it like this\n",
    "<img src=\"../assets/network_diagrams/nn_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers can be stacked into more complex networks let's build this one\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_1.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "hidden_layer=tf.keras.layers.Dense(3)(input_layer) \n",
    "output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try building the following networks yourself\n",
    "\n",
    "Try writing code to make the following networks\n",
    "* 1. Create an Input Layer\n",
    "* 2. Write Dense layers with the right number of units\n",
    "* 3. Make Model name **my_model** using the input layer and your output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "print(my_model.layers[0].output_shape)\n",
    "\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==model_type, \"my model dosen't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==3, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 3\"\n",
    "assert my_model.layers[0].output_shape[0][1]==3, \"Input isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==3, \"Hidden layer isn't 3 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Output layer isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "my_model.layers[0]\n",
    "\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==model_type, \"my_model doesn't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==4, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 4\"\n",
    "assert my_model.layers[0].output_shape[0][1]==3, \"Input isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==3, \"Hidden 1 layer isn't 3 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Hidden 2 layer isn't 3 dimensional\"\n",
    "assert my_model.layers[3].output_shape[1]==3, \"Output layer isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_5_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==model_type, \"my_model doesn't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==3, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 4\"\n",
    "assert my_model.layers[0].output_shape[0][1]==3, \"Input isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==5, \"Hidden 1 layer isn't 5 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Hidden 2 layer isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Design\n",
    "* Input dimension is defined by the input data\n",
    "* Output dimension is defined by target data\n",
    "* Hidden layers add complexity to the model\n",
    "    * More hidden layers or larger hidden layer dimensions can represent more complicated functions\n",
    "    * Too many layers can be hard to train without special tricks\n",
    "        * Can overfit, or fail to train (more on that later)\n",
    "    * Too few layers may not correctly describe the data\n",
    "* The right balance depends on the problem\n",
    "    * Roughly the more data the more layers you can use\n",
    "    * The more complex the target the more layers you'll need\n",
    "* No right answer feel free to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting your Model\n",
    "Lets try to fit a simple line using the model below\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we define an Input layer and one Dense (Fully Connected Layer), in our equation above\n",
    "i=1 n=data_dim\n",
    "if data_dim ==1\n",
    "then \n",
    "\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i  = O_0 =  W_{0,0}*X_0+B_0$\n",
    "\n",
    "You'll notice from last lecture this is the same form as our linear model.\n",
    "\n",
    "* $y_{pred,i}=\\theta_{1}*x_{i}+\\theta_{2} $\n",
    "\n",
    "* Each 'neuron' in a dense network is one linear model\n",
    "\n",
    "in neural network lingo \n",
    "*  $W$ is called the weight matrix \n",
    "*  $B$ the bias\n",
    "*  $W$ is a matrix and can have several parameters and all the parameters in the network are often represented by just $\\theta$ \n",
    "\n",
    "Just as in our Linear model we are going to use the same loss function\n",
    "* $L=\\frac{1}{N}\\sum_i (y_{pred,i}-y_{true,i})^2$\n",
    "* which is Mean Squared Error or mse for short\n",
    "* and we will pick an optimizer 'adam'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Fit a slightly harder straight line\n",
    "\n",
    "We're going to make a data set where x is a series of 3 features, and a target value $y = 2*x_0+1$ \n",
    "\n",
    "$y$ is just a line with respect to $x_0$, and completely ignores $x_{1,2,3,4}$ \n",
    "\n",
    "This problem is a little bit more difficult than the 1st lecture since we need to learn that two of the input features don't correlate at all to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=3\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1  # #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#MSE= Mean Squared Error \n",
    "linear_model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Fit Our Simple Neural Network\n",
    "#Fit \n",
    "linear_model.fit(X,Y,epochs=100,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "#Pro tip, don't want to split the dataset yourself, you can have keras do it for you with validation_split=\n",
    "\n",
    "# Even more Pro-tip - be careful if you run this cell more than once you'll keep training the same model \n",
    "# with a different train/develop split each time which can cause the model to overfit both the train and develop\n",
    "# sets - a problem you'll only see when using the test set, so make sure to keep a test set around!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent you've fit your first neural network, now lets use it\n",
    "* we split our initial dataset into a train/develop using validation split\n",
    "* Lets make a new dataset that has X_0 from -5-15, with X_1,X_2 being random\n",
    "    * This is just a way to plot the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lets plot the output as a function of X_0\n",
    "\n",
    "#Create some Random 5-d data\n",
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "#Set the first dimention to be a line\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "\n",
    "#Get the True distribution from our test function\n",
    "Y_test=func(X_test)\n",
    "\n",
    "#Get the prediction from our model\n",
    "Y_pred=linear_model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth',marker='+')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Lets Look at it wrt X[:,1]\n",
    "plt.scatter(X_test[:,1],Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test[:,1],Y_test,label='truth',marker='+')\n",
    "plt.xlabel('X[:,1]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also look at a models weights\n",
    "We expect $W_{0,0}$=2, and $B_0$=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weights=linear_model.get_weights()\n",
    "print(\"W=\",weights[0])\n",
    "print(\"W[0,0]=\",weights[0][0,0])\n",
    "print(\"B=\",weights[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself \n",
    "Run the cell below to create a similar data set, but this time with some noise\n",
    "\n",
    "$y = 2*x_0+1+N(0,2)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=5\n",
    "\n",
    "X_train=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "X_test=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1 + np.random.normal(0,2,size=(len(X))) #Ignore all other input have the output only depend on the first dimention\n",
    "Y_train=func(X_train)\n",
    "Y_test=func(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write your Model\"\"\"\n",
    "\n",
    "\"Your Code Here\"\n",
    "\"split dataset\"\n",
    "\"Input\"\n",
    "\"Dense Layer\"\n",
    "\"Create Model\"\n",
    "\"Fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets try something a bit more complicated a sin wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=15,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Oops this didn't work. Why? So far what we wrote above can only represent linear functions\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "we need to add something called an activation function $\\sigma$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sigma(\\sum_n W_{i,n}*X_n+B_i)$    \n",
    "</p>\n",
    "\n",
    "$\\sigma$ has to be non-linear and a good choice is a LeakyReLU\n",
    "\n",
    "<img src='../assets/leakyReLU.png'>\n",
    "\n",
    "an activation can be added just like any other layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(input_layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=20,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Still not very good, let's add an activated hidden layer to get more flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "hidden_layer = tf.keras.layers.Dense(20)(input_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=200,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Still not very good. Let's also make our model a bit more powerful, by adding more layers $l$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i,o=X_i$\n",
    "</p>\n",
    " \n",
    "<p style=\"text-align: center;\">  \n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(input_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=200,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data fits the sin curve perfectly where it had seen training data 0-10, and not so well where there was no training data. Neural networks are universal function approximators, you have little control of what they predict when given data that is completely new. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Review\n",
    "**Hyper Parmeter** Anything that goes into the model number layers, number of units..., or model fit learning rates, optimizers, etc.\n",
    "\n",
    "**batch size**: The number of examples seen when doing gradient decent \n",
    "\n",
    "**epoch**: The number of times the entire dataset has been used (selected in batch sized chunks)\n",
    "\n",
    "**learning rate**: Controls the distance of each gradient step\n",
    "\n",
    "**optimizer**: Algorithm that (using the learning rate) decides on how big a gradient step to take\n",
    "  * sgd\n",
    "  * adam\n",
    "  * rmsprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to reset the weights of the model below, so we can experiment with training\n",
    "def reset_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model): #if you're using a model as a layer\n",
    "            reset_weights(layer) #apply function recursively\n",
    "            continue\n",
    "\n",
    "        #where are the initializers?\n",
    "        if hasattr(layer, 'cell'):\n",
    "            init_container = layer.cell\n",
    "        else:\n",
    "            init_container = layer\n",
    "\n",
    "        for key, initializer in init_container.__dict__.items():\n",
    "            if \"initializer\" not in key: #is this item an initializer?\n",
    "                  continue #if no, skip it\n",
    "\n",
    "            # find the corresponding variable, like the kernel or the bias\n",
    "            if key == 'recurrent_initializer': #special case check\n",
    "                var = getattr(init_container, 'recurrent_kernel')\n",
    "            else:\n",
    "                var = getattr(init_container, key.replace(\"_initializer\", \"\"))\n",
    "\n",
    "            var.assign(initializer(var.shape, var.dtype))\n",
    "            #use the initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Experiment\n",
    "\n",
    "Try adjusting hyperparameters used for fitting, see how long it takes and how low the val_loss is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "optimizer=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "#optimizer=tf.keras.optimizers.RMSprop(lr=1e-3)\n",
    "#optimizer=tf.keras.optimizers.SGD(lr=1e-4)\n",
    "sine_model.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "\n",
    "reset_weights(sine_model)\n",
    "\n",
    "i_time=time.time()\n",
    "sine_model.fit(X,Y,epochs=10,validation_split=0.5,batch_size=10) #Have Keras make a test/validation split for us\n",
    "\n",
    "print(time.time()-i_time)\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did you see ?\n",
    "\n",
    "Does the code run faster or slower with a larger batch size?\n",
    "   * Is the loss better or worse\n",
    "Which optimizer gives the best results (SGD, Adam, RMSProp)?\n",
    "   * What is the effect of the learning rate on each optimizer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting is when a model memorizes it's training data. If your training data is noisy this is a problem because it tries to recreate meaningless bumps and wiggles. This hurts the preformance on new data! \n",
    "* Since overfitting hurts preformance on new data, we can use a witheld chunk of data to check for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "data_dim=1\n",
    "optimizer=tf.keras.optimizers.SGD(1e-4)\n",
    "optimizer=tf.keras.optimizers.Adam()#(1e-4)\n",
    "\n",
    "X=np.random.uniform(0,10,size=(100,data_dim))\n",
    "x_plot=np.random.uniform(0,10,size=(1000,data_dim))\n",
    "x_plot[:,0]=np.linspace(0,10,1000)\n",
    "\n",
    "def func(X,noise=True):\n",
    "    return np.sin(X[:,0])+np.random.normal(0,.3,size=(X[:,0].shape))*int(noise)  # #Ignore all other input have the output only depend on the first dimention\n",
    "\n",
    "\n",
    "Y=func(X)\n",
    "y_true=func(x_plot,noise=False)\n",
    "split=int(0.5*len(X))\n",
    "x_train=X[0:split]\n",
    "y_train=Y[0:split]\n",
    "\n",
    "\n",
    "x_test=X[split:]\n",
    "y_test=Y[split:]\n",
    "\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(40)(input_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(40)(activation_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(40)(activation_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(40)(activation_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "test_loss=[]\n",
    "train_loss=[]\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    history=sine_model.fit(x_train,y_train,epochs=int(50//(len(x_train)/batch_size)),batch_size=batch_size,verbose=0) #Have Keras make a test/validation split for us\n",
    "    y_pred=sine_model.predict(x_test)[:,0]\n",
    "    y_plot=sine_model.predict(x_plot)[:,0]\n",
    "    y_predt=sine_model.predict(x_train)[:,0]\n",
    "\n",
    "    test_loss.append(np.mean((y_pred-y_test)**2))\n",
    "    train_loss.append(np.mean((y_predt-y_train)**2))\n",
    "\n",
    "    if i %50==0:\n",
    "\n",
    "        plt.scatter(x_train[:,0],y_train,label='train pred')\n",
    "        plt.scatter(x_test[:,0],y_test,label='test pred')\n",
    "\n",
    "        plt.plot(x_plot[:,0],y_true,color='gray',label='truth')\n",
    "        plt.plot(x_plot[:,0],y_plot,label='func',color='red')\n",
    "   \n",
    "    \n",
    "        plt.ylim((-1.6,1.6))\n",
    "        plt.xlabel('X[:,0]')\n",
    "        plt.ylabel('Y')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,label='Train Loss')\n",
    "plt.plot(test_loss,label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the easiets ways to prevent overfitting is to train for fewer epochs (or fewer steps)\n",
    "### The more data you have the harder it is to overfit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Problem MPG problem\n",
    "\n",
    "Lets look at a real world dataset. This datasets has a number of features about vechicles. The question is based on this data can we predict a vechilce MPG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv('/gpfs/projects/bgmp/shared/2019_ML_workshop/datasets/mpg/auto-mpg.data', names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=raw_dataset['MPG']\n",
    "X=raw_dataset[['Cylinders','Displacement','Horsepower','Weight','Acceleration','Model Year','Origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y[0:10])\n",
    "print(X[0:10])\n",
    "# what "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer=tf.keras.optimizers.Adam()#(1e-4)\n",
    "\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(7,)) \n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)(input_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(64)(input_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(64)(activation_layer)\n",
    "activation_layer = tf.keras.layers.Activation('relu')(hidden_layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "MPG_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "MPG_model.summary()\n",
    "MPG_model.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "MPG_model.fit(X,Y,validation_split=0.5,epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nan :(\n",
    "A lot of real world data needs to be cleaned up before it can be used in a neural network\n",
    "* Missing data or data with Nans need to be removed\n",
    "* Input features need to be normalized by subtracting the mean, and dividing by the standard deviation\n",
    "* Categorical Variables need to be One-Hot encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization and Data Cleaning\n",
    "\n",
    "print(np.isnan(X).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Clean Nan Values\n",
    "\n",
    "cleaner_dataset=raw_dataset.dropna()\n",
    "cleaner_dataset['Origin_text'] = cleaner_dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "\n",
    "# Encode Orgin into a one-hot or dummy encoding\n",
    "cleaner_dataset=pd.get_dummies(cleaner_dataset, columns=['Origin_text'], prefix='', prefix_sep='')\n",
    "\n",
    "# Split dataset using Pandas\n",
    "train_dataset = cleaner_dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = cleaner_dataset.drop(train_dataset.index)\n",
    "\n",
    "Y=train_dataset['MPG']\n",
    "X=train_dataset[['Cylinders','Displacement','Horsepower','Weight','Acceleration','USA','Europe','Japan','Model Year']]\n",
    "\n",
    "Y_test=test_dataset['MPG']\n",
    "X_test=test_dataset[['Cylinders','Displacement','Horsepower','Weight','Acceleration','USA','Europe','Japan','Model Year']]\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "print(np.isnan(X).any())\n",
    "print(np.isnan(Y).any())\n",
    "\n",
    "\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(1e-2)\n",
    "\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(9,)) \n",
    "#New Keras Layer Does the Normalization for you!!!\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)(input_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(64)(normalizer)\n",
    "activation_layer = tf.keras.layers.Activation('elu')(hidden_layer)\n",
    "#activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(64)(activation_layer)\n",
    "activation_layer = tf.keras.layers.Activation('elu')(hidden_layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "MPG_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "MPG_model.summary()\n",
    "MPG_model.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "\n",
    "history=MPG_model.fit(X,Y,validation_split=0.2,epochs=200,verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some plots you should look at\n",
    "* History of training and validation loss - Is the model overfit?\n",
    "* Histogram of truth-prediction - are there outliers how good is the prediction?\n",
    "* Scatter Chart of Truth vs. Prediction - Is there any trend to your errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "Y_pred=MPG_model.predict(X_test)[:,0]\n",
    "print(Y_pred.shape,Y.shape)\n",
    "plt.show()\n",
    "\n",
    "prediction=plt.hist(Y_pred-Y_test)\n",
    "plt.show()\n",
    "print(np.mean(Y_pred-Y_test))\n",
    "print(np.std(Y_pred-Y_test))\n",
    "plt.scatter(Y_pred,Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
