{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Machine Learning in Python\n",
    "\n",
    "Learn how to get started training Neural Networks with keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Reminder\n",
    "* Find f(x) such that f(x) best approximates y\n",
    "* Examples:\n",
    "    * Given some pixels (x) tell me the probability itâ€™s a cat (y)\n",
    "    * Given news articles (x) tell me a stocks value (y)\n",
    "    * Given some sequences x find some low dimensional space (z) that represent my data \n",
    "      * f1(x)=z f2(z)=x  \n",
    "\n",
    "# Outline\n",
    "* Dense (Fully Connected Neural Networks)\n",
    "  * Example Linear Fits\n",
    "  * Classifications\n",
    "  \n",
    "Jake! Insert goals!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Packages\n",
    "\n",
    "We're going to be working primarily with Keras and Tensorflow. They're some alternatives like PyTorch, but they all allow you to build ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Fit a slightly harder straight line\n",
    "\n",
    "We're going to make a data set where x is a series of 5 features, and a target value $y = 2*x_0+1$ \n",
    "\n",
    "$y$ is just a line with respect to $x_0$, and completely ignores $x_{1,2,3,4}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=5\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1  # #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first Layer\n",
    "A Dense or fully connected layer\n",
    "\n",
    "<img src=\"../assets/dense.png\">\n",
    "\n",
    "A dense layer has a connection between every input variable and every output node. Each connection is represented by a weight $W_{i,n}$ from and input $X_n$ to an output $O_i$. The output is a sum over all the input variables times there weights plus a bias $B_i$\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "We will need to fit this to data, which means finding the best values for $W_{i,n}$ and $B_i$ to approximate our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we define and Input layer and one Dense (Fully Connected Layer), in our equation above\n",
    "i=1 n=data_dim\n",
    "if data_dim ==1\n",
    "then \n",
    "\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i  = O_0 =  W_{0,0}*X_0+B_0$\n",
    "\n",
    "You'll notice from last lecture this is the same form as our linear model.\n",
    "\n",
    "* $y_{pred,i}=\\theta_{1}*x_{i}+\\theta_{2} $\n",
    "\n",
    "* Each 'neuron' in a dense network is one linear model\n",
    "\n",
    "in neural network lingo \n",
    "*  $W$ is called the weight matrix \n",
    "*  $B$ the bias\n",
    "*  $W$ is a matrix and can have several parameters and all the parameters in the network are often represented by just $\\theta$ \n",
    "\n",
    "Just as in our Linear model we are going to use the same loss function\n",
    "* $L=\\frac{1}{N}\\sum_i (y_{pred,i}-y_{true,i})^2$\n",
    "* which is Mean Squared Error or mse for short\n",
    "* and we will pick an optimizer 'adam'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#MSE= Mean Squared Error \n",
    "linear_model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Fit Our Simple Neural Network\n",
    "# Stop fitting when the validation loss stops improving\n",
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto') #Jake, delete?\n",
    "#Fit\n",
    "linear_model.fit(X,Y,epochs=100,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lets plot the output as a function of X_0\n",
    "\n",
    "#Create some Random 5-d data\n",
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "#Set the first dimention to be a line\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "\n",
    "#Get the True distribution from our test function\n",
    "Y_test=func(X_test)\n",
    "\n",
    "#Get the prediction from our model\n",
    "Y_pred=linear_model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Lets Look at it wrt X[:,1]\n",
    "plt.scatter(X_test[:,1],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,1],Y_test,label='truth')\n",
    "plt.xlabel('X[:,1]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can look at a models weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weights=linear_model.get_weights()\n",
    "print(len(weights))\n",
    "print(weights[0].shape,weights[1].shape)\n",
    "\n",
    "print(weights[0])\n",
    "\n",
    "#Jake, add some English to help students distinguish? Difference between this cell and the next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We expect $W_{0,0}$=2, and $B_0$=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"W=\",weights[0])\n",
    "print(\"W[0,0]=\",weights[0][0,0])\n",
    "print(\"B=\",weights[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself \n",
    "Run the cell below to create a similar data set, but this time with some noise\n",
    "\n",
    "$y = 2*x_0+1+N(0,2)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=5\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1 + np.random.normal(0,2,size=(len(X))) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write your Model\"\"\"\n",
    "\"Input\"\n",
    "\"Dense Layer\"\n",
    "\"Create Model\"\n",
    "\"Fit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets try something a bit more complicated a sin wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=100,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Oops this didn't work. So far what we wrote above can only be linear\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "we need to add something called an activation function $\\sigma$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sigma(\\sum_n W_{i,n}*X_n+B_i)$    \n",
    "</p>\n",
    "\n",
    "$\\sigma$ has to be non-linear and a good choice is a LeakyReLU\n",
    "\n",
    "<img src='../assets/leakyReLU.png'>\n",
    "\n",
    "Let's also make our model a bit more powerful, by adding more layers $l$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i,o=X_i$\n",
    "</p>\n",
    " \n",
    "<p style=\"text-align: center;\">  \n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "###Lets Add another layer and an Activation###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "nn_model.compile(loss='mse',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "nn_model.fit(X,Y,epochs=50,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data fits the sin curve perfectly where it had seen training data 0-10, and not so well where there was no training data. Neural networks are universal function approximators, you have little control of what they predict when given data that is completely new. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Try and fix the above plot so the prediction and truth matches the whole range from -5 15\"\"\"\n",
    "\n",
    "X=np.random.uniform(-5,15,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "###Lets Add another layer and an Activation### Jake, do this in steps?\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "nn_model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "nn_model.fit(X,Y,epochs=50,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n",
    "\n",
    "X_test=np.random.uniform(-5,15,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification is problem where each of our examples (x) belongs to a class (y). Since Neural networks are universal function approximators, we can use\n",
    "\n",
    "$P(y|x)$\n",
    "\n",
    "How do we do that? We use a loss function that is minimized when f(x)= $P(y|x)$\n",
    "When we just have 2 class (A or B/True or False) we can use binary cross-entropy\n",
    "\n",
    "$L=-y_{true}*ln(y_{pred})-(1-y_{true})*ln(1-y_{pred})$\n",
    "\n",
    "\n",
    "* if you're curious\n",
    "\n",
    "This is essentially the negative log likelihood of a Bernoulli distribution\n",
    "\n",
    "$P(y,p)=p^{y}(1-p)^{1-y}$\n",
    "\n",
    "$-ln(P(y,p))=-ln(p^{y}(1-p)^{1-y})= -y*ln(p)-(1-y)*ln(1-p)$\n",
    "\n",
    "# What it Means for our Neural Network\n",
    "\n",
    "Be default a layer in keras has no (or linear) Activation which can take on any value from -inf - inf. In the above loss function we're trying to approximate a probability which must be between 0-1. To make this happen when have to add an activation on the output layer. We'll use a sigmoid.\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "\n",
    "Lets try one last toy problem.\n",
    "\n",
    "1. Class P(y=A|x) = N(1,1)\n",
    "2. Class P(y=B|x) = N(-1,1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class A\n",
    "n_data=10000\n",
    "\n",
    "y_train=np.ones(n_data) #Class A =1\n",
    "y_develop=np.ones(n_data)\n",
    "y_test=np.ones(n_data)\n",
    "\n",
    "x_train=np.random.normal(1,1,size=n_data)\n",
    "x_develop=np.random.normal(1,1,size=n_data)\n",
    "x_test=np.random.normal(1,1,size=n_data)\n",
    "\n",
    "\n",
    "#Class B\n",
    "y_train=np.append(y_train,np.zeros(n_data)) #Class B=0\n",
    "y_develop=np.append(y_develop,np.zeros(n_data)) \n",
    "y_test=np.append(y_test,np.zeros(n_data)) \n",
    "\n",
    "x_train=np.append(x_train,np.random.normal(-1,1,size=n_data))\n",
    "x_develop=np.append(x_develop,np.random.normal(-1,1,size=n_data))\n",
    "x_test=np.append(x_test,np.random.normal(-1,1,size=n_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple way to look at a feature is a histogram\n",
    "Plot how many times does class A have a feature value within a bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for class A\n",
    "a_mask=y_train==1\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-4,4),histtype='step',label='Class A')\n",
    "\n",
    "#Histogram for Class B\n",
    "b_mask=y_train==0\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-4,4),histtype='step',label='Class B')\n",
    "len(a_mask)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('N Examples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(1,)) \n",
    "###Same as Before###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "#Sigmoid!!!!\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "#Binary Cross Entropy!!!\n",
    "nn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(x_train,y_train,validation_data=(x_develop,y_develop),epochs=10,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test=np.linspace(-4,4,100)\n",
    "Y_true=1/(1+np.exp(-2*X_test))#Proof Left to Reader\n",
    "\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "plt.scatter(X_test,Y_pred,label='prediction')\n",
    "plt.scatter(X_test,Y_true,label='Truth')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('P(y=a|x)')\n",
    "plt.legend()\n",
    "plt.show() # Jake overlay histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense network summary\n",
    "\n",
    "* Dense networks take fixed length input and have a fixed length output\n",
    "* Like All Neural Network layers they require an activation function\n",
    "* They can be stacked to represent more complicated functions\n",
    "* You're taking your chances when predicting data that's very different from you're training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real World Example\n",
    "We're going to use a number of measured values to try and predict good wine.\n",
    "Source:https://archive.ics.uci.edu/ml/datasets/Wine+Quality (Cortez et al., 2009)\n",
    "\n",
    "* Goal: Predict whether a wine is excellent based score given to the wine by an expert panel (1-10)\n",
    "    * Score > 5 Excellent\n",
    "    * Score <=5 Average\n",
    "\n",
    "* Features:\n",
    "  * 1 - fixed acidity\n",
    "  * 2 - volatile acidity\n",
    "  * 3 - citric acid\n",
    "  * 4 - residual sugar\n",
    "  * 5 - chlorides\n",
    "  * 6 - free sulfur dioxide\n",
    "  * 7 - total sulfur dioxide\n",
    "  * 8 - density\n",
    "  * 9 - pH\n",
    "  * 10 - sulphates\n",
    "  * 11 - alcohol\n",
    "  * 12 - Red/White\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winedata\n",
    "import importlib\n",
    "importlib.reload(winedata)\n",
    "\n",
    "\n",
    "wd=winedata.WineData()\n",
    "print(wd.x_train.shape,wd.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.y_develop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(12,)) \n",
    "###Lets Add another layer and an Activation### Jake, get rid of dropout layers?\n",
    "nn = tf.keras.layers.Dense(50)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dropout(0.3)(nn)\n",
    "nn = tf.keras.layers.Dense(50)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dropout(0.3)(nn)\n",
    "nn = tf.keras.layers.Dense(50)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dropout(0.3)(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "wine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "wine_model.summary()\n",
    "wine_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_model.fit(wd.x_train,wd.y_train,epochs=50,validation_data=(wd.x_develop,wd.y_develop),callbacks=[es]) #Have Keras make a test/validation split for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "pred=wine_model.predict(wd.x_develop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape,wd.y_develop.shape) \n",
    "\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if i],bins=50,range=(0,1),density=True,histtype='step',label=\"Good\")\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if not i],bins=50,range=(0,1),density=True,histtype='step',label=\"Bad\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('NN Output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "for n in range(12):\n",
    "    good=[]\n",
    "    bad=[]\n",
    "    for i,v in enumerate(pred[:,0]):\n",
    "        if v >.8:\n",
    "            good.append(wd.x_develop[i,n])\n",
    "        else:\n",
    "            bad.append(wd.x_develop[i,n])\n",
    "    print(len(good),len(bad))\n",
    "    label=wd.header[n]\n",
    "    max_range=max([max(good),max(bad)])\n",
    "    min_range=min([min(good),min(bad)])\n",
    "    \n",
    "    plt.hist(good,density=True,histtype='step',label='good',range=(min_range,max_range),bins=20)\n",
    "    plt.hist(bad,density=True,histtype='step',label='bad',range=(min_range,max_range),bins=20)\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
