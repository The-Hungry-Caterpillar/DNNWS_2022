{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Reminder\n",
    "* Find f(x) such that f(x) best approximates y\n",
    "* Examples:\n",
    "    * Given some pixels (x) tell me the probability itâ€™s a cat (y)\n",
    "    * Given news articles (x) tell me a stocks value (y)\n",
    "    * Given some sequences x find some low dimensional space (z) that represent my data \n",
    "      * f1(x)=z f2(z)=x  \n",
    "\n",
    "# Outline\n",
    "* Dense (Fully Connected Neural Networks)\n",
    "  * Example Linear Fits\n",
    "  * Classifications\n",
    "  \n",
    "# Goals\n",
    "\n",
    "Dense Neural Netrworks are an essential building block used in Deep Learning image analysis our goals are:\n",
    "* Know what a Dense Neural Network is\n",
    "    * Know what an activation function is and what it does\n",
    "* Know how to write a Dense Neural Network\n",
    "* How do I train a Dense Neural Network\n",
    "    * Experiment with hyperparameters\n",
    "* How do I use a trained neural network\n",
    "    * Using a Neural Network to predict good wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Packages\n",
    "\n",
    "We're going to be working primarily with Keras and Tensorflow. They're some alternatives like PyTorch, but they all allow you to build ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first Layer\n",
    "A Dense or fully connected layer\n",
    "\n",
    "<img src=\"../assets/network_diagrams/dense.png\">\n",
    "\n",
    "A dense layer has a connection between every input variable and every output node. Each connection is represented by a weight $W_{i,n}$ from and input $X_n$ to an output $O_i$. The output is a sum over all the input variables times there weights plus a bias $B_i$\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "We will need to fit this to data, which means finding the best values for $W_{i,n}$ and $B_i$ to approximate our data.\n",
    "\n",
    "We will often also stack layers $l$, so the output of one layer feeds into the next\n",
    "\n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$   \n",
    "\n",
    "\n",
    "* We'll discuss this more in later in the lecture, but it's important when stacking layers we use an non-linear activation function $\\sigma$\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use a Dense Network\n",
    "\n",
    "* When you have fixed input size and a fixed output size \n",
    "* When your input size isn't too big\n",
    "    * We'll have to add something extra for image data\n",
    "    \n",
    "# Practice Building Networks\n",
    "\n",
    "## Keras organizes a network by layers\n",
    "\n",
    "Look at the code below, It has \n",
    "* One input layer with an input size = 3 \n",
    "* One output layer and an output size = 1\n",
    "* A layer is connect to a previous layer by passing the previous layer as an argument\n",
    "  * i.e   \n",
    "  output_layer=**tf.keras.layers.Dense**(*these arguments initialize the layer* )(**input_layer** *this argument connects the layers* ) #this call is to connect to input layer\n",
    "  \n",
    "## Networks are wrapped up into a Model\n",
    "\n",
    "A model tells Keras which inputs/outputs you want to use for example\n",
    "\n",
    "**linear_model=tf.keras.models.Model(input_layer,output_layer)**\n",
    "\n",
    "you'll need this model to fit to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above layer has 4 parameters (a weight for each connection, and one bias term)\n",
    "\n",
    "We can represent it like this\n",
    "<img src=\"../assets/network_diagrams/nn_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers can be stacked into more complex networks let's build this one\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_1.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "hidden_layer=tf.keras.layers.Dense(3)(input_layer) \n",
    "output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try building the following networks yourself\n",
    "\n",
    "Try writing code to make the following networks\n",
    "* 1. Create an Input Layer\n",
    "* 2. Write Dense layers with the right number of units\n",
    "* 3. Make Model name **my_model** using the input layer and your output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "my_model.layers[0]\n",
    "\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==tf.keras.models.Model, \"my model dosen't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==3, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 3\"\n",
    "assert my_model.layers[0].output_shape[1]==3, \"Input has isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==3, \"Hidden layer has isn't 3 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Output layer isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_3_3_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "my_model.layers[0]\n",
    "\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==tf.keras.models.Model, \"my model doesn't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==4, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 4\"\n",
    "assert my_model.layers[0].output_shape[1]==3, \"Input has isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==3, \"Hidden 1 layer has isn't 3 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Hidden 2 layer has isn't 3 dimensional\"\n",
    "assert my_model.layers[3].output_shape[1]==3, \"Output layer isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_5_3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your Code Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "assert 'my_model' in locals(), \"my_model doesn't exist did you get the name correct for your model\"\n",
    "print('Found my model')\n",
    "assert type(my_model)==tf.keras.models.Model, \"my_model doesn't see to be a keras model try tf.keras.models.Model\"\n",
    "assert len(my_model.layers)==3, \"Your model has \"+str(len(my_model.layers))+\" layers and should have 4\"\n",
    "assert my_model.layers[0].output_shape[1]==3, \"Input has isn't 3 dimensional\"\n",
    "assert my_model.layers[1].output_shape[1]==5, \"Hidden 1 layer has isn't 3 dimensional\"\n",
    "assert my_model.layers[2].output_shape[1]==3, \"Hidden 2 layer has isn't 3 dimensional\"\n",
    "print('Great Job Model is Correct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Design\n",
    "* Input dimension is defined by the input data\n",
    "* Output dimension is defined by target data\n",
    "* Hidden layers add complexity to the model\n",
    "    * More hidden layers or larger hidden layer dimensions can represent more complicated functions\n",
    "    * Too many layers can be hard to train without special tricks\n",
    "        * Can overfit, or fail to train (more on that later)\n",
    "    * Too few layers many not correctly describe the data\n",
    "* The right balance depends on the problem\n",
    "    * Roughly the more data the more layers you can use\n",
    "    * The more complex the target the more layer's you'll need\n",
    "* No right answer feel free to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting your Model\n",
    "Lets try to fit a simple line using the model below\n",
    "\n",
    "<img src=\"../assets/network_diagrams/nn_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "input_layer=tf.keras.layers.Input(shape=(3,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "linear_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we define and Input layer and one Dense (Fully Connected Layer), in our equation above\n",
    "i=1 n=data_dim\n",
    "if data_dim ==1\n",
    "then \n",
    "\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i  = O_0 =  W_{0,0}*X_0+B_0$\n",
    "\n",
    "You'll notice from last lecture this is the same form as our linear model.\n",
    "\n",
    "* $y_{pred,i}=\\theta_{1}*x_{i}+\\theta_{2} $\n",
    "\n",
    "* Each 'neuron' in a dense network is one linear model\n",
    "\n",
    "in neural network lingo \n",
    "*  $W$ is called the weight matrix \n",
    "*  $B$ the bias\n",
    "*  $W$ is a matrix and can have several parameters and all the parameters in the network are often represented by just $\\theta$ \n",
    "\n",
    "Just as in our Linear model we are going to use the same loss function\n",
    "* $L=\\frac{1}{N}\\sum_i (y_{pred,i}-y_{true,i})^2$\n",
    "* which is Mean Squared Error or mse for short\n",
    "* and we will pick an optimizer 'adam'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Fit a slightly harder straight line\n",
    "\n",
    "We're going to make a data set where x is a series of 3 features, and a target value $y = 2*x_0+1$ \n",
    "\n",
    "$y$ is just a line with respect to $x_0$, and completely ignores $x_{1,2,3,4}$ \n",
    "\n",
    "This problem is a little bit more difficult than the 1st lecture since we need to learn that two of the input features don't correlate at all to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=3\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1  # #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#MSE= Mean Squared Error \n",
    "linear_model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Fit Our Simple Neural Network\n",
    "#Fit \n",
    "linear_model.fit(X,Y,epochs=20,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "#Pro tip, don't want to split the dataset yourself, you can have keras do it for you with validation_split=\n",
    "\n",
    "# Even more Pro-tip - be careful if you run this cell more than once you'll keep training the same model \n",
    "# with a different train/develop split each time which can cause the model to overfit both the train and develop\n",
    "# sets - a problem you'll only see when using the test set, so make sure to keep a test set around!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent you've fit your first neural network, now let use it\n",
    "* we split our initial dataset into a train/develop using validation split\n",
    "* Lets make a new dataset that has X_0 from -5-15, with X_1,X_2 being random\n",
    "    * This is just a way to plot the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lets plot the output as a function of X_0\n",
    "\n",
    "#Create some Random 5-d data\n",
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "#Set the first dimention to be a line\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "\n",
    "#Get the True distribution from our test function\n",
    "Y_test=func(X_test)\n",
    "\n",
    "#Get the prediction from our model\n",
    "Y_pred=linear_model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth',marker='+')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Lets Look at it wrt X[:,1]\n",
    "plt.scatter(X_test[:,1],Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test[:,1],Y_test,label='truth',marker='+')\n",
    "plt.xlabel('X[:,1]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also look at a models weights\n",
    "We expect $W_{0,0}$=2, and $B_0$=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weights=linear_model.get_weights()\n",
    "print(\"W=\",weights[0])\n",
    "print(\"W[0,0]=\",weights[0][0,0])\n",
    "print(\"B=\",weights[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself \n",
    "Run the cell below to create a similar data set, but this time with some noise\n",
    "\n",
    "$y = 2*x_0+1+N(0,2)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Dataset\n",
    "\n",
    "data_dim=5\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1 + np.random.normal(0,2,size=(len(X))) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write your Model\"\"\"\n",
    "\"Input\"\n",
    "\"Dense Layer\"\n",
    "\"Create Model\"\n",
    "\"Fit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets try something a bit more complicated a sin wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=15,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Oops this didn't work. Why? So far what we wrote above can only represent linear functions\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "we need to add something called an activation function $\\sigma$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sigma(\\sum_n W_{i,n}*X_n+B_i)$    \n",
    "</p>\n",
    "\n",
    "$\\sigma$ has to be non-linear and a good choice is a LeakyReLU\n",
    "\n",
    "<img src='../assets/leakyReLU.png'>\n",
    "\n",
    "an activation can be added just like any other layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(input_layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=20,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Still not very good, let's add an activated hidden layer to get more flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "hidden_layer = tf.keras.layers.Dense(20)(input_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=20,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Still not very good. Let's also make our model a bit more powerful, by adding more layers $l$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i,o=X_i$\n",
    "</p>\n",
    " \n",
    "<p style=\"text-align: center;\">  \n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(input_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(20)(activation_layer)\n",
    "activation_layer = tf.keras.layers.LeakyReLU()(hidden_layer)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(activation_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "sine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "sine_model.compile(loss='mse',optimizer='adam')\n",
    "sine_model.fit(X,Y,epochs=20,validation_split=0.5) #Have Keras make a test/validation split for us\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data fits the sin curve perfectly where it had seen training data 0-10, and not so well where there was no training data. Neural networks are universal function approximators, you have little control of what they predict when given data that is completely new. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Review\n",
    "**Hyper Parmeter** Anything that goes into the model number layers, number of units..., or model fit learning rates, optimizers, etc.\n",
    "\n",
    "**batch size**: The number of examples seen when doing gradient decent \n",
    "\n",
    "**epoch**: The number of times the entire dataset has been used (selected in batch sized chunks)\n",
    "\n",
    "**learning rate**: Controls the distance of each gradient step\n",
    "\n",
    "**optimizer**: Algorithm that (using the learning rate) decides on how big a gradient step to take\n",
    "  * sgd\n",
    "  * adam\n",
    "  * rmsprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to reset the weights of the model below, so we can experiment with training\n",
    "def reset_weights(model):\n",
    "    session = tf.keras.backend.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Experiment\n",
    "\n",
    "Try adjusting hyperparameters used for fitting, see how long it takes and how low the val_loss is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "optimizer=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "#optimizer=tf.keras.optimizers.RMSprop(lr=1e-3)\n",
    "#optimizer=tf.keras.optimizers.SGD(lr=1e-4)\n",
    "sine_model.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "\n",
    "reset_weights(nn_model)\n",
    "\n",
    "i_time=time.time()\n",
    "sine_model.fit(X,Y,epochs=10,validation_split=0.5,batch_size=1000) #Have Keras make a test/validation split for us\n",
    "print(time.time()-i_time)\n",
    "\n",
    "Y_pred=sine_model.predict(X_test)\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did you see ?\n",
    "\n",
    "Does the code run faster or slower with a larger batch size?\n",
    "   * Is the loss better or worse\n",
    "Which optimizer gives the best results (SGD, Adam, RMSProp)?\n",
    "   * What is the effect of the learning rate on each optimizer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification is problem where each of our examples (x) belongs to a class (y). Since Neural networks are universal function approximators, we can use\n",
    "\n",
    "$P(y|x)$\n",
    "\n",
    "\n",
    "\n",
    "## Binary Classification\n",
    "When we just have 2 class (A or B/True or False) then we only need to predict the probability of one since\n",
    "\n",
    "$P(y=b|x)$=1-$P(y=a|x)$\n",
    "\n",
    "Our networks inputs are our features, and our networks output is $P(y=a|x)$.\n",
    "\n",
    "**Important:** We need to use the correct activation function and the correct loss function. We know $y_pred$\n",
    "is between zero and one, so we can add a 'sigmoid' activation\n",
    "\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "<img src='../assets/sigmoid.png'>\n",
    "\n",
    "For the loss function we use something called binary cross-entropy, which uses our labels (0 or 1) to train our network.\n",
    "\n",
    "$L=\\frac{1}{N}*\\sum_i(-y_{true,i}*ln(y_{pred,i})-(1-y_{true,i})*ln(1-y_{pred,i}))$\n",
    "\n",
    "Since our labels are only 0 or 1 this loss will \n",
    "* Make $-1*ln(y_{pred})$ as small as possible ($y_{pred}=1$) when $y_{true}=1$\n",
    "* Make $-1*ln(1-y_{pred})$ as small as possible ($y_{pred}=0$) when $y_{true}=0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## If you're curious\n",
    "\n",
    "This is essentially the negative log likelihood of a Bernoulli distribution\n",
    "\n",
    "$P(y,p)=p^{y}(1-p)^{1-y}$\n",
    "\n",
    "$-ln(P(y,p))=-ln(p^{y}(1-p)^{1-y})= -y*ln(p)-(1-y)*ln(1-p)$\n",
    "\n",
    "# What it Means for our Neural Network\n",
    "\n",
    "* When we build our model we need to use a sigmoid activation on the last layer\n",
    "\n",
    "    output=tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    \n",
    "    **or**\n",
    "    \n",
    "    dense=tf.keras.layers.Dense(1)\n",
    "    \n",
    "    output=tf.keras.layers.Activation('sigmoid')(dense)\n",
    "    \n",
    "    \n",
    "* When we compile a model we need to use \"binary_crossentropy\" as a loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5b3d7f7c1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Class A =1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_develop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Class A\n",
    "n_data=10000\n",
    "\n",
    "y_train=np.ones(n_data) #Class A =1\n",
    "y_develop=np.ones(n_data)\n",
    "y_test=np.ones(n_data)\n",
    "\n",
    "x_train=np.random.normal(1,1,size=n_data)\n",
    "x_develop=np.random.normal(1,1,size=n_data)\n",
    "x_test=np.random.normal(1,1,size=n_data)\n",
    "\n",
    "\n",
    "#Class B\n",
    "y_train=np.append(y_train,np.zeros(n_data)) #Class B=0\n",
    "y_develop=np.append(y_develop,np.zeros(n_data)) \n",
    "y_test=np.append(y_test,np.zeros(n_data)) \n",
    "\n",
    "x_train=np.append(x_train,np.random.normal(-1,1,size=n_data))\n",
    "x_develop=np.append(x_develop,np.random.normal(-1,1,size=n_data))\n",
    "x_test=np.append(x_test,np.random.normal(-1,1,size=n_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple way to look at a feature is a histogram\n",
    "Plot how many times does class A have a feature value within a bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for class A\n",
    "a_mask=y_train==1\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-4,4),histtype='step',label='Class A')\n",
    "\n",
    "#Histogram for Class B\n",
    "b_mask=y_train==0\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-4,4),histtype='step',label='Class B')\n",
    "len(a_mask)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('N Examples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(1,)) \n",
    "###Same as Before###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "#Sigmoid!!!!\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "\n",
    "nn_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "nn_model.summary()\n",
    "#Binary Cross Entropy!!!\n",
    "nn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(x_train,y_train,validation_data=(x_develop,y_develop),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test=np.linspace(-4,4,100)\n",
    "Y_true=1/(1+np.exp(-2*X_test))#Proof Left to Reader\n",
    "\n",
    "Y_pred=nn_model.predict(X_test)\n",
    "plt.scatter(X_test,Y_pred,label='prediction',marker='x')\n",
    "plt.scatter(X_test,Y_true,label='Truth',marker='+')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('P(y=a|x)')\n",
    "\n",
    "plt.hist(x_train[b_mask],bins=16,range=(-4,4),histtype='step',label='Class B',density=True)\n",
    "plt.hist(x_train[a_mask],bins=16,range=(-4,4),histtype='step',label='Class A',density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense network summary\n",
    "\n",
    "* Dense networks take fixed length input and have a fixed length output\n",
    "* Like All Neural Network layers they require an activation function\n",
    "    * The activation of the last layer is usally determined by the goal\n",
    "        * Regression - Linear\n",
    "        * Classification - Sigmoid\n",
    "        * Multi-Class Classification - Softmax (more later)\n",
    "* They can be stacked to represent more complicated functions\n",
    "* You're taking your chances when predicting data that's very different from you're training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real World Example\n",
    "We're going to use a number of measured values to try and predict good wine.\n",
    "Source:https://archive.ics.uci.edu/ml/datasets/Wine+Quality (Cortez et al., 2009)\n",
    "\n",
    "* Goal: Predict whether a wine is excellent based score given to the wine by an expert panel (1-10)\n",
    "    * Score > 5 Excellent\n",
    "    * Score <=5 Average\n",
    "\n",
    "* Features:\n",
    "  * 1 - fixed acidity\n",
    "  * 2 - volatile acidity\n",
    "  * 3 - citric acid\n",
    "  * 4 - residual sugar\n",
    "  * 5 - chlorides\n",
    "  * 6 - free sulfur dioxide\n",
    "  * 7 - total sulfur dioxide\n",
    "  * 8 - density\n",
    "  * 9 - pH\n",
    "  * 10 - sulphates\n",
    "  * 11 - alcohol\n",
    "  * 12 - Red/White\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winedata\n",
    "import importlib\n",
    "importlib.reload(winedata)\n",
    "\n",
    "\n",
    "wd=winedata.WineData()\n",
    "print(wd.x_train.shape,wd.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.y_develop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make histograms for each of our 12 input features, and check to see how many wines are labeled Excellent vs. Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n in range(12):\n",
    "    good=[]\n",
    "    bad=[]\n",
    "    for vals,label in zip(wd.x_train,wd.y_train):\n",
    "        if label:\n",
    "            good.append(vals[n])\n",
    "        else:\n",
    "            bad.append(vals[n])\n",
    "\n",
    "    label=wd.header[n]\n",
    "    \n",
    "    \n",
    "    max_range=max([max(good),max(bad)])\n",
    "    min_range=min([min(good),min(bad)])\n",
    "    \n",
    "    plt.hist(good,density=True,histtype='step',label='Excellent',range=(min_range,max_range),bins=20)\n",
    "    plt.hist(bad,density=True,histtype='step',label='Average',range=(min_range,max_range),bins=20)\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print(\"Excellent Data Points\",len(good))\n",
    "print(\"Average Data Points\",len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(12,)) \n",
    "###Lets Add another layer and an Activation\n",
    "nn = tf.keras.layers.Dense(50)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dense(50)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "nn = tf.keras.layers.Dense(50)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "wine_model=tf.keras.models.Model(input_layer,output_layer)\n",
    "wine_model.summary()\n",
    "wine_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_model.fit(wd.x_train,wd.y_train,epochs=50,validation_data=(wd.x_develop,wd.y_develop)) #Have Keras make a test/validation split for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape,wd.y_develop.shape) \n",
    "\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if i],bins=50,range=(0,1),density=True,histtype='step',label=\"Excellent\")\n",
    "plt.hist([p for i,p in zip(wd.y_develop,np.squeeze(pred)) if not i],bins=50,range=(0,1),density=True,histtype='step',label=\"Average\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('NN Output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "pred=wine_model.predict(wd.x_develop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "for n in range(12):\n",
    "    good=[]\n",
    "    bad=[]\n",
    "    for i,v in enumerate(pred[:,0]):\n",
    "        if v >.8:\n",
    "            good.append(wd.x_develop[i,n])\n",
    "        else:\n",
    "            bad.append(wd.x_develop[i,n])\n",
    "    print(len(good),len(bad))\n",
    "    label=wd.header[n]\n",
    "    max_range=max([max(good),max(bad)])\n",
    "    min_range=min([min(good),min(bad)])\n",
    "    \n",
    "    plt.hist(good,density=True,histtype='step',label='Excellent',range=(min_range,max_range),bins=20)\n",
    "    plt.hist(bad,density=True,histtype='step',label='Average',range=(min_range,max_range),bins=20)\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we happy with the model's preformance on the develop set? \n",
    "* Yes let's confirm with the test set\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the evaluate function to caculate the loss and accuracy of different datasets\n",
    "develop_loss,develop_accuracy=wine_model.evaluate(wd.x_develop,wd.y_develop)\n",
    "\n",
    "test_loss,test_accuracy=wine_model.evaluate(wd.x_test,wd.y_test)\n",
    "\n",
    "print(\"Develop loss\",develop_loss, \" Develop Accuracy\", develop_accuracy)\n",
    "print(\"Test loss\",test_loss, \" Test Accuracy\", test_accuracy)\n",
    "print(\"Test Loss Differs from develop loss by \",round((test_loss-develop_loss)/(test_loss)*100,2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test loss should only slightly differ from the develop loss, if that's true you have a well trained network ready to evaluate new wine.\n",
    "\n",
    "\n",
    "# Try it yourself can you train a better neural network?\n",
    "* Start with the network above\n",
    "* Try adding layers, or hidden units\n",
    "* Watch the val_loss when training make sure it doesn't start going up \n",
    "    * if it does stop the training \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
