{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camelyonpatch import CamelyOnPatch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image in full Resolution is too large to fit in memory, but it doesn't mean we can't still use what we've learned to start analyzing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder our Dataset\n",
    "\n",
    "Instead of looking at the whole image we can look at one small piece say 96x96 pixels worth. \n",
    "* Ask are any of the pixels in the middle 32x32 cancerous\n",
    "    * If yes label it with a 1\n",
    "    * If no label it with a zero\n",
    "    * Why only the middle 32x32 pixels\n",
    "        * Allows network to have some context to understanding the center pixels\n",
    "        * Otherwise one pixel in the top corner being cancerous would label the whole image cancerous\n",
    "* This turns one picture into 1000s of trainning examples\n",
    "\n",
    "* **Important Detail** about the data split\n",
    "    * Having a lot of training examples is great\n",
    "        * But what we really want to know is can we use this algorithm to identify tumors it hasn't seen before\n",
    "        * The dataset used above splits the data by randomly assigning each tumor to a dataset rather than randomly assigning each image to a dataset\n",
    "            * Make sure the model gernalizes to new tumors and not just those seen in the training set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Our Image Dataset\n",
    "\n",
    "We've placed a lot of small images in folders, how do we feed this to keras?\n",
    "   * We'll use a data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Generator\n",
    "\n",
    "python has a neat concept called a generator that we can use in our ML models\n",
    "* A function that generates data each time it is called\n",
    "* Often a while loop the loops forever \n",
    "* uses the **yield** keyword\n",
    "* Each new element can be grabbed with the next keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_random_numbers():\n",
    "    while True:\n",
    "        yield( np.random.uniform(size=5))\n",
    "my_generator=five_random_numbers()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(my_generator))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give it a try\n",
    "We'll use a generator to scan our input slide, try writing one that returns batch_sizex96x96x3 random images\n",
    "\n",
    "`hint np.random.normal(size=(batch_size,96,96,3))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def my_generator(\"Your code from here\"\"\"                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Image Data Generator\n",
    "\n",
    "This is a built in generator like the one we used to scan our full slide, but it has the ability to change images on the fly, so no image you use in training is exactly the same.\n",
    "\n",
    "```\n",
    "keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False, \n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False, \n",
    "    zca_epsilon=1e-06, \n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0, \n",
    "    height_shift_range=0.0, \n",
    "    brightness_range=None, \n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0, \n",
    "    channel_shift_range=0.0, \n",
    "    fill_mode='nearest', \n",
    "    cval=0.0, \n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None, \n",
    "    preprocessing_function=None, \n",
    "    data_format=None, \n",
    "    validation_split=0.0, \n",
    "    dtype=None)\n",
    "```\n",
    "\n",
    "## Input\n",
    "\n",
    "There are several ways of using this, but we're going to use raw (96x96x3) images stored in folders.\n",
    "Classes are identifided in sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True,\n",
    "            shear_range=1,\n",
    "            zoom_range=.05,\n",
    "            rotation_range=15                           \n",
    "            )  # randomly flip images\n",
    "train_generator=data_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/train',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=True)\n",
    "\n",
    "develop_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False,\n",
    "            shear_range=0,\n",
    "            zoom_range=.00,\n",
    "            rotation_range=0                          \n",
    "            )  # randomly flip images\n",
    "\n",
    "develop_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/develop',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "\n",
    "test_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/test',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_image,cpd_labels=next(train_generator)\n",
    "\n",
    "for image in cpd_image:\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is just like the examples from before, but with slightly larger images\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try writing your own model, and training it \n",
    "* Start with an input layer\n",
    "* Add Convolutional layers\n",
    "    * Remember to add an activation\n",
    "* Downsample with either pooling or striding\n",
    "* Make a keras model, call it **model**\n",
    "    * compile the model including accuracy as a metric \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Create your model here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Add model training code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer these Questions\n",
    "\n",
    "* How many parameters were in your model?\n",
    "* How long did an epoch take?\n",
    "* What accuracy were you able to reach?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Simple CNN\n",
    "\n",
    "* If you want to use your own model written above, go for it! (just skip the cell below)\n",
    "* Otherwise use the network below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=input(\"Are you sure you want to use my model? Y/N\")\n",
    "if ans.lower() == 'y':\n",
    "    cnn_input=tf.keras.layers.Input( shape=(96,96,3) ) # Shape here does not including the batch size \n",
    "    cnn_layer1=tf.keras.layers.Convolution2D(64, (2,2),strides=2,padding='same')(cnn_input) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "    cnn_layer2=tf.keras.layers.Convolution2D(126, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "    cnn_layer3=tf.keras.layers.Convolution2D(256, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "\n",
    "    flat=tf.keras.layers.Flatten()(cnn_activation) \n",
    "\n",
    "    drop=tf.keras.layers.Dropout(0.3)(flat) \n",
    "\n",
    "    dense_layer=tf.keras.layers.Dense(1)(drop) \n",
    "    output=tf.keras.layers.Activation('sigmoid')(dense_layer)\n",
    "\n",
    "    model=tf.keras.models.Model([cnn_input],[output])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_1.h5')\n",
    "\n",
    "    if False:\n",
    "        history=model.fit(cpd.X_train, cpd.Y_train, \n",
    "              batch_size=100, epochs=10, verbose=1,\n",
    "             validation_data=(cpd.X_develop,cpd.Y_develop),\n",
    "                      shuffle='batch'\n",
    "             )\n",
    "        model.save_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_1.h5')\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot of our predictions\n",
    "\n",
    "Let's check to see how well our model is working\n",
    " * We'll look at the same histogram we did in previous lectures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.squeeze(model.predict_generator(develop_generator))\n",
    "y_develop=develop_generator.classes[0:len(y_pred)]\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "develop_generator.classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(10,10))\n",
    "tumor=[pred for pred,truth in zip(y_pred,y_develop) if truth  ]\n",
    "fine=[pred for pred,truth in zip(y_pred,y_develop) if not truth  ]\n",
    "\n",
    "plt.hist(tumor,range=(0,1),bins=200,density=True,histtype='step',label='Tumor')  \n",
    "plt.hist(fine,range=(0,1),bins=200,density=True,histtype='step',label='Normal')\n",
    "plt.xlabel('Neural Network Response')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopefully there is a clear separation between tumor and normal\n",
    "\n",
    "## Is it good enough?\n",
    "\n",
    "In order to answer this question it's important to decide how we want to use it\n",
    "\n",
    "**Goal**: Assist in diagnosis by highlighting potential tumors.\n",
    "\n",
    "Let's see if it's good enough by giving it a try\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use open-slide to grab 96x96x3 pixel chunks and feed them to our model\n",
    "\n",
    "Our training Data was split into 96x96 pixel images at level-2\n",
    " * Let's check to make sure that's not crazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "tiff_file='/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif'\n",
    "slide_image=openslide.OpenSlide(tiff_file)\n",
    "# This is the resolution at level 2\n",
    "initial_dimension=slide_image.level_dimensions[2]\n",
    "\n",
    "# This is the resolution after we've turned each 32x32 pixel box into a predictions\n",
    "final_dimension= np.array(initial_dimension)/32.\n",
    "\n",
    "print(\"prescan dimension\",initial_dimension,\"final scan\",final_dimension)\n",
    "n_predictions=np.product(final_dimension)\n",
    "print(n_predictions/1e6,\" Million Predictions Required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a lot of predictions\n",
    "\n",
    "It's possible to do the whole slide, but it takes awhile (30 minutes or so...) let's try a smaller section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#These coordinates are at level 0\n",
    "width=6400\n",
    "height=6400\n",
    "\n",
    "x_start=60000\n",
    "y_start=120000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Predictions Required\",width//32*height//32)\n",
    "\n",
    "\n",
    "#Covert these for later use to level2\n",
    "level0_dimension=slide_image.level_dimensions[0]\n",
    "level2_dimension=slide_image.level_dimensions[2]\n",
    "\n",
    "sfactor_x=level0_dimension[0]/level2_dimension[0]\n",
    "sfactor_y=level0_dimension[1]/level2_dimension[1]\n",
    "\n",
    "x_stop=x_start+width*4\n",
    "y_stop=y_start+height*4\n",
    "\n",
    "\n",
    "print('Scaling factor to level 2',sfactor_x,sfactor_y)\n",
    "\n",
    "# It's just a factor of 4\n",
    "f=plt.figure(figsize=(50,50))\n",
    "plt.imshow(slide_image.read_region( (x_start,y_start),2,(height,width)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad - let's scan this image\n",
    "How can we feed this to our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Generator to load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_image(image_file,batch_size,x_range,y_range):\n",
    "    slide_image=openslide.OpenSlide(image_file)\n",
    "    res_x,res_y=slide_image.level_dimensions[2]\n",
    "    \n",
    "    coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "    #This is factor we need to scale the pixels at resolution 2 to the coordinates in resolution 0\n",
    "\n",
    "    sfactor_x=coord_x/res_x  \n",
    "    sfactor_y=coord_y/res_y \n",
    "    \n",
    "    batch=[]\n",
    "    index=0\n",
    "    for x in range(x_range[0]//4,x_range[1]//4,32):\n",
    "        for y in range(y_range[0]//4,y_range[1]//4,32):\n",
    "            image=np.asarray(slide_image.read_region( (int(x*sfactor_x),int(y*sfactor_y)),2,(96,96)  ))/255.\n",
    "            batch.append(np.expand_dims(image[:,:,0:3],0))                                \n",
    "            if len(batch)==batch_size:\n",
    "                yield(np.concatenate(batch,0))\n",
    "                batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "n_predictions=width//32*height//32\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "generator=scan_image('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif',batch_size,x_range,y_range)\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "output_scan=model.predict_generator(generator,steps)\n",
    "output_map=np.zeros((height//32,width//32))\n",
    "print(output_scan)\n",
    "for index,v in enumerate(output_scan):\n",
    "    \n",
    "    y=index%(width//32)\n",
    "    x=index//(width//32)\n",
    "    output_map[y,x]=v\n",
    "    \n",
    "plt.imshow(output_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are probabilites\n",
    "Let's check which are greater than 50%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(10,10))\n",
    "plt.imshow(output_map > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Manipulating Data\n",
    "\n",
    "1. Use Open Slide to find the coordinates of a different region of cells\n",
    "2. Use the coordinates, The generator code above, and our model to predict a new region\n",
    "3. What diagnosis would you make (or would you?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hmmm\n",
    "The full slide looks like this (as you can guess from the file name it has a tumor)\n",
    "<img src='../assets/full_slide_scan_1.png'>\n",
    "\n",
    "But wait a slide without a tumor looks like this\n",
    "\n",
    "<img src='../assets/full_slide_scan_normal.png'>\n",
    "\n",
    "\n",
    "The results above don't look great, lots of likely Tumors even in places without cells!\n",
    "\n",
    "* Our training data didn't include things like slide edges and slide area's without tissues\n",
    "    * The results on these can be fairly random\n",
    "* A bigger issue is a problem with statistics\n",
    "\n",
    "* Remember we are approximating    \n",
    "    $P(y|x)$\n",
    "\n",
    "* Which using Bayes’ theorem\n",
    "\n",
    "    $P(y|x)=\\frac{P(x|y)P(y)}{P(x)}$\n",
    "\n",
    "* What is P(y) in our dataset?\n",
    "    * This the probability an image contained at least one tumor pixel\n",
    "    * Our dataset was artifically built so $P(y=tumor)=1/2$ and $P(y=healthy)=1/2$\n",
    "        * This makes training easier\n",
    "        * This is called class Balancing\n",
    "    * However, looking at cells in the wild it is vastly more likely that they are non-cancerous\n",
    "* How do we use these predictions?\n",
    "    * We know $P(y)$ is quite a bit smaller in the real world\n",
    "    * We $P(y|x)$ is porportional to $P(y)$\n",
    "        * The real world $P(y|x)$ is smaller than the one measured in our experiment\n",
    "    * An easy way to reduce false positives is to increase the threshold we used from 50% to something larger\n",
    "        * How high is a judgment cell\n",
    "        * Too high could miss tumors\n",
    "        * Too low and there are fake tumors everywhere\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for threshhold in [0.5,0.7,0.8,0.9,0.95,0.99]:    \n",
    "    print('Threshhold',threshhold)\n",
    "    f=plt.figure(figsize=(5,12))\n",
    "    plt.imshow(output_map > threshhold)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
