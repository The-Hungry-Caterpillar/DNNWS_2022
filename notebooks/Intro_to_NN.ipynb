{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Machine Learning in Python\n",
    "\n",
    "Learn how to get started training Neural Networks with keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Warning \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Mauritius_Road_Signs_-_Warning_Sign_-_Other_dangers.svg/556px-Mauritius_Road_Signs_-_Warning_Sign_-_Other_dangers.svg.png\" style=\"width:50px\">\n",
    "\n",
    "It's actually pretty easy to get started training Machine learning algorithms, but be aware there are plenty of examples of well trained, well coded, and well intentioned ML algorithms that do harmful things.\n",
    "\n",
    "\n",
    "<a href=\"https://www.technologyreview.com/s/613274/facebook-algorithm-discriminates-ai-bias\"> Facebook’s ad-serving algorithm discriminates by gender and race\n",
    " </a>\n",
    "    \n",
    "<a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\"> Amazon scraps secret AI recruiting tool that showed bias against women\n",
    " </a>\n",
    "\n",
    "<a href=\"https://www.thedailybeast.com/why-doctors-arent-afraid-of-better-more-efficient-ai-diagnosing-cancer\"> Ruler in picture an indicator for Cancer </a>\n",
    "\n",
    "<a href=\"https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\"> Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than a day </a>\n",
    "\n",
    "Be aware and careful before you deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://s3.ap-south-1.amazonaws.com/techleer/207.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vocab\n",
    "## Artificial Intelligence\n",
    "An all encompassing term for a broad field the most promising of which is currently machine learning\n",
    "    \n",
    "## Machine Learning\n",
    "* Deep Learning - Deep Neural Networks of all forms\n",
    "* ‘Traditional’ Machine Learning  - Pretty much everything else\n",
    "    Trees, SVMs, Linear Regression, Naive Bayes...\n",
    "* **X’s = Input variables**\n",
    "* **Y’s = Target Variables**\n",
    "* Loss function - Numerical Goal of the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "* Find f(x) such that f(x) best approximates y\n",
    "* Examples:\n",
    "    * Given some pixels (x) tell me the probability it’s a cat (y)\n",
    "    * Given news articles (x) tell me a stocks value (y)\n",
    "    * Given some sequences x find some low dimensional space (z) that represent my data \n",
    "      * f1(x)=z f2(z)=x  \n",
    "* **Important Note: No prediction of causality** \n",
    "* Function outputs and targets can be stochastic \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "* Dense (Fully Connected Neural Networks)\n",
    "  * Example Fits\n",
    "* Sequences and Recurrent Neural Networks\n",
    "  * Learning to Count\n",
    "* Sentiment Analysis on movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine learning algorithms all start form a series data examples\n",
    "\n",
    "\n",
    "# Input Data\n",
    "* **numpy arrays** \n",
    "* pandas dataframes\n",
    "* hdf5, etc\n",
    "* **shape = (examples x data dimentions)** \n",
    "    * RGB Image Dataset (Number of images x Height x Width x 3) *(3=RGB)\n",
    "    * Text (Text blocks x ? ) examples with varying length can have an unspecified dimension size\n",
    "        * When training each batch needs to be the same length\n",
    "* Divide into 2 or 3 splits\n",
    "  * 2 Training/Testing (one for training and one for checking for over-fitting)\n",
    "  * 3 Training/Development/Testing \n",
    "      * One for training, one for checking for over-fitting (Development) )\n",
    "      * One for testing performance, but not for making any modeling decisions\n",
    "          * i.e. in this case testing is the data you want the model to actually work with  \n",
    "          \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets start with a simple prediction a straight Line\n",
    "data_dim=5\n",
    "\n",
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return 2*X[:,0]+1 #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first Layer\n",
    "A Dense or fully connected layer\n",
    "\n",
    "<img src=\"../assets/dense.png\">\n",
    "\n",
    "A dense layer has a connection between every input variable and every output node. Each connection is represented by a weight $W_{i,n}$ from and input $X_n$ to an output $O_i$. The output is a sum over all the input variables times there weights plus a bias $B_i$\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "We will need to fit this to data, which means finding the best values for $W_{i,n}$ and $B_i$ to approximate our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jsearcy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jsearcy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "WARNING:tensorflow:From /home/jsearcy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 0s 48us/sample - loss: 125.3614 - val_loss: 87.2678\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 69.2855 - val_loss: 50.1583\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 42.6983 - val_loss: 34.1084\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 31.2755 - val_loss: 27.4743\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 26.2939 - val_loss: 24.1254\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 23.2431 - val_loss: 21.5440\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 20.6449 - val_loss: 19.1112\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 18.1927 - val_loss: 16.7545\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 15.8593 - val_loss: 14.5015\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 13.6578 - val_loss: 12.4170\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 11.6181 - val_loss: 10.4983\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 9.7651 - val_loss: 8.7627\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 8.1025 - val_loss: 7.2209\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 6.6369 - val_loss: 5.8761\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 5.3632 - val_loss: 4.7147\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 4.2681 - val_loss: 3.7213\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 3.3439 - val_loss: 2.8909\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 2.5805 - val_loss: 2.2088\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.9571 - val_loss: 1.6601\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.4573 - val_loss: 1.2233\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.0647 - val_loss: 0.8839\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.7624 - val_loss: 0.6261\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.5343 - val_loss: 0.4335\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.3657 - val_loss: 0.2934\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.2445 - val_loss: 0.1934\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.1600 - val_loss: 0.1253\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.1025 - val_loss: 0.0793\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.0647 - val_loss: 0.0497\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.0404 - val_loss: 0.0309\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0255 - val_loss: 0.0197\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.0166 - val_loss: 0.0131\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 0s 27us/sample - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0086 - val_loss: 0.0074\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0061 - val_loss: 0.0056\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 9.7345e-04 - val_loss: 8.4356e-04\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 8.0167e-04 - val_loss: 6.9017e-04\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 6.5017e-04 - val_loss: 5.5523e-04\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 5.1792e-04 - val_loss: 4.3666e-04\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 4.0512e-04 - val_loss: 3.3857e-04\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 3.1009e-04 - val_loss: 2.5667e-04\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 2.3258e-04 - val_loss: 1.9009e-04\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.7129e-04 - val_loss: 1.3907e-04\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 1.2204e-04 - val_loss: 9.7079e-05\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 8.4619e-05 - val_loss: 6.7516e-05\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 5.7049e-05 - val_loss: 4.3601e-05\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 3.7033e-05 - val_loss: 2.8060e-05\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.3486e-05 - val_loss: 1.7247e-05\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.4301e-05 - val_loss: 1.0440e-05\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 8.2695e-06 - val_loss: 5.7747e-06\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 4.6195e-06 - val_loss: 3.1755e-06\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 2.4213e-06 - val_loss: 1.6124e-06\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.2191e-06 - val_loss: 7.7628e-07\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 5.7501e-07 - val_loss: 3.7315e-07\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.5690e-07 - val_loss: 1.5323e-07\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.0658e-07 - val_loss: 6.1660e-08\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 4.1949e-08 - val_loss: 2.5065e-08\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 1.5083e-08 - val_loss: 8.3026e-09\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 4.8978e-09 - val_loss: 2.3869e-09\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 1.4517e-09 - val_loss: 7.5099e-10\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 4.1209e-10 - val_loss: 1.8985e-10\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.1727e-10 - val_loss: 5.7652e-11\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 0s 21us/sample - loss: 3.5575e-11 - val_loss: 1.7936e-11\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.4973e-11 - val_loss: 9.0340e-12\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 7.8946e-12 - val_loss: 6.0007e-12\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 7.0223e-12 - val_loss: 6.3739e-12\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 6.4858e-12 - val_loss: 6.2371e-12\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 5.7326e-12 - val_loss: 4.6024e-12\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 4.5569e-12 - val_loss: 3.9334e-12\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 3.7078e-12 - val_loss: 1.2855e-12\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.3609e-12 - val_loss: 1.9350e-12\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.0082e-12 - val_loss: 1.8662e-12\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.2423e-12 - val_loss: 1.9880e-12\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.1878e-12 - val_loss: 2.2353e-12\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 2.0108e-12 - val_loss: 2.6281e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb4cb50dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is class used for fitting it takes input layers and output layers\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "\n",
    "#MSE= Mean Squared Error \n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Fit Our Simple Neural Network\n",
    "# Stop fitting when the validation loss stops improving\n",
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "#Fit\n",
    "model.fit(X,Y,epochs=100,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffb4c0f5898>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can look at a models weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(5, 1) (1,)\n",
      "[[ 2.0000005e+00]\n",
      " [-5.2095615e-08]\n",
      " [-3.1360525e-08]\n",
      " [-1.3854486e-08]\n",
      " [-2.8575475e-09]]\n"
     ]
    }
   ],
   "source": [
    "weights=model.get_weights()\n",
    "print(len(weights))\n",
    "print(weights[0].shape,weights[1].shape)\n",
    "\n",
    "print(weights[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We expect $W_{0,0}$=2, and $B_0$=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= [[ 2.0000005e+00]\n",
      " [-5.2095615e-08]\n",
      " [-3.1360525e-08]\n",
      " [-1.3854486e-08]\n",
      " [-2.8575475e-09]]\n",
      "W[0,0]= 2.0000005\n",
      "B= [0.99999756]\n"
     ]
    }
   ],
   "source": [
    "print(\"W=\",weights[0])\n",
    "print(\"W[0,0]=\",weights[0][0,0])\n",
    "print(\"B=\",weights[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets try something a bit more complicated a sin wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.uniform(0,10,size=(10000,data_dim))\n",
    "def func(X):\n",
    "    return np.sin(X[:,0]) #Ignore all other input have the output only depend on the first dimention\n",
    "Y=func(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 0s 48us/sample - loss: 126.7203 - val_loss: 90.3394\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 62.1705 - val_loss: 42.6584\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 28.5956 - val_loss: 19.7001\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 13.5932 - val_loss: 10.1995\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 7.8312 - val_loss: 6.7302\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 5.8092 - val_loss: 5.4383\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 4.9645 - val_loss: 4.7564\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 4.4077 - val_loss: 4.2251\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 3.9155 - val_loss: 3.7397\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 3.4481 - val_loss: 3.2781\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 3.0028 - val_loss: 2.8452\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 2.5911 - val_loss: 2.4496\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 2.2163 - val_loss: 2.0919\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 1.8816 - val_loss: 1.7749\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.5894 - val_loss: 1.5002\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.3384 - val_loss: 1.2643\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 1.1274 - val_loss: 1.0694\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.9540 - val_loss: 0.9091\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.8159 - val_loss: 0.7837\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.7075 - val_loss: 0.6854\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.6262 - val_loss: 0.6110\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.5659 - val_loss: 0.5572\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 0.5229 - val_loss: 0.5179\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 0.4932 - val_loss: 0.4913\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4735 - val_loss: 0.4727\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4607 - val_loss: 0.4614\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 0.4530 - val_loss: 0.4533\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4487 - val_loss: 0.4487\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4459 - val_loss: 0.4459\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 0.4447 - val_loss: 0.4442\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4440 - val_loss: 0.4440\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4437 - val_loss: 0.4427\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4434 - val_loss: 0.4418\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4433 - val_loss: 0.4415\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4431 - val_loss: 0.4412\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4432 - val_loss: 0.4430\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 0s 25us/sample - loss: 0.4431 - val_loss: 0.4415\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4434 - val_loss: 0.4412\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4429 - val_loss: 0.4407\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4429 - val_loss: 0.4404\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4426 - val_loss: 0.4411\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4425 - val_loss: 0.4402\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4430 - val_loss: 0.4402\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4423 - val_loss: 0.4400\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4418 - val_loss: 0.4418\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4423 - val_loss: 0.4430\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4423 - val_loss: 0.4396\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4417 - val_loss: 0.4395\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4422 - val_loss: 0.4421\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4424 - val_loss: 0.4439\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4413 - val_loss: 0.4402\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4412 - val_loss: 0.4392\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4413 - val_loss: 0.4394\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4415 - val_loss: 0.4410\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4409 - val_loss: 0.4451\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4424 - val_loss: 0.4392\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4417 - val_loss: 0.4387\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4414 - val_loss: 0.4395\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4420 - val_loss: 0.4389\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 0s 22us/sample - loss: 0.4435 - val_loss: 0.4401\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 0s 24us/sample - loss: 0.4413 - val_loss: 0.4401\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 0s 23us/sample - loss: 0.4427 - val_loss: 0.4396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb447d05c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All models start out with an input layer\n",
    "\n",
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "output_layer = tf.keras.layers.Dense(1)(input_layer)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X,Y,epochs=100,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffb4c053710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Oops this didn't work. So far what we wrote above can only be linear\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sum_n W_{i,n}*X_n+B_i$    \n",
    "</p>\n",
    "\n",
    "we need to add something called an activation function $\\sigma$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i = \\sigma(\\sum_n W_{i,n}*X_n+B_i)$    \n",
    "</p>\n",
    "\n",
    "$\\sigma$ has to be non-linear and a good choice is a LeakyReLU\n",
    "\n",
    "<img src='../assets/leakyReLU.png'>\n",
    "\n",
    "Let's also make our model a bit more powerful, but adding more layers $l$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "$O_i,o=X_i$\n",
    "</p>\n",
    " \n",
    "<p style=\"text-align: center;\">  \n",
    "$O_{i,l} = \\sigma(\\sum_n W_{i,l,n}*O_{i,l-1}+B_{i,l})$    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                120       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 981\n",
      "Trainable params: 981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer=tf.keras.layers.Input(shape=(data_dim,)) \n",
    "###Lets Add another layer and an Activation###\n",
    "nn = tf.keras.layers.Dense(20)(input_layer)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "nn = tf.keras.layers.Dense(20)(nn)\n",
    "nn = tf.keras.layers.LeakyReLU()(nn)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1)(nn)\n",
    "#A keras model is a way of going from one layer to the next\n",
    "model=tf.keras.models.Model(input_layer,output_layer)\n",
    "model.summary()\n",
    "model.compile(loss='mse',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 0s 79us/sample - loss: 0.5067 - val_loss: 0.4160\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.3811 - val_loss: 0.3575\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.3482 - val_loss: 0.3230\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.3197 - val_loss: 0.3211\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 0s 35us/sample - loss: 0.3012 - val_loss: 0.2815\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.2801 - val_loss: 0.2755\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 0s 32us/sample - loss: 0.2582 - val_loss: 0.2480\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.2377 - val_loss: 0.2370\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.2258 - val_loss: 0.2329\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.2077 - val_loss: 0.1979\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.1965 - val_loss: 0.1843\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.1875 - val_loss: 0.1718\n",
      "Epoch 13/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.1706 - val_loss: 0.1623\n",
      "Epoch 14/50\n",
      "5000/5000 [==============================] - 0s 38us/sample - loss: 0.1606 - val_loss: 0.1503\n",
      "Epoch 15/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.1506 - val_loss: 0.1417\n",
      "Epoch 16/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.1405 - val_loss: 0.1289\n",
      "Epoch 17/50\n",
      "5000/5000 [==============================] - 0s 35us/sample - loss: 0.1398 - val_loss: 0.1276\n",
      "Epoch 18/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.1228 - val_loss: 0.1252\n",
      "Epoch 19/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.1088 - val_loss: 0.0953\n",
      "Epoch 20/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0948 - val_loss: 0.0880\n",
      "Epoch 21/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.0837 - val_loss: 0.0765\n",
      "Epoch 22/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0729 - val_loss: 0.1012\n",
      "Epoch 23/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0607 - val_loss: 0.0554\n",
      "Epoch 24/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0515 - val_loss: 0.0603\n",
      "Epoch 25/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.0449 - val_loss: 0.0460\n",
      "Epoch 26/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0370 - val_loss: 0.0330\n",
      "Epoch 27/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0313 - val_loss: 0.0354\n",
      "Epoch 28/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0256 - val_loss: 0.0237\n",
      "Epoch 29/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0206 - val_loss: 0.0221\n",
      "Epoch 30/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 31/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0155 - val_loss: 0.0163\n",
      "Epoch 32/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0143 - val_loss: 0.0119\n",
      "Epoch 33/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 34/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 35/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 36/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0087 - val_loss: 0.0077\n",
      "Epoch 37/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 38/50\n",
      "5000/5000 [==============================] - 0s 44us/sample - loss: 0.0066 - val_loss: 0.0055\n",
      "Epoch 39/50\n",
      "5000/5000 [==============================] - 0s 36us/sample - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 40/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 41/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 42/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0048 - val_loss: 0.0067\n",
      "Epoch 43/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 44/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0039 - val_loss: 0.0089\n",
      "Epoch 45/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 46/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 47/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 48/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 49/50\n",
      "5000/5000 [==============================] - 0s 34us/sample - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 50/50\n",
      "5000/5000 [==============================] - 0s 33us/sample - loss: 0.0032 - val_loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb3c5c16a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=50,validation_split=0.5,callbacks=[es]) #Have Keras make a test/validation split for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffb3c567908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_test=np.random.uniform(0,10,size=(100,data_dim))\n",
    "X_test[:,0]=np.linspace(-5,15,100)\n",
    "Y_test=func(X_test)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test[:,0],Y_pred,label='prediction')\n",
    "plt.scatter(X_test[:,0],Y_test,label='truth')\n",
    "plt.xlabel('X[:,0]')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data fits the sin curve perfectly where it had seen training data 0-10, and not so well where there was no training data. Neural networks are universal function approximators, you have little control of what they predict when given data that is completely new. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense network summary\n",
    "\n",
    "* Dense networks take fixed length input and have a fixed length output\n",
    "* Like All Neural Network layers they require an activation function\n",
    "* They can be stacked to represent more complicated functions\n",
    "* You're taking your chances when predicting data that's very different from you're training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequences\n",
    "A lot of data we have doesn't have a fixed dimension, for example text. To make our predictions we need another kind of layer called a recurrent layer. \n",
    "\n",
    "<img src=\"../assets/rnn.gif\">\n",
    "\n",
    "Recurrent layers step through each data point in a sequence, and output one number at the end (or another sequence)\n",
    "Examples:\n",
    "* RNN : First simplest recurrent layer\n",
    "* LSTM: Long Short Term Memory Networks\n",
    "* GRU: Gated Recurrent Unit\n",
    "\n",
    "All of the above are implemented differently with different strengths, but for now lets stick with an LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.]\n",
      " [20.]\n",
      " [21.]\n",
      " [22.]\n",
      " [23.]] 24.0\n",
      "(10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for i in range(10000):\n",
    "    _dp=[]\n",
    "    _dp=np.random.randint(50)+np.linspace(0,4,5)\n",
    "    Y.append(_dp[-1]+1)\n",
    "    X.append(np.expand_dims(_dp,-1))\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "print(X[0],Y[0])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_7:0\", shape=(?, ?, 1), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, None, 1)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 1s 258us/sample - loss: 523.2997 - val_loss: 313.9036\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 197.6597 - val_loss: 115.7130\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 72.1972 - val_loss: 40.6802\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 25.9955 - val_loss: 15.4394\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 10.9359 - val_loss: 7.4971\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 6.0313 - val_loss: 4.7683\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 4.1488 - val_loss: 3.5220\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 3.1525 - val_loss: 2.7842\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 2.5422 - val_loss: 2.2953\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 2.1153 - val_loss: 1.9627\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 1.8040 - val_loss: 1.6884\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 1.5670 - val_loss: 1.4965\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 1.3867 - val_loss: 1.3273\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 1.2340 - val_loss: 1.1923\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 1.1194 - val_loss: 1.0851\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 1.0214 - val_loss: 0.9915\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.9346 - val_loss: 0.9077\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.8577 - val_loss: 0.7640\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 1s 111us/sample - loss: 0.3430 - val_loss: 0.2674\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.2318 - val_loss: 0.2380\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 1s 109us/sample - loss: 0.2023 - val_loss: 0.2078\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.1794 - val_loss: 0.1888\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.1649 - val_loss: 0.1726\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.1531 - val_loss: 0.1672\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 1s 111us/sample - loss: 0.1448 - val_loss: 0.1543\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.1367 - val_loss: 0.1445\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.1307 - val_loss: 0.1390\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.1243 - val_loss: 0.1330\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.1195 - val_loss: 0.1256\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.1149 - val_loss: 0.1205\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.1094 - val_loss: 0.1135\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.1034 - val_loss: 0.1068\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0986 - val_loss: 0.1018\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0949 - val_loss: 0.0979\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0921 - val_loss: 0.0951\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0896 - val_loss: 0.0917\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.0873 - val_loss: 0.0896\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0859 - val_loss: 0.0881\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0846 - val_loss: 0.0861\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0821 - val_loss: 0.0830\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0797 - val_loss: 0.0812\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0782 - val_loss: 0.0799\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0768 - val_loss: 0.0774\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0752 - val_loss: 0.0759\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0737 - val_loss: 0.0738\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0724 - val_loss: 0.0729\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0708 - val_loss: 0.0706\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.0687 - val_loss: 0.0688\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0675 - val_loss: 0.0677\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 1s 108us/sample - loss: 0.0664 - val_loss: 0.0671\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 1s 110us/sample - loss: 0.0656 - val_loss: 0.0663\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0651 - val_loss: 0.0658\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 1s 116us/sample - loss: 0.0640 - val_loss: 0.0638\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 1s 103us/sample - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0581 - val_loss: 0.0579\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.0554 - val_loss: 0.0552\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 1s 113us/sample - loss: 0.0531 - val_loss: 0.0529\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0511 - val_loss: 0.0510\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0477 - val_loss: 0.0477\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0464 - val_loss: 0.0463\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0451 - val_loss: 0.0472\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0441 - val_loss: 0.0443\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.0431 - val_loss: 0.0433\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0423 - val_loss: 0.0422\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0409 - val_loss: 0.0409\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0398 - val_loss: 0.0422\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0386 - val_loss: 0.0388\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0380 - val_loss: 0.0374\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 0s 100us/sample - loss: 0.0362 - val_loss: 0.0398\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0359 - val_loss: 0.0352\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 1s 100us/sample - loss: 0.0340 - val_loss: 0.0345\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0327 - val_loss: 0.0334\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 1s 101us/sample - loss: 0.0316 - val_loss: 0.0324\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 1s 108us/sample - loss: 0.0313 - val_loss: 0.0326\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0306 - val_loss: 0.0317\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0302 - val_loss: 0.0309\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.0300 - val_loss: 0.0303\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0297 - val_loss: 0.0313\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0294 - val_loss: 0.0299\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.0292 - val_loss: 0.0297\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0287 - val_loss: 0.0303\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 1s 108us/sample - loss: 0.0288 - val_loss: 0.0295\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 1s 102us/sample - loss: 0.0280 - val_loss: 0.0288\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0283 - val_loss: 0.0288\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0276 - val_loss: 0.0284\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0278 - val_loss: 0.0292\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0273 - val_loss: 0.0275\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0270 - val_loss: 0.0274\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0261 - val_loss: 0.0274\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0261 - val_loss: 0.0265\n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.0256 - val_loss: 0.0288\n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 1s 105us/sample - loss: 0.0241 - val_loss: 0.0238\n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 1s 107us/sample - loss: 0.0224 - val_loss: 0.0228\n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 1s 108us/sample - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 1s 104us/sample - loss: 0.0216 - val_loss: 0.0215\n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0202 - val_loss: 0.0206\n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 1s 106us/sample - loss: 0.0190 - val_loss: 0.0190\n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 1s 109us/sample - loss: 0.0180 - val_loss: 0.0183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffab71ccf60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer=tf.keras.layers.Input((None,1))\n",
    "print(input_layer)\n",
    "output_layer=tf.keras.layers.LSTM(1,activation='linear')(input_layer)\n",
    "model=tf.keras.models.Model([input_layer],[output_layer])\n",
    "opt=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='mse',optimizer=opt)\n",
    "model.summary()\n",
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.fit(X,Y,validation_split=0.5,epochs=100,callbacks=[es])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0952587]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.expand_dims([[2,3,4,5,6]],-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Data\n",
    "The coding/research part of most machine learning algorithms is how to utilize data in a way an algorthim understands.\n",
    "\n",
    "### Goal\n",
    "Read the raw the raw text from these movie reviews, and predict wether the review is positive or not\n",
    "* Need to go from an array (1-D unknown length) to a probability (1 number)\n",
    "* Need to build a series of layers to make that possible\n",
    "* We will take text that is transformed into an sequence of integers\n",
    "  * For this data we assign each word (token) in a sentence a unique integer\n",
    "* We will transform the sequence of integers into an sequence of vectors\n",
    "    * Do this with a new Embedding Layer\n",
    "* Then use an LSTM layer, and a Dense layer to make a prediction\n",
    "\n",
    "Array of Ints -> **Embedding** -> Array of Vectors -> **RNN** -> fixed output -> **Dense** -> Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Lets load some input data in the 2-split format\n",
    "index_from=3\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=index_from)\n",
    "\n",
    "\n",
    "word_2_index={k:(v+index_from) for k,v in tf.keras.datasets.imdb.get_word_index().items()}\n",
    "word_2_index['<PAD>']=0\n",
    "word_2_index['<START>']=1\n",
    "word_2_index['<UNK>']=2\n",
    "\n",
    "index_2_word={}\n",
    "\n",
    "for word in word_2_index:\n",
    "    index_2_word[ word_2_index[word]]=word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence'] [14, 9, 6, 4130]\n"
     ]
    }
   ],
   "source": [
    "check=['this','is','a','sentence']\n",
    "print(check, [word_2_index[i] for i in check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "last_word=np.max(list(word_2_index.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "1 = Positive Review 0 = Negative Review \n",
      "label 1\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "print(\" \".join([index_2_word[i] for i in x_train[0]]))\n",
    "\n",
    "print('1 = Positive Review','0 = Negative Review ')\n",
    "print('label',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note:**\n",
    "This data has already been 'tokenized' meaning the text has been pre-processed. In this case made lowercase with punctuation removed. There are many different ways of doing this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_5:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_layer=tf.keras.layers.Input( (None,))\n",
    "print(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         8858700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                4440      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 8,863,261\n",
      "Trainable params: 8,863,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn=tf.keras.layers.Embedding(last_word,100)(input_layer)\n",
    "nn=tf.keras.layers.LSTM(10)(nn)\n",
    "nn=tf.keras.layers.Dense(10)(nn)\n",
    "nn=tf.keras.layers.LeakyReLU()(nn)\n",
    "output=tf.keras.layers.Dense(1,activation='sigmoid')(nn)\n",
    "\n",
    "model=tf.keras.models.Model(input_layer,output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binary Cross Entropy\n",
    "\n",
    "Our last layer here is using a sigmoid activation, which is bounded between zero and 1\n",
    "\n",
    "<img src=\"../assets/sigmoid.png\">\n",
    "\n",
    "The activation on your last layer has to match your loss function in this case 'Binary Cross-Entropy'\n",
    "<p style=\"text-align: center;\">\n",
    "$L= -1*\\sum_i y_{true,i}*ln(y_{pred,i}) + (1-y_{true,i})*ln(1-y_{pred,i}) $\n",
    "</p>\n",
    "Which is minimized when $y_{pred}=y_{true}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We need each batch given to the model to have the same size\n",
    "# For right now we will just make all the data the same size by padding or cropping to a length of 200\n",
    "\n",
    "x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200, dtype='int32',value=0.0)\n",
    "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200, dtype='int32',value=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 137s 5ms/sample - loss: 0.4627 - acc: 0.7792 - val_loss: 0.3602 - val_acc: 0.8487\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 136s 5ms/sample - loss: 0.2134 - acc: 0.9193 - val_loss: 0.3915 - val_acc: 0.8471\n"
     ]
    }
   ],
   "source": [
    "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_2_ints(sentence):\n",
    "    return np.array([[word_2_index[s] for s in sentence.split()]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87367845]]\n",
      "[[0.5959961]]\n",
      "[[0.73580027]]\n",
      "[[0.10441049]]\n",
      "[[0.44264108]]\n",
      "[[0.3767595]]\n",
      "[[0.3082046]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(sentence_2_ints('<START> this movie is the very best i have ever seen') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> i have mixed feelings about this movie') ))\n",
    "print(model.predict(sentence_2_ints('<START> i have mixed feelings about this movie i may like it in the end') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> i have never seen a worse film') ))\n",
    "print(model.predict(sentence_2_ints('<START> hi is this where i google the information') ))\n",
    "\n",
    "print(model.predict(sentence_2_ints('<START> star trek') ))\n",
    "print(model.predict(sentence_2_ints('<START> star wars') ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A short Menu of other ML layers\n",
    "* Convolutional Layers (Conv1D, Conv2D, Conv3D)\n",
    "    * Input sequences of fixed or varying length best when array values that are close together are correlate i.e pictures\n",
    "    * Output a new sequence normally lower dimension, but with more channels    \n",
    "* Recurrent Neural Networks (RNN, LSTM, GRUS)\n",
    "    * Input sequence\n",
    "    * Output sequence or a fixed dimensional output    \n",
    "\n",
    "* Embedding Network\n",
    "    * A learnable mapping from a large set of integers, to a fixed output\n",
    "    * Input integer\n",
    "    * Ouput vector\n",
    "\n",
    "* Dense Network\n",
    "    * Fixed Input\n",
    "    * Fixed Output\n",
    "\n",
    "* Dropout\n",
    "    * Good at preventing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
