{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Classification can tell you what object you are seeing but you may also need to answer\n",
    "* Where is an object\n",
    "* How many are there\n",
    "\n",
    "We generally call this object detection\n",
    "\n",
    "# Outline\n",
    "\n",
    "\n",
    "\n",
    "* Our Goal for this lecture is to identify signs of Malaria in human blood smears\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Data Set\n",
    "\n",
    "P. vivax (malaria) infected human blood smears\n",
    "\n",
    "Accession number BBBC041 Â· Version 1\n",
    "\n",
    "\n",
    "\n",
    "From the Broad Institute https://data.broadinstitute.org/bbbc/BBBC041/\n",
    "\n",
    "\n",
    "<img src=https://data.broadinstitute.org/bbbc/BBBC041/BBBC041_example1.png>\n",
    "    \n",
    "Malaria is a disease caused by Plasmodium parasites that remains a major threat in global health, affecting 200 million people and causing 400,000 deaths a year. The main species of malaria that affect humans are Plasmodium falciparum and Plasmodium vivax.\n",
    "\n",
    "For malaria as well as other microbial infections, manual inspection of thick and thin blood smears by trained microscopists remains the gold standard for parasite detection and stage determination because of its low reagent and instrument cost and high flexibility. Despite manual inspection being extremely low throughput and susceptible to human bias, automatic counting software remains largely unused because of the wide range of variations in brightfield microscopy images. However, a robust automatic counting and cell classification solution would provide enormous benefits due to faster and more accurate quantitative results without human variability; researchers and medical professionals could better characterize stage-specific drug targets and better quantify patient reactions to drugs.\n",
    "\n",
    "Previous attempts to automate the process of identifying and quantifying malaria have not gained major traction partly due to difficulty of replication, comparison, and extension. Authors also rarely make their image sets available, which precludes replication of results and assessment of potential improvements. The lack of a standard set of images nor standard set of metrics used to report results has impeded the field. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinates \n",
    "\n",
    "We are going to use the index coordinate system\n",
    "\n",
    "<img src=../assets/index_coords.png>\n",
    "\n",
    "* **Packages often have different systems so be careful!**\n",
    "    \n",
    "\n",
    "## Bounding Boxes\n",
    "\n",
    "Data is often labeled by bounding boxes, which also can be describe in a number of different ways. We'll use \n",
    "Upper left corner (note this is the smallest y-coordinate), the length and the width. \n",
    "\n",
    "<img src=../assets/BBox.png>\n",
    "\n",
    "Each bounding box also has a category attached to it. For example moon or cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jsearcy/anaconda3/envs/tensorflow/lib/python3.6/site-packages/mpl_toolkits/axes_grid1/__init__.cpython-36m-x86_64-linux-gnu.so'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-331110d3b98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbbtoydata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBBToyData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/racsml/notebooks/bbtoydata.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookfor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpected_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_as_bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mreset_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mWRAP_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m73\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mreset_plugins\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0m_clear_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0m_load_preferred_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36m_load_preferred_plugins\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                 'imread']\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mio_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0m_set_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_plugins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mplugin_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreferred_plugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36m_set_plugin\u001b[0;34m(plugin_type, plugin_list)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0muse_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36muse_plugin\u001b[0;34m(name, kind)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(plugin)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mmodname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugin_module_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         plugin_module = __import__('skimage.io._plugins.' + modname,\n\u001b[0;32m--> 302\u001b[0;31m                                    fromlist=[modname])\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mprovides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugin_provides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/io/_plugins/matplotlib_plugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bbtoydata import BBToyData\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pixel=0\n",
    "y_pixel=1\n",
    "\n",
    "\n",
    "\n",
    "img=np.zeros((3,3))\n",
    "\n",
    "img[y_pixel,x_pixel]=1.0## Noice y,x here\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(img,cmap='gray')\n",
    "\n",
    "plt.xlabel(r\" X -0.5 $\\rightarrow$ 3.5 \")\n",
    "plt.ylabel(r\" 2.5 $\\leftarrow$ -0.5  Y \")\n",
    "print(img)\n",
    "\n",
    "\n",
    "##Draw a bounding box\n",
    "rec_corner=((x_pixel-0.5,y_pixel-0.5)) #half a pixel Width\n",
    "width=1\n",
    "height=1\n",
    "# We'll uses pat\n",
    "rec=patches.Rectangle(rec_corner, width,height,edgecolor='r',facecolor='none')\n",
    "ax.add_patch(rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add a lot of bounding boxes so let's write a function to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bbox(ax,bbox,color='r'):\n",
    "    corner_x,corner_y,width,height=bbox\n",
    "    rec=patches.Rectangle((corner_x-0.5,corner_y-0.5), width,height,edgecolor=color,facecolor='none')\n",
    "    ax.add_patch(rec)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying where something is with a neural network\n",
    "\n",
    "Again We'll use a toy example that's easy to under and visualize.\n",
    "Start by finding just one object.\n",
    "\n",
    "* 3-Object Classes \n",
    "    * Square\n",
    "    * Circle\n",
    "    * Triangle\n",
    "* 1 - Background Class\n",
    "\n",
    "* 4 - bbox x,y,width,height\n",
    "\n",
    "We need a classification network to identify the shape, and a new network go predict the bounding box\n",
    "\n",
    "* We'll use two prediction networks that share features!\n",
    "\n",
    "<img src=../assets/network_diagrams/1_object_identifier.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=BBToyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, [9.16463740976103, 15.668403826066479, 26.454397849332167, 3.063784274662856]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, [6.242924583548555, 12.157509898107799, 31.12835044044199, 3.24276476773146]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, [22.736121321345838, 10.139542505801655, 16.238982107013378, 21.901847400390746]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, [16.561419765711154, 19.883967042822576, 27.91109245154409, 10.802397590869205]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, [16.279329445659034, 19.17731258064654, 17.297261736651844, 23.4928955598236]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, [14.039501903734056, 34.346041858761275, 25.700386284609444, 4.760041673053234]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, [8.86785797466947, 9.344827485804515, 13.139560281244078, 29.56518872912859]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, [8.408211677918832, 12.027210811599009, 27.20761786344678, 22.890637184477157]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()\n",
    "def plot_example(image,labels):\n",
    "    colors=['r','g','b','y','#000000']\n",
    "    fig,ax = plt.subplots(1,figsize=(10,10))\n",
    "    ax.imshow(image,cmap='gray')\n",
    "    for cat,bbox in labels:\n",
    "        add_bbox(ax,bbox,color=colors[cat])\n",
    "    plt.show()\n",
    "for i in range(10):\n",
    "    index=np.random.randint(bb.X_train.shape[0])\n",
    "    print(bb.Y_train[index])\n",
    "\n",
    "    plot_example(np.squeeze(bb.X_train[index]),(bb.Y_train[index]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build our Convolutional Feature Finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_input=tf.keras.layers.Input( shape=bb.X_train.shape[1:] ) # Shape here does not including the batch size \n",
    "cnn_layer1=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_input) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "\n",
    "cnn_layer2=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_activation) \n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "\n",
    "#cnn_layer3=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_activation) \n",
    "#cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "\n",
    "flat=tf.keras.layers.Flatten()(cnn_activation) \n",
    "flat=tf.keras.layers.Dropout(0.5)(flat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Our Prediction layer\n",
    "cat_output_layer=tf.keras.layers.Dense(4)(flat) \n",
    "cat_output=tf.keras.layers.Activation('softmax')(cat_output_layer)\n",
    "\n",
    "\n",
    "#Make our bounding box regressor\n",
    "bbox_output=tf.keras.layers.Dense(50)(flat) \n",
    "bbox_output=tf.keras.layers.LeakyReLU()(bbox_output)\n",
    "bbox_output=tf.keras.layers.Dense(50)(bbox_output) \n",
    "bbox_output=tf.keras.layers.LeakyReLU()(bbox_output)\n",
    "bbox_output=tf.keras.layers.Dense(4)(bbox_output) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 25, 25, 64)   1088        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 25, 25, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 13, 64)   65600       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10816)        0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10816)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           540850      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           2550        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            43268       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 50)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            204         leaky_re_lu_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 653,560\n",
      "Trainable params: 653,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Model(cnn_input,[cat_output,bbox_output])\n",
    "\n",
    "\n",
    "##Two!! loss fuctions one for each output, the total loss is the sum of each of these\n",
    "model.compile(loss=['categorical_crossentropy','mae'],\n",
    "              optimizer='adam',\n",
    "              metrics=[])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make our Data\n",
    "We need to transform labels from a list to a one hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_single_object_labels(labels):\n",
    "    Y_bbox=[]\n",
    "    Y_cat=[]\n",
    "    for l in labels:\n",
    "        if len(l)==0:\n",
    "            Y_cat.append(3) #Background\n",
    "            Y_bbox.append(np.array([[0,0,1,1]])) #Divide by our image size so everything is between 0-1\n",
    "        else:\n",
    "            cat,bbox=l[0]\n",
    "            Y_cat.append(cat)\n",
    "            Y_bbox.append(np.expand_dims(bbox,0)/50,) #Divide by our image size so everything is between 0-1\n",
    "    Y_cat=tf.keras.utils.to_categorical(Y_cat, num_classes=4)\n",
    "    Y_bbox=np.concatenate(Y_bbox)\n",
    "    return [Y_cat,Y_bbox]\n",
    "\n",
    "Y_train=get_single_object_labels(bb.Y_train)\n",
    "Y_develop=get_single_object_labels(bb.Y_develop)\n",
    "Y_test=get_single_object_labels(bb.Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18014 samples, validate on 79943 samples\n",
      "Epoch 1/25\n",
      "18014/18014 [==============================] - 16s 905us/step - loss: 0.4520 - activation_loss: 0.3860 - dense_3_loss: 0.0660 - val_loss: 0.1788 - val_activation_loss: 0.1358 - val_dense_3_loss: 0.0430\n",
      "Epoch 2/25\n",
      "18014/18014 [==============================] - 14s 758us/step - loss: 0.1498 - activation_loss: 0.1102 - dense_3_loss: 0.0396 - val_loss: 0.1083 - val_activation_loss: 0.0755 - val_dense_3_loss: 0.0328\n",
      "Epoch 3/25\n",
      "18014/18014 [==============================] - 14s 756us/step - loss: 0.1093 - activation_loss: 0.0729 - dense_3_loss: 0.0364 - val_loss: 0.0793 - val_activation_loss: 0.0511 - val_dense_3_loss: 0.0282\n",
      "Epoch 4/25\n",
      "18014/18014 [==============================] - 14s 754us/step - loss: 0.0978 - activation_loss: 0.0635 - dense_3_loss: 0.0343 - val_loss: 0.0598 - val_activation_loss: 0.0362 - val_dense_3_loss: 0.0236\n",
      "Epoch 5/25\n",
      "18014/18014 [==============================] - 14s 755us/step - loss: 0.0755 - activation_loss: 0.0464 - dense_3_loss: 0.0291 - val_loss: 0.0556 - val_activation_loss: 0.0353 - val_dense_3_loss: 0.0203\n",
      "Epoch 6/25\n",
      "18014/18014 [==============================] - 14s 750us/step - loss: 0.0748 - activation_loss: 0.0454 - dense_3_loss: 0.0295 - val_loss: 0.0604 - val_activation_loss: 0.0322 - val_dense_3_loss: 0.0281\n",
      "Epoch 7/25\n",
      "18014/18014 [==============================] - 13s 729us/step - loss: 0.0681 - activation_loss: 0.0404 - dense_3_loss: 0.0277 - val_loss: 0.0421 - val_activation_loss: 0.0229 - val_dense_3_loss: 0.0192\n",
      "Epoch 8/25\n",
      "18014/18014 [==============================] - 13s 749us/step - loss: 0.0663 - activation_loss: 0.0381 - dense_3_loss: 0.0282 - val_loss: 0.0429 - val_activation_loss: 0.0220 - val_dense_3_loss: 0.0210\n",
      "Epoch 9/25\n",
      "18014/18014 [==============================] - 14s 749us/step - loss: 0.0496 - activation_loss: 0.0254 - dense_3_loss: 0.0243 - val_loss: 0.0427 - val_activation_loss: 0.0243 - val_dense_3_loss: 0.0184\n",
      "Epoch 10/25\n",
      "18014/18014 [==============================] - 14s 756us/step - loss: 0.0582 - activation_loss: 0.0318 - dense_3_loss: 0.0264 - val_loss: 0.0582 - val_activation_loss: 0.0236 - val_dense_3_loss: 0.0346\n",
      "Epoch 11/25\n",
      "18014/18014 [==============================] - 14s 756us/step - loss: 0.0582 - activation_loss: 0.0324 - dense_3_loss: 0.0258 - val_loss: 0.0347 - val_activation_loss: 0.0171 - val_dense_3_loss: 0.0176\n",
      "Epoch 12/25\n",
      "18014/18014 [==============================] - 14s 754us/step - loss: 0.0440 - activation_loss: 0.0216 - dense_3_loss: 0.0224 - val_loss: 0.0393 - val_activation_loss: 0.0225 - val_dense_3_loss: 0.0168\n",
      "Epoch 13/25\n",
      "18014/18014 [==============================] - 13s 748us/step - loss: 0.0444 - activation_loss: 0.0222 - dense_3_loss: 0.0222 - val_loss: 0.0551 - val_activation_loss: 0.0308 - val_dense_3_loss: 0.0243\n",
      "Epoch 14/25\n",
      "17568/18014 [============================>.] - ETA: 0s - loss: 0.0556 - activation_loss: 0.0303 - dense_3_loss: 0.0252"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model.fit(bb.X_train, Y_train, \n",
    "          batch_size=32, epochs=25, verbose=1,\n",
    "         validation_data=(bb.X_develop,Y_develop)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cat_pred,bbox_pred=model.predict(bb.X_develop)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):    \n",
    "    cat=np.argmax(cat_pred[i])\n",
    "    label=[ [cat,bbox_pred[i]*50]]\n",
    "    plot_example(np.squeeze(bb.X_develop[i]),label)\n",
    "    print(cat,bbox_pred[i]*100)\n",
    "    print(bb.Y_develop[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying more than one thing at a time\n",
    "\n",
    "**Run the next several cells to get your model training before we get started**\n",
    "\n",
    "\n",
    "That worked pretty well, but we have another problem what if there is more than one object in the image. \n",
    "* There are several ways to handle this\n",
    "    * Scan the image (like we did with the cancer example)\n",
    "        * This works but can be expensive\n",
    "    * Use multiple detectors\n",
    "        * Strategry used by algorithms like SSD (single shot detector) or YOLO (you only look one)\n",
    "        * Faster and better suited for devices\n",
    "* Will discuss using multiple detectors\n",
    "    * The algorithm above had 1 detector, and predicted one bounding box and one \n",
    "    * We can add more of them to detect several objects\n",
    "* Challenges\n",
    "    * How do we efficiently add detectors\n",
    "    * How do we assign objects to detectors\n",
    "\n",
    "* Convolutional Detectors\n",
    "    * One way to add a lot of detectors quickly is to not use a dense layer at all\n",
    "        * We replace the dense network above with two convolutional layers\n",
    "        * Each layer will have 4 filters (so the output will have 4 channels)\n",
    "            * We can apply a softmax to each pixel in one layer to get a category prediction\n",
    "            * We can use the second layer for bounding box predictions\n",
    "        \n",
    "    \n",
    "\n",
    "<img src=../assets/network_diagrams/multi_object_identifier.png>\n",
    "\n",
    "In the case below our output CNN layers will have a shape of 5 pixels by 5 pixels,\n",
    "so we we'll assign each of these pixels to watch a box in the original input image.\n",
    "The area each pixel is watching for is called a prior box or an anchor. \n",
    "\n",
    "If the center of an object in in the box we assign it to that detector, otherwise a detector is assigned a background class.\n",
    "\n",
    "<img src=../assets/network_diagrams/Detectors.png>\n",
    "\n",
    "Another detail is instead of directly predicting the bounding box with make a prediction with respect to the prior box, we predict the x,y offsets to the new bounding box and the length and width scales.  \n",
    "\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build It\n",
    "Same problem now with the possiblity of multiple objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_multi=BBToyData(multi_object= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(np.squeeze(bb_multi.X_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_input=tf.keras.layers.Input( shape=bb_multi.X_train.shape[1:] ) # Shape here does not including the batch size \n",
    "cnn_layer1=tf.keras.layers.Convolution2D(64, (4,4),strides=2,padding='same')(cnn_input) #50x50\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "cnn_activation=tf.keras.layers.BatchNormalization()(cnn_activation) \n",
    "\n",
    "cnn_layer2=tf.keras.layers.Convolution2D(128, (4,4),strides=2,padding='same')(cnn_activation) #25x25\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "cnn_activation=tf.keras.layers.BatchNormalization()(cnn_activation) \n",
    "\n",
    "cnn_layer3=tf.keras.layers.Convolution2D(256, (10,10),strides=5,padding='same')(cnn_activation) #5x5\n",
    "cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "cnn_activation=tf.keras.layers.BatchNormalization()(cnn_activation) \n",
    "cnn_activation=tf.keras.layers.Dropout(.2)(cnn_activation) \n",
    "\n",
    "\n",
    "###Prediction Layers\n",
    "prediction=tf.keras.layers.Convolution2D(4,(5,5),padding='same',activation='softmax')(cnn_activation)\n",
    "#Softmax is applied to the last \n",
    "bbox_offset=tf.keras.layers.Convolution2D(4,(5,5),padding='same')(cnn_activation)\n",
    "\n",
    "\n",
    "model=tf.keras.models.Model(cnn_input,[prediction,bbox_offset])\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','mse'],\n",
    "                  optimizer='adam',\n",
    "                  metrics=[])\n",
    "model.summary()\n",
    "\n",
    "test,test1=model.predict(bb_multi.X_train[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_labels(input_labels):\n",
    "    Y_train=[[],[]]\n",
    "    \n",
    "    for image_labels in input_labels:\n",
    "        class_features=np.zeros((5,5,4))\n",
    "        class_features[:,:,3]=1. #Defaults to no class\n",
    "\n",
    "        bbox_features=np.zeros((5,5,4))\n",
    "        bbox_features[:,:,2:]=1. #Scale =1\n",
    "\n",
    "        detected=[]\n",
    "        \n",
    "        \n",
    "        for i,(cat,bbox) in enumerate(image_labels):\n",
    "            bbox=[i/100. for i in bbox] # Normalize\n",
    "            bbox_center_x=int((bbox[0]+bbox[2]/2.)//0.2) #priorbox bin\n",
    "            bbox_center_y=int((bbox[1]+bbox[3]/2.)//0.2) #priorbox bin\n",
    "            \n",
    "            bbox_offset_x=(bbox[0]+bbox[2]/2.)/0.2-(bbox_center_x+0.5)#priorbox bin\n",
    "            bbox_offset_y=(bbox[1]+bbox[3]/2.)/0.2-(bbox_center_y+0.5) #priorbox bin\n",
    "            \n",
    "            bbox_scale_x=bbox[2]/0.2\n",
    "            bbox_scale_y=bbox[3]/0.2\n",
    "            # Y comes first here\n",
    "            class_features[bbox_center_y,bbox_center_x,cat]=25\n",
    "            class_features[bbox_center_y,bbox_center_x,3]=0\n",
    "\n",
    "            bbox_features[bbox_center_y,bbox_center_x,:]=[bbox_offset_x,bbox_offset_y,bbox_scale_x,bbox_scale_y]\n",
    "        Y_train[0].append(np.expand_dims(class_features,0))\n",
    "        Y_train[1].append(np.expand_dims(bbox_features,0))\n",
    "    Y_train[0]=np.concatenate(Y_train[0])\n",
    "    Y_train[1]=np.concatenate(Y_train[1])\n",
    "    \n",
    "    return Y_train\n",
    "Y_train=get_labels(bb_multi.Y_train)\n",
    "Y_develop=get_labels(bb_multi.Y_develop)\n",
    "Y_test=get_labels(bb_multi.Y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plt.imshow(np.argmax(Y_train[0][0],axis=-1))\n",
    "bbox=Y_train[1][0]\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    index=np.random.randint(len(bb_multi.X_train))\n",
    "    plot_example(np.squeeze(bb_multi.X_train[index]),bb_multi.Y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(bb_multi.X_train, Y_train, \n",
    "        batch_size=32, epochs=25, verbose=1,\n",
    "        validation_data=(bb_multi.X_develop,Y_develop)\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_labels(image,cat_map,bbox_map,show_background=False):\n",
    "    # Each map is a 5x5 image \n",
    "    labels=[]\n",
    "    for y,r in enumerate(bbox_map):\n",
    "        for x,v in enumerate(r):\n",
    "            cat=np.argmax(cat_map[y,x,:])\n",
    "#            if cat_map[y,x,cat] < 0.9:continue\n",
    "            if not show_background and cat==3:continue\n",
    "            #Center - l/2\n",
    "            width=.2*bbox_map[y,x,2]\n",
    "            height=.2*bbox_map[y,x,3]\n",
    "            if cat !=3:print(bbox_map[y,x,:])\n",
    "\n",
    "            x_start=(x+bbox_map[y,x,0]+.5)*.2 -width/2.\n",
    "            y_start=(y+bbox_map[y,x,1]+.5)*.2 -height/2.\n",
    "            labels.append([cat,[(x_start)*100,y_start*100,width*100,height*100]])\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "cat_map,bbox_map=model.predict(bb_multi.X_develop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    labels=predictions_to_labels(np.squeeze(bb_multi.X_develop[i]),cat_map[i],bbox_map[i],show_background=True)\n",
    "\n",
    "#    labels=predictions_to_labels(np.squeeze(bb_multi.X_develop[i]),Y_develop[0][i],Y_develop[1][i],show_background=True)\n",
    "\n",
    "    \n",
    "    plot_example(np.squeeze(bb_multi.X_develop[i]),labels)\n",
    "    plt.imshow(np.hstack([cat_map[i,:,:,n] for n in range(4)]))\n",
    "  \n",
    "    plt.show()\n",
    "    plt.imshow(np.hstack([Y_develop[0][i,:,:,n] for n in range(4)]))\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(np.hstack([Y_develop[1][i,:,:,n] for n in range(4)]))\n",
    "    plt.show()\n",
    "\n",
    "    print(Y_develop[1][i].shape)\n",
    "    plt.imshow(Y_develop[1][i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fullly Functional Algorigthm\n",
    "\n",
    "There are still problems with the simple model above\n",
    "* What happens if more than one object matches a box\n",
    "* You might see problems if the objects are really big\n",
    "* You might also see problems if the objects are really smalle\n",
    "\n",
    "The best object detection algorithm use several prior boxes per location: i.e SSD\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/974/1*51joMGlhxvftTxGtA4lA7Q.png\">\n",
    "\n",
    "* We used one convolutional map that was 5x5\n",
    "* These algorithms use several maps with different sizes (for smaller and larger objects)\n",
    "* We only used square prior boxes\n",
    "      * These algorithms use several aspect ratio bounding boxes per point\n",
    "**These are well egenieered, and can take some time to reproduce, so we'll use an existing implementation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model is a good example of how everything we've talked about can be rolled to a one analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./ssd_keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to design a system to help with malaria Diagnosis\n",
    "   * We have blood smear slides labeled with bounding boxes\n",
    "   * Each bounding box identifies an infected cell and it's stage of development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict={}\n",
    "cat_dict[\"background\"]=0\n",
    "cat_dict[\"ring\"]=1\n",
    "cat_dict['trophozoite']=2\n",
    "cat_dict['schizont']=3\n",
    "cat_dict['gametocyte']=4\n",
    "\n",
    "int_2_cat={}\n",
    "for i,v in cat_dict.items():\n",
    "    int_2_cat[v]=i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ring stage example\n",
    "<img src=\"https://www.mcdinternational.org/trainings/malaria/english/DPDx5/images/ParasiteImages/M-R/Malaria/falciparum/Pfal-rings-atlasdx.JPG\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Annotations come in a json format\n",
    "* These generally look like dictionaries to python users\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is stored on Talapas\n",
    "images_dir=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/\"\n",
    "annotations_train=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/train.json\"\n",
    "annotations_develop=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/develop.json\"\n",
    "annotations_test=\"/projects/bgmp/shared/2019_ML_workshop/datasets/BBBC041o/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_label=json.load(open(annotations_train))\n",
    "\n",
    "\n",
    "print(train_label.keys())\n",
    "print(\"Image Data\",train_label['images'][0])\n",
    "\n",
    "print(\"Annotations 0\",train_label['annotations'][0])\n",
    "print(\"Annotations 1\",train_label['annotations'][1])\n",
    "print(\"Annotations 2\",train_label['annotations'][2])\n",
    "print(\"Annotations 3\",train_label['annotations'][3])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above the list of annotations is matched to images using the tag image_id\n",
    "* This is informational, but our package knows how to read these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important data checks\n",
    "\n",
    "\n",
    "all_bboxs=[ anno['bbox'] for anno in train_label['annotations']]\n",
    "all_classes=[ anno['category_id'] for anno in train_label['annotations']]\n",
    "\n",
    "for select in range(1,5):\n",
    "    widths=[bb[2] for bb,cat in zip(all_bboxs,all_classes) if cat == select  ] \n",
    "    heights=[bb[3] for bb,cat in zip(all_bboxs,all_classes) if cat == select ] \n",
    "    areas=[bb[3]*bb[2] for bb,cat in zip(all_bboxs,all_classes) if cat == select  ] \n",
    "\n",
    "    print( len(widths), int_2_cat[select], \" in training set\" )\n",
    "    print(\"Average Width\", np.mean(widths),'pixels')\n",
    "    print(\"Average Height\", np.mean(heights),'pixels')\n",
    "\n",
    "    print(\"Max Width\", np.max(widths),'pixels')\n",
    "    print(\"Max Height\", np.max(heights),'pixels')\n",
    "\n",
    "    print(\"Min Width\", np.min(widths),'pixels')\n",
    "    print(\"Min Height\", np.min(heights),'pixels')\n",
    "\n",
    "    print(\"__________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our targets are mostly trophozites \n",
    "* Our targets are fairly square \n",
    "* Our targets are ~70-255 pixels in size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of imports for our package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "\n",
    "\n",
    "#from models.keras_ssd7 import build_model\n",
    "from models.keras_ssd300 import ssd_300 as build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data generators to load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 1200 # Height of the input images\n",
    "img_width = 1600 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "image_augmentation=SSDDataAugmentation(img_height,img_width)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.parse_json(        images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_train],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "develop_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "\n",
    "\n",
    "develop_dataset.parse_json(       images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_develop],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n",
    "\n",
    "test_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "\n",
    "test_dataset.parse_json(         images_dirs=images_dir,\n",
    "                                 annotations_filenames=[annotations_test],\n",
    "                                 ground_truth_available=True,\n",
    "                                 include_classes='all'\n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_generator =    train_dataset.generate(batch_size=10,\n",
    "                                         shuffle=False,\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and draw example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab a batch of images\n",
    "\n",
    "batch_images, batch_labels, batch_filenames = next(plot_generator)\n",
    "print(len(batch_images))\n",
    "#These labels are in corner representation (xmin,ymin) (xmax,ymax)\n",
    "# Need to convert to (xmin,ymin),(lenth,width)\n",
    "\n",
    "def convert_ssd_labels(label):\n",
    "    if len(label)==5:\n",
    "        return [int(label[0]),[label[1],label[2],label[3]-label[1],label[4]-label[2]]] \n",
    "    if len(label)==6: #This are labels that also include a prediction\n",
    "        return [int(label[0]),[label[2],label[3],label[4]-label[2],label[5]-label[3]]] \n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    labels=[convert_ssd_labels(l) for l in batch_labels[i]]\n",
    "    print(labels)\n",
    "    plot_example(batch_images[i],labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color)\n",
    "\n",
    "\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "\n",
    "\n",
    "#predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "#                   model.get_layer('classes5').output_shape[1:3],\n",
    "#                   model.get_layer('classes6').output_shape[1:3],\n",
    "#                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "#ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "#                                    img_width=img_width,\n",
    "#                                    n_classes=n_classes,\n",
    "#                                    predictor_sizes=predictor_sizes,\n",
    "#                                    variances=[1.,1., 1., 1.],\n",
    "#                                    matching_type='multi',\n",
    "#                                    pos_iou_threshold=0.5,\n",
    "#                                    neg_iou_limit=0.3,\n",
    "#                                    normalize_coords=normalize_coords,\n",
    "#                                    background_id=0 # This has to be zero for decoder to work                               \n",
    "#                                   )\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "batch_size=5\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[image_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "develop_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "_1,_2=next(train_generator)\n",
    "\n",
    "print(_2.shape)\n",
    "\n",
    "#model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "input_1 (InputLayer)            (None, 1200, 1600, 3) 0                                            \n",
    " __________________________________________________________________________________________________\n",
    "\n",
    "classes4 (Conv2D)               (None, 150, 200, 20) 11540       elu4[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "classes5 (Conv2D)               (None, 75, 100, 20)  8660        elu5[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "classes6 (Conv2D)               (None, 37, 50, 20)   8660        elu6[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "classes7 (Conv2D)               (None, 18, 25, 20)   5780        elu7[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "boxes4 (Conv2D)                 (None, 150, 200, 16) 9232        elu4[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "boxes5 (Conv2D)                 (None, 75, 100, 16)  6928        elu5[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "boxes6 (Conv2D)                 (None, 37, 50, 16)   6928        elu6[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "boxes7 (Conv2D)                 (None, 18, 25, 16)   4624        elu7[0][0]               \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors at different resolution\n",
    "### Boxes layer size\n",
    "\n",
    "4 boxes per pixel * 4 predictions per box = 16 measurements\n",
    "### Boxes layer size\n",
    "\n",
    "5 (4+1 background) classes per box = 4*5=20\n",
    "\n",
    "| Map Size      |Pixels in original image per prediction feature pixel|\n",
    "| ------------- |-------------|\n",
    "|150x200 |  8x8   |\n",
    "|75x100  |  16x16 |\n",
    "|37x50   |  ~32x32 |\n",
    "|18x25   |  ~64x64 |\n",
    "\n",
    "\n",
    "**One thing to notice immedialte is that our 'biggest' detector starts out at 64x64 pixels**\n",
    "* What was the average size of our objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd.h5\")\n",
    "\n",
    "if True:\n",
    "\n",
    "    final_epoch     = 500\n",
    "    \n",
    "    history = model.fit_generator(generator=train_generator,\n",
    "                              epochs=final_epoch,\n",
    "                              steps_per_epoch=len(train_dataset.images)//batch_size,\n",
    "                              validation_data=develop_generator,\n",
    "                              validation_steps=len(develop_dataset.images)//batch_size,\n",
    "                              )\n",
    "\n",
    "    #model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'processed_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_images,batch_labels=next(example_generator)\n",
    "y_pred = model.predict(batch_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                       confidence_thresh=0.1,\n",
    "                                       iou_threshold=0.1,\n",
    "                                       top_k=200,\n",
    "                                       normalize_coords=True,#normalize_coords,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    labels=[convert_ssd_labels(l) for l in y_pred_decoded[i]]\n",
    "    plot_example(batch_images[i],labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets evaluate the model and do some experiments\n",
    "\n",
    "We are far from perfect, but lets evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "all_truth=[]\n",
    "develop_images= np.concatenate([ np.expand_dims(i,0) for i in develop_dataset.images])\n",
    "\n",
    "true_labels=develop_dataset.labels\n",
    "y_pred=model.predict(develop_images,batch_size=2)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.2,\n",
    "                                   iou_threshold=0.1,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=True,#normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU\n",
    "Intersect over union\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "def area_minmax(bbox):\n",
    "    xmin,ymin,xmax,ymax=bbox\n",
    "    return(xmax-xmin)*(ymax-ymin)\n",
    "\n",
    "def intersect_minmax(bbox1,bbox2):\n",
    "    xmin1,ymin1,xmax1,ymax1=bbox1\n",
    "    xmin2,ymin2,xmax2,ymax2=bbox2\n",
    "\n",
    "    xintmin=max(xmin1,xmin2)\n",
    "    yintmin=max(ymin1,ymin2)\n",
    "\n",
    "    xintmax=min(xmax1,xmax2)\n",
    "    yintmax=min(ymax1,ymax2)\n",
    "   \n",
    "    int_width=(xintmax-xintmin)    \n",
    "    int_height=(yintmax-yintmin)\n",
    "    if int_width <0 or int_height <0 :return 0\n",
    "    return int_width*int_height\n",
    "            \n",
    "def iou(bbox1,bbox2):\n",
    "    intarea=intersect_minmax(bbox1,bbox2)\n",
    "    u=area_minmax(bbox1)+area_minmax(bbox2)-intarea\n",
    "    return intarea/u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten_labels(labels):\n",
    "    values=[]\n",
    "    for l in labels:\n",
    "        for n in l:\n",
    "            values.append(n)\n",
    "    return values\n",
    "\n",
    "\n",
    "t_all=flatten_labels(true_labels)\n",
    "p_all=flatten_labels(y_pred_decoded)\n",
    "\n",
    "true_area=[area_minmax(l[1:5]) for l in t_all]\n",
    "pred_area=[area_minmax(l[2:6]) for l in p_all]\n",
    "\n",
    "true_width=[l[3]-l[1] for l in t_all]\n",
    "pred_width=[l[4]-l[2] for l in p_all]\n",
    "\n",
    "\n",
    "true_cat=[l[0] for l in t_all]\n",
    "pred_cat=[l[0] for l in p_all]\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(true_area,range=(50*50,200*200),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_area,range=(50*50,200*200),bins=10,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_width,range=(50,200),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_width,range=(50,200),bins=10,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_cat,range=(0,10),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_cat,range=(0,10),bins=10,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#match best bo\n",
    "def match(truth,pred):\n",
    "    best=[]\n",
    "    for t in truth:\n",
    "        matches=[ [iou(t[1:],p[2:]),i] for i,p in enumerate(pred)]\n",
    "        matches.sort()\n",
    "        if matches!=[]:            \n",
    "            best.append(matches[-1]) # largest iou\n",
    "        else:\n",
    "            best.append([0,-1])\n",
    "    return best\n",
    "\n",
    "matches=[match(t,p) for t,p in zip(true_labels,y_pred_decoded)]\n",
    "\n",
    "n_no_matches=0\n",
    "n_matches=0\n",
    "correct=0\n",
    "for index,image in enumerate(matches):\n",
    "    for li,m in enumerate(image):\n",
    "        if m[0] < .1 or m[1]==-1:n_no_matches+=1 # no match\n",
    "        else:\n",
    "            n_matches+=1 # no match\n",
    "            y_pred_decoded[index][m[1]][0]==true_labels[index][li][0]\n",
    "            correct+=1\n",
    "\n",
    "print(n_matches/(n_no_matches+n_matches)*100,\"% True Boxes Matched\")\n",
    "print(correct/(n_no_matches+n_matches)*100,\"% True Boxes Matched and identified\")\n",
    "print(len(p_all)-n_matches,\" wrong boxes\", correct,\"correct boxes\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see\n",
    "* Bad at predicting rings\n",
    "* Boxes don't have a very wide range\n",
    "* Predictions aren't very confident\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ((((((((((((((((((((()))))))))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_classes = 4 \n",
    "normalize_coords = True \n",
    "\n",
    "\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    scales=[.2,.15,.12,.1,.08],\n",
    "                    mode='training',\n",
    "                    normalize_coords=normalize_coords\n",
    "                   )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    variances=[1.,1., 1., 1.],\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=.9,\n",
    "                                    neg_iou_limit=0.7,\n",
    "                                    normalize_coords=normalize_coords,\n",
    "                                    background_id=0 # This has to be zero for decoder to work                               \n",
    "                                   )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=5\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[image_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "develop_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "\n",
    "    final_epoch     = 50\n",
    "    \n",
    "    history = model.fit_generator(generator=train_generator,\n",
    "                              epochs=final_epoch,\n",
    "                              steps_per_epoch=len(train_dataset.images)//batch_size,\n",
    "                              validation_data=develop_generator,\n",
    "                              validation_steps=len(develop_dataset.images)//batch_size,\n",
    "                              )\n",
    "\n",
    "    model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd_scale.h5\")\n",
    "\n",
    "\n",
    "example_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'processed_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_images,batch_labels=next(example_generator)\n",
    "y_pred = model.predict(batch_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                       confidence_thresh=0.2,\n",
    "                                       iou_threshold=0.0,\n",
    "                                       top_k=200,\n",
    "                                       normalize_coords=True,#normalize_coords,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    labels=[convert_ssd_labels(l) for l in y_pred_decoded[i]]\n",
    "    plot_example(batch_images[i],labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ssd_input_encoder.predictor_sizes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    variances=[1.,1., 1., 1.],\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=.9,\n",
    "                                    neg_iou_limit=0.7,\n",
    "                                    normalize_coords=normalize_coords,\n",
    "                                    background_id=0 # This has to be zero for decoder to work                               \n",
    "                                   )\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[image_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'processed_labels',\n",
    "                                                 'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "__,test,lab=next(train_generator)\n",
    "print(lab)\n",
    "print(np.sum(test[:,:,0]!=1))\n",
    "print(np.sum(test[:,:,0]!=1))\n",
    "#####################################33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_images,batch_labels=next(example_generator)\n",
    "y_pred = model.predict(batch_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                       confidence_thresh=0.2,\n",
    "                                       iou_threshold=0.1,\n",
    "                                       top_k=200,\n",
    "                                       normalize_coords=True,#normalize_coords,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    labels=[convert_ssd_labels(l) for l in y_pred_decoded[i]]\n",
    "    plot_example(batch_images[i],labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds=[]\n",
    "all_truth=[]\n",
    "develop_images= np.concatenate([ np.expand_dims(i,0) for i in develop_dataset.images])\n",
    "\n",
    "true_labels=develop_dataset.labels\n",
    "y_pred=model.predict(develop_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.0,\n",
    "                                   iou_threshold=0.0,\n",
    "                                   top_k=2000,\n",
    "                                   normalize_coords=True,#normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "t_all=flatten_labels(true_labels)\n",
    "p_all=flatten_labels(y_pred_decoded)\n",
    "\n",
    "true_area=[area_minmax(l[1:5]) for l in t_all]\n",
    "pred_area=[area_minmax(l[2:6]) for l in p_all]\n",
    "\n",
    "true_width=[l[3]-l[1] for l in t_all]\n",
    "pred_width=[l[4]-l[2] for l in p_all]\n",
    "\n",
    "\n",
    "true_cat=[l[0] for l in t_all]\n",
    "pred_cat=[l[0] for l in p_all]\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(true_area,range=(10*10,500*500),bins=50,label='true',histtype='step')\n",
    "plt.hist(pred_area,range=(10*10,500*500),bins=50,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_width,range=(0,1200),bins=100,label='true',histtype='step')\n",
    "plt.hist(pred_width,range=(0,1200),bins=100,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(true_cat,range=(0,10),bins=10,label='true',histtype='step')\n",
    "plt.hist(pred_cat,range=(0,10),bins=10,label='pred',histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#match best bo\n",
    "def match(truth,pred):\n",
    "    best=[]\n",
    "    for t in truth:\n",
    "        matches=[ [iou(t[1:],p[2:]),i] for i,p in enumerate(pred)]\n",
    "        matches.sort()\n",
    "        if matches!=[]:            \n",
    "            best.append(matches[-1]) # largest iou\n",
    "        else:\n",
    "            best.append([0,-1])\n",
    "    return best\n",
    "\n",
    "matches=[match(t,p) for t,p in zip(true_labels,y_pred_decoded)]\n",
    "\n",
    "n_no_matches=0\n",
    "n_matches=0\n",
    "correct=0\n",
    "for index,image in enumerate(matches):\n",
    "    for li,m in enumerate(image):\n",
    "        if m[0] < .1 or m[1]==-1:n_no_matches+=1 # no match\n",
    "        else:\n",
    "            n_matches+=1 # no match\n",
    "            y_pred_decoded[index][m[1]][0]==true_labels[index][li][0]\n",
    "            correct+=1\n",
    "\n",
    "print(n_matches/(n_no_matches+n_matches)*100,\"% True Boxes Matched\")\n",
    "print(correct/(n_no_matches+n_matches)*100,\"% True Boxes Matched and identified\")\n",
    "print(len(p_all)-n_matches,\" wrong boxes\", correct,\"correct boxes\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K.clear_session()\n",
    "\n",
    "img_height = 600 #height of the input images\n",
    "img_width = 800 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "n_classes = 4 # Number of positive classes\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=127.5,\n",
    "                    divide_by_stddev=127.5,\n",
    "                    l2_regularization=0.0005\n",
    "                   )\n",
    "\n",
    "\n",
    "#New input \n",
    "new_input_layer=keras.layers.Input((1200,1600,3))\n",
    "half_size=keras.layers.AveragePooling2D(2)(new_input_layer)\n",
    "\n",
    "new_outputs=model(half_size)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "new_model=keras.models.Model(new_input_layer,new_outputs)\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    variances=[1.,1., 1., 1.],\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords,\n",
    "                                    background_id=0)\n",
    "\n",
    "batch_size=10\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "develop_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "\n",
    "new_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "model.summary()\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#new_model.load_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd_half.h5\")\n",
    "\n",
    "history = new_model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=len(train_dataset.images)//batch_size,\n",
    "                              epochs=200,\n",
    "                              validation_data=develop_generator,\n",
    "                              validation_steps=len(develop_dataset.images)//batch_size,\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd_half.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_height=1200\n",
    "img_width=1600\n",
    "batch_images,batch_labels=next(example_generator)\n",
    "y_pred = new_model.predict(batch_images)\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                       confidence_thresh=0.,\n",
    "                                       iou_threshold=0.1,\n",
    "                                       top_k=200,\n",
    "                                       normalize_coords=True,#normalize_coords,\n",
    "                                       img_height=img_height,\n",
    "                                       img_width=img_width)\n",
    "\n",
    "print(img_height,img_width)\n",
    "for i in range(batch_size):\n",
    "    print(y_pred_decoded[i])\n",
    "    print(batch_labels[i])\n",
    "    labels=[convert_ssd_labels(l) for l in y_pred_decoded[i]]\n",
    "\n",
    "    print(labels)\n",
    "    plot_example(batch_images[i],labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do some fine tunning\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "img_height = 300 #height of the input images\n",
    "img_width = 400 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "n_classes = 4 # Number of positive classes\n",
    "\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "\n",
    "aspect_ratios = [0.75, 1.0, 1.5] # The list of aspect ratios for the anchor boxes\n",
    "\n",
    "\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords\n",
    "                   )\n",
    "\n",
    "\n",
    "#New input \n",
    "new_input_layer=keras.layers.Input((1200,1600,3))\n",
    "half_size=keras.layers.AveragePooling2D(4)(new_input_layer)\n",
    "print(half_size)\n",
    "print(model.input)\n",
    "new_outputs=model(half_size)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "new_model=keras.models.Model(new_input_layer,new_outputs)\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "batch_size=50\n",
    "print(n_classes)\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "develop_generator = develop_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "\n",
    "new_model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "test,test_l=next(train_generator)\n",
    "print(test_l[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = new_model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=final_epoch,\n",
    "                              workers=1,\n",
    "                              #callbacks=callbacks,\n",
    "                              validation_data=develop_generator,\n",
    "                              validation_steps=len(develop_dataset.images)//batch_size,\n",
    "                              initial_epoch=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_weights(\"/projects/bgmp/shared/2019_ML_workshop/models/ssd_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
